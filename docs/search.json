[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data science capstone preparation",
    "section": "",
    "text": "PSTAT197A/CMPSC190DD is the first course in UC Santa Barbara’s yearlong data science capstone sequence. The course aims to provide preparation and training to undergraduate students of any discipline with a basic background in data science for an independent research or project experience.\nMost students take this course to prepare for their work on sponsored team projects during the remainder of the capstone sequence (PSTAT197B-C/CMPSC190DE-DF). However, some may elect to take the course for other reasons, such as an upcoming faculty-supervised research project or internship.\nThis site hosts course information and resources for currently enrolled students.",
    "crumbs": [
      "Data science capstone preparation"
    ]
  },
  {
    "objectID": "about/technology.html",
    "href": "about/technology.html",
    "title": "Technology",
    "section": "",
    "text": "Computing in PSTAT197A will be shown in R, and codes and other materials will be shared via GitHub. The following software will be required to access course materials:\n\nR\nRStudio\nGit\nGitHub Desktop (or another visual GitHub client)\n\nInstallations and basic functionality will be covered in the first section meeting.\nWhile PSTAT197A is not language-agnostic and some instruction in R is provided, it is also not a course especially emphasizing programming technique in R. Students are free to use or experiment with other software at their discretion provided it does not interfere with their participation in the class, but are expected to submit work and collaborate using RStudio-supported files.",
    "crumbs": [
      "PSTAT197A",
      "Technology"
    ]
  },
  {
    "objectID": "about/technology.html#sec-software",
    "href": "about/technology.html#sec-software",
    "title": "Technology",
    "section": "",
    "text": "Computing in PSTAT197A will be shown in R, and codes and other materials will be shared via GitHub. The following software will be required to access course materials:\n\nR\nRStudio\nGit\nGitHub Desktop (or another visual GitHub client)\n\nInstallations and basic functionality will be covered in the first section meeting.\nWhile PSTAT197A is not language-agnostic and some instruction in R is provided, it is also not a course especially emphasizing programming technique in R. Students are free to use or experiment with other software at their discretion provided it does not interfere with their participation in the class, but are expected to submit work and collaborate using RStudio-supported files.",
    "crumbs": [
      "PSTAT197A",
      "Technology"
    ]
  },
  {
    "objectID": "about/technology.html#sec-github",
    "href": "about/technology.html#sec-github",
    "title": "Technology",
    "section": "GitHub",
    "text": "GitHub\nStudents will learn and practice basic functionality of Git and GitHub for version control and collaboration by accessing course materials via GitHub repositories and submitting work via repository contributions.\nWe have a GitHub classroom for the data science capstone. Materials will be deployed via direct links. Students will be asked to submit work by contributing to team repositories; any such contributions will remain visible to course staff and team contributors, and so are not strictly private.\nTo access GitHub Classroom materials students will need to create a GitHub account if they do not already have one. Here is some advice on choosing a username.",
    "crumbs": [
      "PSTAT197A",
      "Technology"
    ]
  },
  {
    "objectID": "about/links.html",
    "href": "about/links.html",
    "title": "Links",
    "section": "",
    "text": "Lecture Attendance reporting form; fill out once per class meeting.\nCapstone project intake form; fill out by October 6.",
    "crumbs": [
      "PSTAT197A",
      "Links"
    ]
  },
  {
    "objectID": "about/links.html#sec-class-forms",
    "href": "about/links.html#sec-class-forms",
    "title": "Links",
    "section": "",
    "text": "Lecture Attendance reporting form; fill out once per class meeting.\nCapstone project intake form; fill out by October 6.",
    "crumbs": [
      "PSTAT197A",
      "Links"
    ]
  },
  {
    "objectID": "about/links.html#resources",
    "href": "about/links.html#resources",
    "title": "Links",
    "section": "Resources",
    "text": "Resources\nTextbooks:\n\nModern Data Science with R by Baumer, Kaplan, and Horton.\nIntroduction to Statistical Learning with Applications in R by James et al.\nFundamentals of Data Visualization by Claus Wilke.\nR for Data Science by Wickham and Grolemund.\nDeep Learning by Goodfellow, Bengio, and Courville.\n\nDocumentation:\n\nTidyverse and tidymodels packages\nGitHub Docs",
    "crumbs": [
      "PSTAT197A",
      "Links"
    ]
  },
  {
    "objectID": "pstat197bc/interim-report-guidelines.html",
    "href": "pstat197bc/interim-report-guidelines.html",
    "title": "Interim report guidelines",
    "section": "",
    "text": "This assignment is an opportunity for your team to reflect on their progress and plan for the term ahead. It is also an opportunity to practice writing a technical report. You will get more out of the assignment if you take the time to do that reflection together.\nBroadly, the interim report should summarize your work to date in a presentable manner."
  },
  {
    "objectID": "pstat197bc/interim-report-guidelines.html#purpose",
    "href": "pstat197bc/interim-report-guidelines.html#purpose",
    "title": "Interim report guidelines",
    "section": "",
    "text": "This assignment is an opportunity for your team to reflect on their progress and plan for the term ahead. It is also an opportunity to practice writing a technical report. You will get more out of the assignment if you take the time to do that reflection together.\nBroadly, the interim report should summarize your work to date in a presentable manner."
  },
  {
    "objectID": "pstat197bc/interim-report-guidelines.html#expectations",
    "href": "pstat197bc/interim-report-guidelines.html#expectations",
    "title": "Interim report guidelines",
    "section": "Expectations",
    "text": "Expectations\n\nAudience\nYou are writing for a broad audience. You can assume the reader is familiar with statistics and data science at about the same level as your peers, but should not assume they have any expertise in your area or with the data or methods you are using.\nAvoid jargon, define key terms or acronyms when they first appear, and dedicate space to explaining data and methodology carefully, but avoid over-explaining or providing excessive detail – you are describing research, not teaching a class.\n\n\nLength\nThe report should be no less than 5 pages and no more than 10 pages. If necessary, you can include supplementary material or appendices following the main body of the report or as separate files/documents.\n\n\nContent\nOrganize your report into the sections outlined below. Use subheadings as appropriate to your project to further differentiate report contents within sections. Whenever possible, please make subheadings specific (e.g., “Biological species surveys” is better than “Data”). In general, the headings and subheadings should provide an outline of the contents of a report that is easy for the reader to skim and informative.\n\nIntroduction\nIntroduce the topic of your project and provide any background needed to understand your work. Above all, your introduction should convey the primary motivation and potential value of your work. You are not expected to do a literature review, but if there is closely related prior work or your sponsor has provided you with a collection of relevant papers, you should summarize these and provide citations in the introduction. Close the introduction with a statement of your project objective(s) and a summary of the organization of the report.\nProblem(s) of interest\nConcisely state the problems you’re working on. These should be the high-level problems corresponding to the project objectives that you hope to solve by the end of the capstone – not practical/technical problems encountered along the way like data representation or wrangling, issues with model training or algorithm implementations, computational constraints, etc.\nIf your project has multiple arms, dedicate one short paragraph to each arm. Aim to sum up the problems of interest for each arm in just 1-3 sentences (not required, but a good aspiration).\nTip: If you find yourself struggling to summarize the problems of interest concisely or providing a lot of context or explanation, that’s probably a sign that you need to go back and revise your introduction.\nMaterials and methods\nDescribe your data sets and the statistical/computational techniques you are using to address the problems of interest. Provide high-level summaries of any key methods: if you are fitting a model, write the model clearly and explain how you estimate parameters; if you are designing an algorithm, write it up in pseudo-code; if you are performing hypothesis tests, state the hypotheses and indicate the test. This should be detailed but not long; if it begins to sprawl over many pages, that’s a sign that you need to edit it down.\nPreliminary findings\nPresent key findings from this quarter. Resist the temptation to describe everything you’ve done – focus just on the results that capture the most significant advances. Prepare a table or figure that helps to communicate each key finding with an appropriate caption. If you include a table or figure, you should also describe it in words and explain its significance. No tables or figures should appear that you do not also discuss in the text of the report.\nTip: start by preparing the tables and figures, determine the order to present them in, and then write this section around those elements.\nDiscussion\nThis is your opportunity to describe any particular challenges, interpretations of findings, and the like. Try to stick to the facts in the findings section, and use this section to explain how they relate to your problems of interest and project objectives.\nFuture work\nDescribe how you anticipate extending your project into spring quarter. Be specific. You do not need to present a detailed plan or timeline, but should convey what you expect to work on during Spring quarter. Use this as an opportunity to set goals and expectations for your group.\n\n\n\nFormat\nYour report should be in PDF (.pdf) or Word (.docx) format.\nAll figures and tables should be labeled and captioned, e.g., “Fig. 1: map of sampling locations in 2018”.\nCopies of any figures should be provided as separate images in JPEG, TIFF, or PNG format; images should have a resolution of at least 300 dots per inch (dpi).\nSupplementary material can be submitted in any format."
  },
  {
    "objectID": "pstat197bc/interim-report-guidelines.html#how-to-submit",
    "href": "pstat197bc/interim-report-guidelines.html#how-to-submit",
    "title": "Interim report guidelines",
    "section": "How to submit",
    "text": "How to submit\nUpload your PDF report and any supplementary files to this Drive folder. Create a folder with your team name. Within the folder use the naming conventions:\n\nreport document: TEAM-interim-report-S24.pdf\n\n\n\ncopies of figures from the report: img/TEAM-fig1.png (filename should match figure label in document)\nsupplementary material: TEAM-[description].[extention]"
  },
  {
    "objectID": "pstat197bc/about-spring-meetings.html",
    "href": "pstat197bc/about-spring-meetings.html",
    "title": "Spring class meetings",
    "section": "",
    "text": "Schedule is tentative and subject to change. Check back for updates.\n\n\n\n\n\n\n\nWeek\nClass meeting\nAssignments/comments\n\n\n\n\n2 (4/10/24)\nIntroduction and spring quarter overview\n\n\n\n3 (4/15/24)\nOffice Hours/iterim report planning\nInterim reports due Friday 11:59pm PST\n\n\n4 (4/22/24)\nIndividual presentations\n\n\n\n5 (4/29/24)\nIndividual presentations\n\n\n\n6 (5/6/24)\nIndividual presentations\n\n\n\n7 (5/13/24)\nWorkshopping: figure drafts\n\n\n\n8 (5/20/24)\nWorkshopping: presenting figures\n\n\n\n9 (5/29/24)\nWorkshopping: poster drafts\nNo class Monday;WEDNESDAY meeting instead\n\n\n10 (6/3/24)\nPublic showcase\nLoma Pelona center 1pm – 5pm\n\n\n11 (finals)\n\nIndividual project summaries due Monday 6/10, Peer review due Friday 6/14 11:59pm PST"
  },
  {
    "objectID": "pstat197bc/about-spring-meetings.html#useful-links",
    "href": "pstat197bc/about-spring-meetings.html#useful-links",
    "title": "Spring class meetings",
    "section": "Useful links",
    "text": "Useful links\n\nAttendance form (please fill out once per meeting)\nDrive folder for presentation filesharing (be aware, contents visible with a UCSB account and the link)\nPresentation guidelines – requirements and suggestions for work-in-progress presentations and agenda"
  },
  {
    "objectID": "pstat197bc/about-spring-meetings.html#week-2",
    "href": "pstat197bc/about-spring-meetings.html#week-2",
    "title": "Spring class meetings",
    "section": "Week 2",
    "text": "Week 2\n\nMonday meeting: welcome back and spring term overview [slides]\n[Work-in-progress presentation Groups]\nClass assignments:\n\ninterim reports due Friday 4/19 11:59pm PST (upload to Drive folder). See guidelines for interim reports\nfigure for website due Friday 4/19 11:59pm PST [upload here]"
  },
  {
    "objectID": "pstat197bc/about-spring-meetings.html#weeks-4-6",
    "href": "pstat197bc/about-spring-meetings.html#weeks-4-6",
    "title": "Spring class meetings",
    "section": "Weeks 4-6",
    "text": "Weeks 4-6\n\nMonday meetings: work-in-progress presentations, as scheduled by group [slides]"
  },
  {
    "objectID": "pstat197bc/about-spring-meetings.html#week-7",
    "href": "pstat197bc/about-spring-meetings.html#week-7",
    "title": "Spring class meetings",
    "section": "Week 7",
    "text": "Week 7\n\nAssignments: \n\nPrepare figure drafts for poster by next class meeting on 5/20\nreview [poster template]\n\nMonday meeting: Drafting presentation figures [slides]"
  },
  {
    "objectID": "pstat197bc/about-spring-meetings.html#week-8",
    "href": "pstat197bc/about-spring-meetings.html#week-8",
    "title": "Spring class meetings",
    "section": "Week 8",
    "text": "Week 8\n\nAssignments:\n\nPrepare poster draft by next class meeting\n\nMonday meeting: Presenting figures, poster layout [slides]\nOther poster templates: [1], [2], [3], [4]"
  },
  {
    "objectID": "pstat197bc/about-spring-meetings.html#week-9",
    "href": "pstat197bc/about-spring-meetings.html#week-9",
    "title": "Spring class meetings",
    "section": "Week 9",
    "text": "Week 9\n\nAssignments:\n\nFinalize and print poster (see slides for printing instructions)\n\nWednesday meeting: Practice poster presentations, logistics [slides]"
  },
  {
    "objectID": "pstat197bc/about-spring-meetings.html#week-10",
    "href": "pstat197bc/about-spring-meetings.html#week-10",
    "title": "Spring class meetings",
    "section": "Week 10",
    "text": "Week 10\n\nAssignments\n\nSubmit individual project summary by Monday, 6/10 11:59pm PST [guidelines] [upload]\nSubmit PDF copy of your group’s poster [drive folder]\n\nMonday meeting: poster showcase at Loma Pelona. [RSVP] ONLY If you have any dietary restrictions."
  },
  {
    "objectID": "pstat197bc/slides/week1-intro.html#capstone-goals",
    "href": "pstat197bc/slides/week1-intro.html#capstone-goals",
    "title": "Welcome to Data Science Capstone",
    "section": "Capstone goals",
    "text": "Capstone goals\n\nmake original contributions on your research topic;\ndevelop domain expertise in your project area(s);\ndevelop and practice communicating research outcomes to diverse audiences;\nimprove collaboration and teamwork skills;\nadvance your career development;\nproduce a strong work sample for your professional portfolio"
  },
  {
    "objectID": "pstat197bc/slides/week1-intro.html#course-structure",
    "href": "pstat197bc/slides/week1-intro.html#course-structure",
    "title": "Welcome to Data Science Capstone",
    "section": "Course structure",
    "text": "Course structure\n\n1 weekly class meeting, usually Wednesdays.\n1 weekly team meeting, scheduled at a time of your choice\noptional but highly recommended: second team meeting or working session without advisors\nfew class assignments each term:\n\none short presentation\nmid-term peer review\nsummative deliverable at end of term"
  },
  {
    "objectID": "pstat197bc/slides/week1-intro.html#expectations",
    "href": "pstat197bc/slides/week1-intro.html#expectations",
    "title": "Welcome to Data Science Capstone",
    "section": "Expectations",
    "text": "Expectations\n\nweekly time commitment: 12h = 2h in meetings + 10h on projects\nengage fully in project work, with some accountability:\n\ndraft, sign, and adhere to a team contract\ntake on one pre-defined organizational role in your team\nparticipate in peer reviews once per quarter"
  },
  {
    "objectID": "pstat197bc/slides/week1-intro.html#roles",
    "href": "pstat197bc/slides/week1-intro.html#roles",
    "title": "Welcome to Data Science Capstone",
    "section": "Roles",
    "text": "Roles\nYou’ll take on one of the following roles:\n\nspokesperson\nmeeting organizer\nnote-taker\nrepository and data manager\nequity manager"
  },
  {
    "objectID": "pstat197bc/slides/week1-intro.html#peer-review",
    "href": "pstat197bc/slides/week1-intro.html#peer-review",
    "title": "Welcome to Data Science Capstone",
    "section": "Peer review",
    "text": "Peer review\nEach quarter you’ll evaluate yourself/teammates on the extent to which you/they achieved the following:\n\nAttended, participated in, and contributed to project meetings\nHonored agreements specified in the team contract\nFulfilled assigned team role\nContributed fair share of work\nCooperated with teammates\nCommunicated clearly and respectfully with teammates\nHelped teammates when asked"
  },
  {
    "objectID": "pstat197bc/slides/week1-intro.html#class-meetings",
    "href": "pstat197bc/slides/week1-intro.html#class-meetings",
    "title": "Welcome to Data Science Capstone",
    "section": "Class meetings",
    "text": "Class meetings\nClass meetings will be run like a seminar: we’ll rotate through the project teams, with two teams presenting each meeting.\n\n~30min available per team, but budget time for discussion\ncurrently in alphabetical order, but can swap by mutual agreement\nto prepare: 10-ish slides and 1-2 page handout\nfurther guidelines TBA"
  },
  {
    "objectID": "pstat197bc/slides/week1-intro.html#assessments",
    "href": "pstat197bc/slides/week1-intro.html#assessments",
    "title": "Welcome to Data Science Capstone",
    "section": "Assessments",
    "text": "Assessments\n\nattendance record and class assignments;\nend-of-term advisor assessments of individual participation and contributions;\ninstructor assessments of in-class presentations and poster;\npeer reviews and end-of-term individual reflections."
  },
  {
    "objectID": "pstat197bc/slides/week1-intro.html#initial-meetings",
    "href": "pstat197bc/slides/week1-intro.html#initial-meetings",
    "title": "Welcome to Data Science Capstone",
    "section": "Initial meetings",
    "text": "Initial meetings\nSchedule an initial meeting for week 2. Can recommend: Slack channel + [when2meet].\nSuggested agenda:\n\nintroductions (15m):\n\nname, position/background, fun fact\nsomething exciting about the project\n\nproject intro (20m)\n\nhave sponsor tell you about the project in detail in their own words\nfocus on the big picture, ask questions\n\ndata access and logistics (10m)\n\nmake arrangements to access/obtain data\n\naction items (10m)\n\ndetermine what actions will be taken before the next meeting\nassign responsibility for each action"
  },
  {
    "objectID": "pstat197bc/slides/week1-intro.html#assignments",
    "href": "pstat197bc/slides/week1-intro.html#assignments",
    "title": "Welcome to Data Science Capstone",
    "section": "Assignments",
    "text": "Assignments\n\nMotivational statement due Friday 1/19 11:59pm PST. Take some time to write a paragraph or two in response to the following prompts (Submit via Google form):\n\nWhy are you participating in the capstone? What attracted you to the opportunity in the first place and how does it relate to your goals for your undergraduate education?\nWhat do you hope to get out of the experience? Why is that important to you?\n\nTeam contracts due Friday 1/26 11:59pm PST. See template.\n\nSubmit via Google drive"
  },
  {
    "objectID": "pstat197bc/about-syllabus.html",
    "href": "pstat197bc/about-syllabus.html",
    "title": "Course syllabus",
    "section": "",
    "text": "Students will devote the vast majority of their time and effort in this course to project work and are expected to engage fully with their projects. Class meetings will be conducted as seminars with rotating presentations by project teams. Students will meet weekly with their project teams and regular progress is expected. Each team will prepare one written project summary at the end of winter quarter and one poster presentation at the end of spring quarter.\nConcurrent course listing: PSTAT197B-C and CMPSC190DE-DF are held concurrently; enrollment is by instructor consent and admitted students may enroll under either listing. The course content, expectations, assessments, and course policies are identical for students enrolled in either course.\nCatalog description: Research opportunities for undergraduate students. Students practice their data science and applied statistics skills by completing a hands-on team project on a practical problem proposed by a project sponsor. Students are expected to give regular oral presentations and prepare at least one written report on their research. Prerequisite: PSTAT197A/CMPSC190DD."
  },
  {
    "objectID": "pstat197bc/about-syllabus.html#learning-outcomes",
    "href": "pstat197bc/about-syllabus.html#learning-outcomes",
    "title": "Course syllabus",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nThrough their project work and participation in class meetings, students can expect to:\n\nmake original contributions on a methodological or applied research topic involving statistics, data science, and computational science;\ndevelop domain expertise in the area(s) of application relevant to their project topic;\ndevelop and practice effective strategies for communicating research outcomes with clarity and confidence;\nimprove collaboration and teamwork skills;\ndeepen their understanding of careers in data science;\nproduce a strong work sample for inclusion in a professional portfolio."
  },
  {
    "objectID": "pstat197bc/about-syllabus.html#meetings",
    "href": "pstat197bc/about-syllabus.html#meetings",
    "title": "Course syllabus",
    "section": "Meetings",
    "text": "Meetings\n\nWinter 2024\nClass meetings are held once weekly 2pm – 3:15pm Wednesdays in North Hall 1105.\nProject meetings are held once weekly in teams at a time of their choosing either via Zoom, on campus, or on site with the project sponsor.\nWe will not meet regularly at other officially scheduled times but we will use the Monday slot as an alternate class meeting time.\nStudents are encouraged to block off section times in their schedule to facilitate convenient scheduling of project work. Section rooms are available during those times as workspaces."
  },
  {
    "objectID": "pstat197bc/about-syllabus.html#people",
    "href": "pstat197bc/about-syllabus.html#people",
    "title": "Course syllabus",
    "section": "People",
    "text": "People\n\nInstructor\nLaura Baracaldo, Visiting Assistant Professor, Statistics.\n\n\nProject advisors\n(In alphabetical order by last name)\n\nLaura Baracaldo, Visiting Assistant Professor, Statistics\nAlex Franks, Assistant Professor, Statistics\nYan Lashchev, Undergraduate Program Advisor, Statistics\nJose Nino, Data Science Student Advisor, Computer Science\nErika McPhillips, PhD Student, Statistics\nAvani Tanna, Lecturer, Computer Science\n\n\n\nProject sponsors\n\nJordan Tran, Appfolio\nNate Emery, CITRAL\nMichaela Alksne, Scripps Institution of Oceanography\nKatja Seltmann, CCBER\nShraddhanand Shukla, Climate Hazards Center, Geography Department\nMichael Schmidt, Nation Builder\nMatto Mildenberger, Political Science Department\nEric Leidersdorf, P3\nDerek Mendez, SLAC National Accelerator Laboratory"
  },
  {
    "objectID": "pstat197bc/about-syllabus.html#expectations",
    "href": "pstat197bc/about-syllabus.html#expectations",
    "title": "Course syllabus",
    "section": "Expectations",
    "text": "Expectations\nTime commitment. The course carries a time commitment of 12 hours per week (3 hours per credit unit). Class and project meetings account for roughly 2 hours per week, and class assignments are kept to a minimum to facilitate project work; thus, students should anticipate dedicating 8-10 hours outside of the classroom to project work each week. Students are strongly encouraged to establish a secondary meeting without their advisors each week to help ensure effective time and task management among the group.\nTeam contracts. Each team will develop a contract articulating their goals as a group, basic agreements about communication and collaboration, and steps that will be taken in the event of failure to uphold those agreements.\nTeam roles. Each student will take on one of the following roles per term by mutual agreement with their team :\n\nspokesperson (one person) who is responsible for communicating with advisors and course staff on behalf of the team;\nmeeting organizer(one person) who is responsible for arranging project meetings and keeping time during meetings;\nnote-takers(two people) who is responsible for maintaining a written record of each meeting;\nrepository/data/document managers(one-two people) who are responsible for coordinating access to project files and keeping them organized;\nequity manager(one person) who is responsible for ensuring a fair distribution of work among the team members.\n\nPeer review. Teams will conduct an anonymized peer review each quarter in which each student rates their teammates on:\n\ncooperation with the group;\ncommunication;\nadherence to with agreements established by the group contract;\nfulfillment of their assigned role;\nthe extent to which they contributed their fair share to project work.\n\nAggregated results will be used to provide individual feedback and identify any necessary interventions to improve quality of collaboration.\nClass presentations. Each team is expected to present some of their work once per term to the class. In-class presentations should be informal and allow time for intermittent discussion and questions. In Winter, teams will give group presentations and should prepare about 10 slides and a 1-2 page handout. It is strongly recommended that just one team member give each presentation. In Spring, each team member will take a turn presenting individually to a smaller audience consisting of their teammates and one other project group.\nPoster presentation. A public showcase of project work will be held on campus at the end of the Spring quarter. Each team is expected to prepare a poster to present at the showcase.\nNon-disclosure agreements. Some projects are subject to non-disclosure agreements (NDAs) between the university and the sponsoring organization. If so, project advisors will notify students of the terms and share copies of the NDA; students are expected to uphold any such NDAs but should not directly sign any agreements."
  },
  {
    "objectID": "pstat197bc/about-syllabus.html#assessments",
    "href": "pstat197bc/about-syllabus.html#assessments",
    "title": "Course syllabus",
    "section": "Assessments",
    "text": "Assessments\nStudents will be evaluated based on the following:\n\nattendance record and class assignments;\nend-of-term advisor assessments of individual participation and contributions;\ninstructor assessments of in-class presentations and poster;\npeer reviews and end-of-term individual reflections."
  },
  {
    "objectID": "pstat197bc/about-syllabus.html#policies",
    "href": "pstat197bc/about-syllabus.html#policies",
    "title": "Course syllabus",
    "section": "Policies",
    "text": "Policies\nAttendance. Regular attendance at class meetings is expected. Each student can miss one class meeting without notice; further absences may impact course grades. Students are responsible for material discussed in their absence and should review notes and consult a classmate.\nDeadlines. Students are expected to meet assignment deadlines in a timely manner. All deadlines have a 24-hour grace period. Late or amended work may or may not be accepted at the instructor’s discretion.\nEmail. Course staff will make their best effort to reply to email within 48 weekday hours. However, due to high volume, staff cannot guarantee that all messages will receive replies.\nIllness. Students who are ill are required to stay home. Students ill with COVID-19 must comply with university policy regarding reporting and isolation. Accommodations will be made to ensure that students absent due to illness do not fall behind.\nAccommodations. Reasonable accommodations will be made for any student with a qualifying disability. Such requests should be made through the Disabled Students Program (DSP). More information, instructions on how to access accommodations, and information on related resources can be found on the DSP website. Note: in this class there are no timed assessments.\nLetter grades. Letter grades are assigned based only on the assessments identified above and according to university guidelines, with the relative weighting of assessments determined at the discretion of the instructor. While grade calculations will not be disclosed, students are entitled to an explanation of the criteria used to determine their grades if desired. Grades will not be changed except in the case of clerical errors. If students feel their grade has been unfairly assigned, they are entitled to contest it following UCSB procedure for contesting grades.\nConduct. All course participants are expected to maintain respectful and honorable conduct consistent with UCSB ethical standards. Students uncomfortable with the behavior of another course participant for any reason should notify the instructor, course staff, or, if the complaint relates to course staff conduct, an administrative or departmental officer. Evidence of academic dishonesty will be reported to the Office of Student Conduct (OSC); evidence of problematic behavior will be addressed on a case-by-case basis in accord with university policies."
  },
  {
    "objectID": "materials/vignette-guidelines.html",
    "href": "materials/vignette-guidelines.html",
    "title": "Vignette Guidelines",
    "section": "",
    "text": "A vignette is a simple example intended to help learn a method or tool.\nThe overarching goal of creating vignettes is to provide starting points for learning about specialized topics in data science that students in the class can later consult to familiarize themselves with an unfamiliar topic during their project work.\nThis document sets expectations for the organization and content of vignette repositories."
  },
  {
    "objectID": "materials/vignette-guidelines.html#configuring-your-repository",
    "href": "materials/vignette-guidelines.html#configuring-your-repository",
    "title": "Vignette Guidelines",
    "section": "Configuring your repository",
    "text": "Configuring your repository\nCreate a public repository in the PSTAT197-F23 workspace and add your teammates as collaborators.\nSelect options at the creation step to initialize the repository with:\n\na .README file\na .gitignore file\n\nGive the repository a descriptive name. Use the naming convention\n\nvignette-[keyword]\n\nfor example, “vignette-lstm”, “vignette-kriging”, “vignette-cnn”, and the like. A single keyword is best if possible, but consider using two or three if needed to make your repo name sufficiently specific, e.g., “vignette-database-configuration” or “vignette-distribution-based-clustering”. Do not use more than three keywords in your repository name.\nSupply an optional description that contains a long title for your vignette topic, for instance:\n\nDistribution-based clustering in R and application to unsupervised cell type classification\n\nLastly, once the repository is created, add a few topics to the “About” section on the far right of your repository homepage."
  },
  {
    "objectID": "materials/vignette-guidelines.html#directory-organization",
    "href": "materials/vignette-guidelines.html#directory-organization",
    "title": "Vignette Guidelines",
    "section": "Directory organization",
    "text": "Directory organization\nAs a general guideline, all files except the README should be placed in appropriately-named subdirectories so that your repository homepage is free from file clutter.\nIf your project has a single main file – in this case the vignette document – it is reasonable to place that in the root directory with the README. Everything else should go in a subfolder.\nThe high-level directories should clearly differentiate the main project contents, and overall there should not be too many levels of subdirectory, especially for a simple project like a code vignette. Your directory structure might look something like this in the end:\nroot directory\n|-- data\n    |-- raw\n    |-- processed\n|-- scripts\n    |-- drafts\n    |-- vignette-script.R\n|-- img\n    |-- fig1.png\n    |-- fig2.png\n|-- vignette.qmd\n|-- vignette.html\n|-- README.md\nAs a guiding principle, each subdirectory should contain either\n\n(a)a few primary files and one or more subdirectories\nscripts\n|-- functions\n|-- drafts\n|-- exploratory-analysis.R\n|-- model-fitting.R\n|-- visualizations.R\n(b) a single file type with an obvious naming convention\nimg\n|-- fig-autocorrelation.png\n|-- fig-forecasts.png\n|-- fig-rawseries.png\n|-- logo-ucsb.png\n\nTry to organize your repository so that it is easy to navigate for the general coding public (and for your future self)."
  },
  {
    "objectID": "materials/vignette-guidelines.html#readme-contents",
    "href": "materials/vignette-guidelines.html#readme-contents",
    "title": "Vignette Guidelines",
    "section": "README contents",
    "text": "README contents\nYour README file should contain five main pieces of information in the following order:\n\nA one-sentence description at the very top before any (sub)headers:\n\nVignette on implementing distribution-based clustering using cell type data; created as a class project for PSTAT197A in Fall 2023.\n\nContributors\nVignette abstract: a brief description in a few sentences of your vignette topic, example data, and outcomes.\nRepository contents: an explanation of the directory structure of the repository\nReference list: 2 or more references to learn more about your topic.\n\nA typical README file would also contain instructions on use and instructions on contributing to the repository."
  },
  {
    "objectID": "materials/vignette-guidelines.html#repository-contents",
    "href": "materials/vignette-guidelines.html#repository-contents",
    "title": "Vignette Guidelines",
    "section": "Repository contents",
    "text": "Repository contents\nYour repository should contain at minimum the following:\n\nan example dataset with which you illustrate the use of the method(s) or tool(s) of your topic\na primary vignette document – either a notebook or rendered markdown file – that teaches your method(s) and/or tool(s). this document should integrate codes with step-by-step explanation and read much like a lab activity\na script with line annoations that replicates all results shown in the primary vignette document end-to-end"
  },
  {
    "objectID": "materials/vignette-guidelines.html#evaluation",
    "href": "materials/vignette-guidelines.html#evaluation",
    "title": "Vignette Guidelines",
    "section": "Evaluation",
    "text": "Evaluation\nYour work will be evaluated on:\n\nhow well the repository and contents conform to the expectations outlined above\nthe clarity of the vignette, from the perspective of another student in the class\nthe correctness of the data analysis and any other technical aspects of the vignette"
  },
  {
    "objectID": "materials/slides/week1-github.html#announcementsreminders",
    "href": "materials/slides/week1-github.html#announcementsreminders",
    "title": "Basic GitHub actions",
    "section": "Announcements/reminders",
    "text": "Announcements/reminders\n\ntoday, sit with the members of your team (From Wednesday’s section)\nassignment for next time:\n\nread MDSR 9.1 – 9.2\nprepare a reading response"
  },
  {
    "objectID": "materials/slides/week1-github.html#objective-for-today",
    "href": "materials/slides/week1-github.html#objective-for-today",
    "title": "Basic GitHub actions",
    "section": "Objective for today",
    "text": "Objective for today\nLearn how to interact with GitHub repositories:\n\nretrieve and submit file changes;\nexamine repository updates;\nuse branches for parallel workflow;\nresolve conflicts."
  },
  {
    "objectID": "materials/slides/week1-github.html#basic-git-actions",
    "href": "materials/slides/week1-github.html#basic-git-actions",
    "title": "Basic GitHub actions",
    "section": "Basic Git actions",
    "text": "Basic Git actions\n\nCommunication actions for moving file changes between locations"
  },
  {
    "objectID": "materials/slides/week1-github.html#branching-workflow",
    "href": "materials/slides/week1-github.html#branching-workflow",
    "title": "Basic GitHub actions",
    "section": "Branching workflow",
    "text": "Branching workflow\n\nTypical use of repository branches for development of new features"
  },
  {
    "objectID": "materials/slides/week1-github.html#activity-overview",
    "href": "materials/slides/week1-github.html#activity-overview",
    "title": "Basic GitHub actions",
    "section": "Activity overview",
    "text": "Activity overview\n\nMake individual changes to files and create ‘commits’\nCreate repository branches to enable you to work more efficiently in parallel.\nMerge branches with the main branch via pull request.\nCreate and resolve a merge conflict."
  },
  {
    "objectID": "materials/slides/week1-github.html#setup",
    "href": "materials/slides/week1-github.html#setup",
    "title": "Basic GitHub actions",
    "section": "Setup",
    "text": "Setup\n\nhave everyone open their GitHub client, the sandbox project in RStudio, and the group sandbox repository in the browser on github.com"
  },
  {
    "objectID": "materials/course-materials.html",
    "href": "materials/course-materials.html",
    "title": "Course materials",
    "section": "",
    "text": "Objectives: set expectations; explore data science raison d’etre; introduce systems and design thinking; introduce software tools and collaborative coding; conduct exploratory/descriptive analysis of class background and interests.\n\n\n\nThursday meeting: Course orientation [slides]\nAssignments due by next class meeting:\n\ninstall course software and create github account;\nfill out intake form\nread Peng and Parker (2022)\nprepare a reading reading response\n\n\n\n\n\n\nTuesday meeting: On projects in(volving) data science [slides]\nSection meeting: software and technology overview [activity] Teams spreadsheet\nThursday meeting: basic GitHub actions [activity] [slides]\nAssignments due by next class meeting:\n\nread MDSR 9.1 and 9.2\nprepare a reading response\n\n\n\n\n\n\nTuesday meeting: Introducing class intake survey data [slides]\nSection meeting: tidyverse basics [activity]\nThursday meeting: planning group work for analysis of survey data [slides]\nAssignments:\n\nfirst team assignment due Friday, October 18, 11:59 PM PST [accept via GH classroom here]Teams spreadsheet",
    "crumbs": [
      "PSTAT197A",
      "Course materials"
    ]
  },
  {
    "objectID": "materials/course-materials.html#introductory-module",
    "href": "materials/course-materials.html#introductory-module",
    "title": "Course materials",
    "section": "",
    "text": "Objectives: set expectations; explore data science raison d’etre; introduce systems and design thinking; introduce software tools and collaborative coding; conduct exploratory/descriptive analysis of class background and interests.\n\n\n\nThursday meeting: Course orientation [slides]\nAssignments due by next class meeting:\n\ninstall course software and create github account;\nfill out intake form\nread Peng and Parker (2022)\nprepare a reading reading response\n\n\n\n\n\n\nTuesday meeting: On projects in(volving) data science [slides]\nSection meeting: software and technology overview [activity] Teams spreadsheet\nThursday meeting: basic GitHub actions [activity] [slides]\nAssignments due by next class meeting:\n\nread MDSR 9.1 and 9.2\nprepare a reading response\n\n\n\n\n\n\nTuesday meeting: Introducing class intake survey data [slides]\nSection meeting: tidyverse basics [activity]\nThursday meeting: planning group work for analysis of survey data [slides]\nAssignments:\n\nfirst team assignment due Friday, October 18, 11:59 PM PST [accept via GH classroom here]Teams spreadsheet",
    "crumbs": [
      "PSTAT197A",
      "Course materials"
    ]
  },
  {
    "objectID": "materials/slides/week0-intro.html#before-we-begin",
    "href": "materials/slides/week0-intro.html#before-we-begin",
    "title": "Course orientation",
    "section": "Before we begin…",
    "text": "Before we begin…\n\nIdentify the row number you are sitting.\nPlease sign in using the attendance reporting form found here:\nAttendance reporting form"
  },
  {
    "objectID": "materials/slides/week0-intro.html#welcome",
    "href": "materials/slides/week0-intro.html#welcome",
    "title": "Course orientation",
    "section": "Welcome",
    "text": "Welcome\nPSTAT197A/CMPSC190DD is the first course in UCSB’s year-long data science capstone sequence.\n\nAudience: undergraduate students of any discipline with a basic background in data science and an interest in research\nAim: prepare for an independent research or project experience"
  },
  {
    "objectID": "materials/slides/week0-intro.html#capstone-projects",
    "href": "materials/slides/week0-intro.html#capstone-projects",
    "title": "Course orientation",
    "section": "Capstone projects",
    "text": "Capstone projects\nMost students are preparing for capstone projects in winter and spring. Course foci were chosen with this in mind.\n\nProjects are varied ➜ emphasize problem patterns over methodology\nProjects are collaborative ➜ emphasize teamwork and discussion\nProjects are specialized ➜ practice independent learning based on use cases\n\nRead about past projects at https://centralcoastdatascience.org/projects"
  },
  {
    "objectID": "materials/slides/week0-intro.html#outcomes",
    "href": "materials/slides/week0-intro.html#outcomes",
    "title": "Course orientation",
    "section": "Outcomes",
    "text": "Outcomes\nI hope to support all of you in:\n\nusing modern software with version control for collaboration;\nrecognizing problem patterns based on data semantics and research questions;\nidentifying and accessing resources for independent learning given a problem of interest;\ncommunicating data analysis and/or research findings."
  },
  {
    "objectID": "materials/slides/week0-intro.html#classroom-environment",
    "href": "materials/slides/week0-intro.html#classroom-environment",
    "title": "Course orientation",
    "section": "Classroom environment",
    "text": "Classroom environment\n\nLet’s acknowledge:\n\nPreparations and areas of expertise vary widely among the class\nIt’s okay not to know things\nIf you have a question, probably someone else does too"
  },
  {
    "objectID": "materials/slides/week0-intro.html#modules",
    "href": "materials/slides/week0-intro.html#modules",
    "title": "Course orientation",
    "section": "Modules",
    "text": "Modules\nThe course is configured in modules defined by a dataset and questions (much like a project).\nA module typically comprises:\n\nOne session on data introduction (lecture/discussion)\nTwo sessions on problem patterns and related methodology (lecture)\nTwo labs with related examples (section meeting)\nOne session on sharing data analysis results (discussion)"
  },
  {
    "objectID": "materials/slides/week0-intro.html#module-content",
    "href": "materials/slides/week0-intro.html#module-content",
    "title": "Course orientation",
    "section": "Module content",
    "text": "Module content\nThe module datasets are currently as follows:\n\nClass intake survey data (exploratory/descriptive analysis)\nBiomarkers of autism (predictive modeling and variable selection)\nWeb fraud (text processing and deep learning)\nSoil temperatures (correlated data)"
  },
  {
    "objectID": "materials/slides/week0-intro.html#group-assignments",
    "href": "materials/slides/week0-intro.html#group-assignments",
    "title": "Course orientation",
    "section": "Group assignments",
    "text": "Group assignments\nEach module you will be assigned a working group.\nYour group’s objective is to produce an analysis of the dataset:\n\nReproduce analysis presented/discussed in class meeting\nExtend the analysis by\n\napplying an alternative method that addresses the same question(s)\nor addressing a corollary question"
  },
  {
    "objectID": "materials/slides/week0-intro.html#vignettes",
    "href": "materials/slides/week0-intro.html#vignettes",
    "title": "Course orientation",
    "section": "Vignettes",
    "text": "Vignettes\nAt the end of the class in place of a fifth module you will create a vignette (short demonstration) on a topic of interest.\n\npresent a use case\nexplain methodology\ndemonstrate implementation with example code"
  },
  {
    "objectID": "materials/slides/week0-intro.html#expectations-and-assessments",
    "href": "materials/slides/week0-intro.html#expectations-and-assessments",
    "title": "Course orientation",
    "section": "Expectations and assessments",
    "text": "Expectations and assessments\n\n\nStudents are expected to:\n\nprepare for class meetings as directed;\nattend and actively participate in class and section meetings;\ncontribute meaningfully to group activities and assignments.\n\n\nStudents are assessed on:\n\nattendance, preparation, and participation;\nquality of submitted work;\nindividual contributions to group assignments;\noral interview/presentation."
  },
  {
    "objectID": "materials/slides/week0-intro.html#next-time",
    "href": "materials/slides/week0-intro.html#next-time",
    "title": "Course orientation",
    "section": "Next time",
    "text": "Next time\nWe’ll discuss:\n\ndata science as a discipline;\nthe research landscape;\nsystems and design thinking for data science."
  },
  {
    "objectID": "materials/slides/week0-intro.html#checklist",
    "href": "materials/slides/week0-intro.html#checklist",
    "title": "Course orientation",
    "section": "Checklist",
    "text": "Checklist\nComplete all of the following before our next meeting.\n\nReview all content in the about section of the course webpage.\nInstall course software and create a GitHub account.\nFill out capstone project intake form.\nRead Peng, R. D., & Parker, H. S. (2022). Perspective on data science. Annual Review of Statistics and Its Application, 9, 1-20. (access online via UCSB library).\nPrepare a reading response."
  },
  {
    "objectID": "pstat197bc/about-winter-meetings.html",
    "href": "pstat197bc/about-winter-meetings.html",
    "title": "Winter class meetings",
    "section": "",
    "text": "[Attendance reporting form] – fill out once per class meeting.\n[Drive folder for winter presentations] – upload your slides and handouts here.\n[Presentation review form] – fill out once per presentation."
  },
  {
    "objectID": "pstat197bc/about-winter-meetings.html#useful-links",
    "href": "pstat197bc/about-winter-meetings.html#useful-links",
    "title": "Winter class meetings",
    "section": "",
    "text": "[Attendance reporting form] – fill out once per class meeting.\n[Drive folder for winter presentations] – upload your slides and handouts here.\n[Presentation review form] – fill out once per presentation."
  },
  {
    "objectID": "pstat197bc/about-winter-meetings.html#week-2",
    "href": "pstat197bc/about-winter-meetings.html#week-2",
    "title": "Winter class meetings",
    "section": "Week 2",
    "text": "Week 2\n\nComments: teams should be scheduling their initial project meetings this week to take place during week 3.\nMonday meeting: course overview [slides]\nClass assignments:\n\nread syllabus\nprepare a motivational statement and [submit] by Friday 1/19 11:59pm PST\nprepare a team contract and submit signed copy by Friday 1/26 11:59pm PST [upload to Google drive]"
  },
  {
    "objectID": "pstat197bc/about-winter-meetings.html#week-3",
    "href": "pstat197bc/about-winter-meetings.html#week-3",
    "title": "Winter class meetings",
    "section": "Week 3",
    "text": "Week 3\n\nComments: re-read project abstract before your team meeting.\nMonday meeting: getting started and tips for success [slides]"
  },
  {
    "objectID": "pstat197bc/about-winter-meetings.html#week-4",
    "href": "pstat197bc/about-winter-meetings.html#week-4",
    "title": "Winter class meetings",
    "section": "Week 4",
    "text": "Week 4\nComments: teams should have data in hand this week; presentation schedule updated; see meeting notes template\n\nMonday meeting: Meeting on Monday will be run as Office Hours\nIn-class presentations to begin next week; should focus on project background, data, goals, and current/planned work\n\nformat: 20min presentation + 10min discussion\nto prepare: 10-15 slides and 1-2 page handout 24h in advance\n[handout template] [evaluation rubric] [last year’s presentations]"
  },
  {
    "objectID": "pstat197bc/about-winter-meetings.html#week-5",
    "href": "pstat197bc/about-winter-meetings.html#week-5",
    "title": "Winter class meetings",
    "section": "Week 5",
    "text": "Week 5\n\nMonday meeting: Appfolio, HPC Workshop"
  },
  {
    "objectID": "pstat197bc/about-winter-meetings.html#week-6",
    "href": "pstat197bc/about-winter-meetings.html#week-6",
    "title": "Winter class meetings",
    "section": "Week 6",
    "text": "Week 6\n\nMonday meeting: CCBER, CITRAL"
  },
  {
    "objectID": "pstat197bc/about-winter-meetings.html#week-7",
    "href": "pstat197bc/about-winter-meetings.html#week-7",
    "title": "Winter class meetings",
    "section": "Week 7",
    "text": "Week 7\n\nWednesday meeting: Geography Department, Nation Builder"
  },
  {
    "objectID": "pstat197bc/about-winter-meetings.html#week-8",
    "href": "pstat197bc/about-winter-meetings.html#week-8",
    "title": "Winter class meetings",
    "section": "Week 8",
    "text": "Week 8\n\nMonday meeting: P3, Political Science"
  },
  {
    "objectID": "pstat197bc/about-winter-meetings.html#week-9",
    "href": "pstat197bc/about-winter-meetings.html#week-9",
    "title": "Winter class meetings",
    "section": "Week 9",
    "text": "Week 9\n\nMonday meeting: Scripps, SLAC"
  },
  {
    "objectID": "pstat197bc/individual-report-guidelines.html",
    "href": "pstat197bc/individual-report-guidelines.html",
    "title": "Individual project summary guidelines",
    "section": "",
    "text": "The purpose of this assignment is to compose a short project summary of your own authorship that, together with your poster, could comprise a work sample to supplement employment or graduate program applications."
  },
  {
    "objectID": "pstat197bc/individual-report-guidelines.html#instructions",
    "href": "pstat197bc/individual-report-guidelines.html#instructions",
    "title": "Individual project summary guidelines",
    "section": "Instructions",
    "text": "Instructions\nPrepare a 1-3 page (single- or double-spaced) project summary highlighting\n\nResearch objectives and findings\nProject outcomes\nYour contributions\n\nThink of the document as an extended abstract. You do not need to go in-depth on methods and results, though these aspects of your work should be mentioned."
  },
  {
    "objectID": "pstat197bc/individual-report-guidelines.html#suggested-format",
    "href": "pstat197bc/individual-report-guidelines.html#suggested-format",
    "title": "Individual project summary guidelines",
    "section": "Suggested format",
    "text": "Suggested format\nYou can arrange the document as you please and as suits your preferences and project, but as a starting point, a recommended general format is outlined below.\nFront matter. At the top of your document, include a title, an abstract, and a statement of your role or area of contribution. Assume your audience may only read this portion in detail and skim the rest of the document, so you want to be sure that based on this alone they can ascertain: (i) what the project is about; (ii) the main achievements; and (iii) your role on the team.\nProject overview. Provide a brief introduction to the research questions, and indicate the data and methodology. Describe your main findings (one paragraph per finding is suggested) and include figures. Write this in an academic style.\nProject outcomes. Indicate key outputs/outcomes – outputs include your poster presentation and any deliverables rendered to your sponsor, such as additional internal presentations or software products. If applicable, you can mention other anticipated outputs such as manuscripts in preparation, but only if there is a concrete plan for making that happen – don’t include a wish list here. If possible, include citations/links.\nPersonal contributions. Detail which parts of the project work you were primarily responsible for. This may include specific analyses (e.g., “I worked primarily on developing and implementing the cluster analysis arm of the project”) or specific roles (e.g., “I managed code repositories and acted as primary responsible for integrating team members’ work into a cohesive body”).\nSupplemental information. If there is any additional material you feel is important to include or mention as part of your work sample that is not covered by the rest of the document or the poster, you can include that here."
  },
  {
    "objectID": "pstat197bc/individual-report-guidelines.html#evaluation",
    "href": "pstat197bc/individual-report-guidelines.html#evaluation",
    "title": "Individual project summary guidelines",
    "section": "Evaluation",
    "text": "Evaluation\nYour summary will be evaluated based primarily on clarity of writing, in particular:\n\neffectiveness as a summary of research\nclarity of personal contributions to the project"
  },
  {
    "objectID": "pstat197bc/work-in-progress-presentation-guidelines.html",
    "href": "pstat197bc/work-in-progress-presentation-guidelines.html",
    "title": "Work-in-progress presentations",
    "section": "",
    "text": "These work-in-progress presentations are intended to provide:\nWork-in-progress presentations should not aim to provide a broad overview of the project, but instead focus on the leading edge of your (not your teammates’) efforts. So, your goal as presenter should be to identify the most pressing problem you’re currently working on resolving, bring your audience up to speed on that problem, and pose a few specific questions you’d like the group to discuss with you."
  },
  {
    "objectID": "pstat197bc/work-in-progress-presentation-guidelines.html#content",
    "href": "pstat197bc/work-in-progress-presentation-guidelines.html#content",
    "title": "Work-in-progress presentations",
    "section": "Content",
    "text": "Content\nYou will have 15 minutes to present the problem you’re currently working on. In this time, you should cover the following points with roughly the suggested time allocations:\n\n(1min) remind the group which project you’re working on and what the overarching goal is\n(2min) identify the specific problem you’re working on addressing right now\n(10min) explain the problem in detail, filling in background information, data attributes, relevant aspects of software design, prior results, etc., as needed as you go\n(2min) pose a few specific questions you’d like to discuss or want advice on\n\nFollowing your presentation, your group will have 15 minutes for discussion (see agenda below).\n\nThink of the presentation less in terms of performing … and more in terms of getting the help you need from others to make further progress. In that spirit, make sure you allow time to present the leading edge of your work. That means you need to be economical in how you get listeners up to steam about the aspects of your project that you already have firmly in place.\n\nYou should aim to cover exactly the material the audience needs to be in an informed position relative to the questions you pose at the end."
  },
  {
    "objectID": "pstat197bc/work-in-progress-presentation-guidelines.html#agenda",
    "href": "pstat197bc/work-in-progress-presentation-guidelines.html#agenda",
    "title": "Work-in-progress presentations",
    "section": "Agenda",
    "text": "Agenda\nFor each presentation, the group should follow this agenda:\n\n(15min) Work-in-progress presentation\n(5min) As a group, do the following:\n\nMake sure the audience is all on the same page: are there any aspects of the presentation that need to be clarified? Details that someone at the group wasn’t able to follow? Technical terms that need definition?\nSelect 2 audience members to reflect the presenter’s problem back – have them restate the problem in their own words. Ask the presenter: did they understand the problem correctly?\n\n\nWe will take a 5 minute break between presentations each class meeting."
  },
  {
    "objectID": "about/outcomes.html",
    "href": "about/outcomes.html",
    "title": "Learning outcomes",
    "section": "",
    "text": "This course emphasizes collaborative, interactive, and hands-on learning. Instruction in PSTAT197A will support all students in:\n\nusing modern technology and version control to collaborate efficiently on programming for data science projects;\nrecognizing and articulating problem patterns based on data semantics and one or more research questions;\nidentifying and accessing resources to aid in learning independently about methodology and/or application domains pertinent to a problem of interest;\ncommunicating data analysis and/or research findings in a project team setting and to a small audience of peers.\n\nCourse staff are committed to creating an inclusive learning environment. Data science involves a combination of computing, statistics and probability, and domain expertise, as well as use of technology and narrative communication and storytelling, and no one person should expect to be an expert in all of these areas. Course staff recognize this fact that core competencies vary considerably, acknowledge that each student has particular strengths and weaknesses and interests, and make their best effort to avoid promoting one skill set over others in the practice of data science.",
    "crumbs": [
      "PSTAT197A",
      "Learning outcomes"
    ]
  },
  {
    "objectID": "about/syllabus.html",
    "href": "about/syllabus.html",
    "title": "Course syllabus",
    "section": "",
    "text": "Concurrent course listing: PSTAT197A and CMPSC190DD are held concurrently; enrollment is by instructor consent and admitted students may enroll under either listing. The course content, expectations, assessments, and course policies are identical for students enrolled in either course.\nCatalog description: Introduction to research skills. Discussion of current research trends, writing literature reviews, etc. Students will be required to present materials reflecting their interests, which will be critically appraised for both content and presentation. Emphasis will be placed on aiding students to acquire a high-level of professionalism. Prerequisite: PSTAT126.",
    "crumbs": [
      "PSTAT197A",
      "Course syllabus"
    ]
  },
  {
    "objectID": "about/syllabus.html#meetings",
    "href": "about/syllabus.html#meetings",
    "title": "Course syllabus",
    "section": "Meetings",
    "text": "Meetings\nClass meetings are held 2pm – 3:15pm Tuesday and Thursday in South Hall 1431.\nSection meetings are held on Tuesdays:\n\n2pm – 2:50pm in Interactive Learning Pavilion 4105;\n3pm – 3:50pm in Interactive Learning Pavilion 3312;\n4pm – 4:50pm in Phelps Hall 1530.",
    "crumbs": [
      "PSTAT197A",
      "Course syllabus"
    ]
  },
  {
    "objectID": "about/syllabus.html#staff",
    "href": "about/syllabus.html#staff",
    "title": "Course syllabus",
    "section": "Staff",
    "text": "Staff\nInstructors:\n\nLaura Baracaldo. Visiting assistant professor and co-instructor for 2024-2025 capstone projects.\nKathleen Coburn. Lecturer and co-instructor for 2024-2025 capstone projects.\n\nTeaching assistants:\n\nErika McPhillips. PhD student and capstone project mentor in 2024-2025.\nJeffrey Wu. PhD student and capstone project mentor in 2024-2025.",
    "crumbs": [
      "PSTAT197A",
      "Course syllabus"
    ]
  },
  {
    "objectID": "about/syllabus.html#expectations-and-assessments",
    "href": "about/syllabus.html#expectations-and-assessments",
    "title": "Course syllabus",
    "section": "Expectations and assessments",
    "text": "Expectations and assessments\nMuch of the course is designed around group activity and discussion. Students are therefore expected to:\n\nprepare for class meetings in advance by completing any assigned reading or activity;\nattend and actively participate in class meetings and section meetings;\nprovide meaningful, timely, and concrete contributions to group activities.\n\nStudents having any difficulty in meeting these expectations should raise the issue(s) promptly with the instructor.\nQualitative feedback is emphasized over numerical scores. Students are assessed on:\n\nattendance, preparation, and participation;\nquality of submitted work;\nindividual contributions to group assignments;\noral interview.",
    "crumbs": [
      "PSTAT197A",
      "Course syllabus"
    ]
  },
  {
    "objectID": "about/syllabus.html#policies",
    "href": "about/syllabus.html#policies",
    "title": "Course syllabus",
    "section": "Policies",
    "text": "Policies\nAttendance. Regular attendance is expected. Each student can miss two sessions without notice; further absences may impact course grades. Students are responsible for material discussed in their absence and should review posted session notes and consult a classmate.\nDeadlines. Students are expected to meet assignment deadlines in a timely manner. All deadlines have a 24-hour grace period. Late or amended work may not be accepted.\nEmail. Course staff will make their best effort to reply to email within 48 weekday hours. However, due to high volume, staff cannot guarantee that all messages will receive replies.\nIllness. Students who are ill are required to stay home. Students ill with COVID-19 must comply with university policy regarding reporting and quarantine. Accommodations will be made to ensure that students absent due to illness do not fall behind.\nAccommodations. Reasonable accommodations will be made for any student with a qualifying disability. Such requests should be made through the Disabled Students Program (DSP). More information, instructions on how to access accommodations, and information on related resources can be found on the DSP website. Note: in this class there are no timed assessments.\nLetter grades. Letter grades are assigned based only on the assessments identified above and according to university guidelines, with the relative weighting of assessments determined at the discretion of the instructor. While grade calculations will not be disclosed, students are entitled to an explanation of the criteria used to determine their grades if desired. Grades will not be changed except in the case of clerical errors. If students feel their grade has been unfairly assigned, they are entitled to contest it following UCSB procedure for contesting grades.\nConduct. All course participants are expected to maintain respectful and honorable conduct consistent with UCSB ethical standards. Students uncomfortable with the behavior of another course participant for any reason should notify the instructor, course staff, or, if the complaint relates to course staff conduct, an administrative or departmental officer. Evidence of academic dishonesty will be reported to the Office of Student Conduct (OSC); evidence of problematic behavior will be addressed on a case-by-case basis in accord with university policies.",
    "crumbs": [
      "PSTAT197A",
      "Course syllabus"
    ]
  },
  {
    "objectID": "about/schedule.html",
    "href": "about/schedule.html",
    "title": "Course schedule",
    "section": "",
    "text": "This schedule is tentative and may be adjusted at the discretion of the instructor. Check back for updates.\n\n\n\n\n\n\n\n\n\n\nWeek\nTheme\nMonday meeting\nWednesday meeting\nSection meeting\n\n\n\n\n0\nModule 0: Introductions\nNO CLASS\nCourse orientation\nNO LAB\n\n\n1\nModule 0: Introductions\n0.1 Lecture:\n\non research projects in(volving) data science\n\n0.2 Activity:\n\ncollaboration using GitHub\n\nSoftware and technology overview\n\n\n2\nModule 0: Introductions\n0.3 Lecture/discussion:\n\nintroducing class survey data\n\n0.4 Activity:\n\nexploratory and descriptive analysis\n\ntidyverse\n\n\n3\nModule 1: biomarkers\n1.1 Discussion/lecture:\n\nsharing results of survey data analysis;\nintroducing biomarker data\n\n1.2 Lecture:\n\non prediction\n\ntidymodels\n\n\n4\nModule 1: biomarkers\n1.3 Lecture:\n\non classification\n\n1.4 Lecture/discussion:\n\non variable selection;\nreview published analysis of biomarker data\n\nclassification\n\n\n5\nModule 2: web fraud\n2.1 Lecture/discussion:\n\nsharing analysis of soil temperature data;\nintroducing web fraud data\n\n2.2 Lecture:\n\non text as data\n\ntext processing\n\n\n6\nModule 2: web fraud\n2.3 Lecture:\n\non multiclass classification\n\n2.4 Activity:\n\nmeasuring classification accuracy\n\nkeras\n\n\n7\nModule 3: soil temperature\n3.1 Discussion/lecture:\n\nsharing results of biomarker analysis;\nintroducing soil temperature data\n\n3.2 Lecture:\n\non time\n\ntime series analysis\n\n\n8\nModule 3: soil temperature\n3.3 Lecture:\n\non space\n\n3.4 Discussion: results\nspatial analysis\n\n\n9\nModule 4: vignettes\n4.1 Activity:\n\nworkshopping vignettes\n\nNO CLASS\nNO LAB\n\n\n10\nModule 4: vignettes\n4.2 Activity:\n\nteaching exchange\n\n4.3 Activity/discussion:\n\nteaching exchange;\nclosing\n\nNO LAB"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#announcementsreminders",
    "href": "materials/slides/week1-perspectives.html#announcementsreminders",
    "title": "On data science",
    "section": "Announcements/reminders",
    "text": "Announcements/reminders\n\nJoin Slack workspace, monitor channel #F25-pstat197a for announcements.\nInstall course software and bring your laptop to section meetings."
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#google-trends-data-science",
    "href": "materials/slides/week1-perspectives.html#google-trends-data-science",
    "title": "On data science",
    "section": "Google trends: data science",
    "text": "Google trends: data science\n\n\nData science emerged as a term of art in the last decade\nInterest exploded in the last five years"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#origins-data-analysis",
    "href": "materials/slides/week1-perspectives.html#origins-data-analysis",
    "title": "On data science",
    "section": "Origins: ‘data analysis’",
    "text": "Origins: ‘data analysis’\nTukey advocated for ‘data analysis’ as a broader field than statistics (Tukey 1962), including:\n\nstatistical theory and methodology;\nvisualization and data display techniques;\ncomputation and scalability;\nbreadth of application.\n\n\nLook famililar? Tukey’s ‘data analysis’ is proto-modern data science."
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#early-data-analysis-concepts",
    "href": "materials/slides/week1-perspectives.html#early-data-analysis-concepts",
    "title": "On data science",
    "section": "Early data analysis concepts",
    "text": "Early data analysis concepts\nIn the 1960’s and 1970’s, these concepts meant very different things.\n\nvisualization meant drawing\ncomputation meant data re-expression by hand\n\n\nBut the ideas were still somewhat radical. At the time most relied on highly reductive numerical results to interpret data:\n\nANOVA tables\nregression tables\np-values"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#example-boxplots",
    "href": "materials/slides/week1-perspectives.html#example-boxplots",
    "title": "On data science",
    "section": "Example: boxplots",
    "text": "Example: boxplots\n\nFigure from (Tukey et al. 1977)"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#early-data-analysis-concepts-1",
    "href": "materials/slides/week1-perspectives.html#early-data-analysis-concepts-1",
    "title": "On data science",
    "section": "Early data analysis concepts",
    "text": "Early data analysis concepts\nThe new techniques allowed for iterative investigation:\n\nformulate a question\nexamine data graphics and summaries\nadjust computations and graphics to hone in on content of interest\nrefine the question"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#birth-to-death-ratio-by-state",
    "href": "materials/slides/week1-perspectives.html#birth-to-death-ratio-by-state",
    "title": "On data science",
    "section": "Birth-to-death ratio by state",
    "text": "Birth-to-death ratio by state\nSuppose we want to explain variation in birth-to-death ratios in the U.S. 1\n\nInitial question: is population density an associated factor?\n\nThis example follows (Tukey et al. 1977)"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#first-iteration",
    "href": "materials/slides/week1-perspectives.html#first-iteration",
    "title": "On data science",
    "section": "First iteration",
    "text": "First iteration\n\nA first attempt"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#first-iteration-1",
    "href": "materials/slides/week1-perspectives.html#first-iteration-1",
    "title": "On data science",
    "section": "First iteration",
    "text": "First iteration\n\nWhat if we adjust the computation?"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#second-iteration",
    "href": "materials/slides/week1-perspectives.html#second-iteration",
    "title": "On data science",
    "section": "Second iteration",
    "text": "Second iteration\n\nWhat about median age instead?"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#second-iteration-1",
    "href": "materials/slides/week1-perspectives.html#second-iteration-1",
    "title": "On data science",
    "section": "Second iteration",
    "text": "Second iteration\n\nAdjust computations for easy linear approximation"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#third-iteration",
    "href": "materials/slides/week1-perspectives.html#third-iteration",
    "title": "On data science",
    "section": "Third iteration",
    "text": "Third iteration\n\nAre there outliers?"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#fourth-iteration",
    "href": "materials/slides/week1-perspectives.html#fourth-iteration",
    "title": "On data science",
    "section": "Fourth iteration",
    "text": "Fourth iteration\n\nAre outliers spatially correlated?"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#a-bit-of-history",
    "href": "materials/slides/week1-perspectives.html#a-bit-of-history",
    "title": "On data science",
    "section": "A bit of history",
    "text": "A bit of history\nIt’s worth noting that in the first half of the 20th century, much of statistics focused on methodology and theory for the analysis of small iid samples, and in particular:\n\ninference on means and inference on tables;\nanalysis of variance;\ntests of distribution.\n\n\nThe inferential framework brought to bear on these ‘simpler’ problems largely carried over when the field began to specialize."
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#contrasting-approaches",
    "href": "materials/slides/week1-perspectives.html#contrasting-approaches",
    "title": "On data science",
    "section": "Contrasting approaches",
    "text": "Contrasting approaches\nFrom 1960-2010, adopters of the ‘data analysis as a field’ view were largely industry practitioners and applied statisticians who advocated for training and practice that included empirical methods and computation in addition to statistical inference (Donoho 2017).\n\nTheir ideas evolved into an alternative approach to working with data:\n\ndata-driven rather than theory-driven;\niterative rather than conclusive."
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#confirmatory-approach",
    "href": "materials/slides/week1-perspectives.html#confirmatory-approach",
    "title": "On data science",
    "section": "Confirmatory approach",
    "text": "Confirmatory approach\nThe “confirmatory” approach of the classical inferential framework.\n\n\n\n\n\n\n\nconfirm\n\n\ncluster_1\n\ndata generation\n\n\ncluster_2\n\ndata analysis\n\n\ncluster_3\n\ndecision\n\n\n\nsci\n\ndomain \nknowledge\n\n\n\nhyp\n\nhypotheses\n\n\n\nsci-&gt;hyp\n\n\n\n\n\nexp\n\ndesigned \nexperiment\n\n\n\nhyp-&gt;exp\n\n\n\n\n\nmdl\n\nstatistical \nmodel\n\n\n\nexp-&gt;mdl\n\n\n\n\n\ndat\n\ndata\n\n\n\nexp-&gt;dat\n\n\n\n\n\nyay\n\nsupporting \nevidence\n\n\n\nmdl-&gt;yay\n\n\n\n\n\nnay\n\nopposing \nevidence\n\n\n\nmdl-&gt;nay\n\n\n\n\n\ndat-&gt;yay\n\n\n\n\n\ndat-&gt;nay\n\n\n\n\n\n\n\n\n\n\n\noutput is a decision\nstatistical model determined by experimental design\nanalysis based on statistical theory"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#exploratory-approach",
    "href": "materials/slides/week1-perspectives.html#exploratory-approach",
    "title": "On data science",
    "section": "Exploratory approach",
    "text": "Exploratory approach\nThe “exploratory” approach of iterative modern data analysis.\n\n\n\n\n\n\n\nexplore\n\n\ncluster_2\n\nfindings\n\n\ncluster_1\n\ndata analysis\n\n\n\nsci\n\ndomain \nknowledge\n\n\n\nq\n\nquestion \nformulation\n\n\n\nsci-&gt;q\n\n\n\n\n\ndat\n\ndata\n\n\n\nq-&gt;dat\n\n\n\n\n\nmdl\n\nstatistical \nmodel\n\n\n\ndat-&gt;mdl\n\n\n\n\n\nmdl-&gt;q\n\n\n\n\n\nf1\n\nfinding 1\n\n\n\nmdl-&gt;f1\n\n\n\n\n\nf2\n\nfinding 2\n\n\n\nmdl-&gt;f2\n\n\n\n\n\ndots\n\n⋮\n\n\n\n\n\n\n\n\n\noutputs are findings\nstatistical model determined by data\nanalysis techniques include empirical methods"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#drivers-of-change",
    "href": "materials/slides/week1-perspectives.html#drivers-of-change",
    "title": "On data science",
    "section": "Drivers of change",
    "text": "Drivers of change\nIn the 2000s and especially after 2010, the iterative approach enjoys broader applicability than it used to:\n\ndue to automated and/or scalable data collection\n\nobservational data is widely available across domains\nand includes large numbers of variables\n\nhighly specialized data problems evade methodology with theoretical support\nmore accessible to analysts without advanced statistical training"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#machine-learning",
    "href": "materials/slides/week1-perspectives.html#machine-learning",
    "title": "On data science",
    "section": "Machine learning",
    "text": "Machine learning\nMachine learning was largely advanced by computer scientists through 2010 and later (Emmert-Streib et al. 2020), most notably:\n\nneural networks and deep learning\noptimization\nalgorithmic analysis\n\n\nThis was a major driver in advancing modern predictive modeling, and engaging with these tools required going beyond statistics."
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#a-theory-about-data-science",
    "href": "materials/slides/week1-perspectives.html#a-theory-about-data-science",
    "title": "On data science",
    "section": "A theory about data science",
    "text": "A theory about data science\n\nAround mid-century, it was proposed that specialists should be trained in computational as well as statistical methods\nOver time practitioners developed iterative processes for data-driven problem solving that was more flexible than the classical inferential framework\nComputer scientists advanced the field of machine learning substantially\nIterative problem solving together with applied machine learning was well-suited to meet the demands of modern data, but the area was not codified in an academic discipline"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#what-is-research",
    "href": "materials/slides/week1-perspectives.html#what-is-research",
    "title": "On data science",
    "section": "What is research?",
    "text": "What is research?\nResearch is systematic investigation undertaken in order to establish or discover facts.\n\nWhat are facts in data science?\n\nmethod M outperforms method M’ at task T\nwe analyzed data D and reached the conclusion that…"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#the-research-landscape",
    "href": "materials/slides/week1-perspectives.html#the-research-landscape",
    "title": "On data science",
    "section": "The research landscape",
    "text": "The research landscape\nFormal communities – i.e., journals, departments, conferences – have not coalesced around data science research to date.\n\nRelevant research largely occurs in statistics, computer science, and application domains, and can be divided broadly into:\n\nmethodology – creating new techniques to analyze data\napplications – applying existing methods to generate new findings"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#methodological-research",
    "href": "materials/slides/week1-perspectives.html#methodological-research",
    "title": "On data science",
    "section": "Methodological research",
    "text": "Methodological research\nMethodological research might involve:\n\ndesigning a faster algorithm for solving a particular problem\nproposing a new technique for analyzing a particular type of data\ngeneralizing a technique to a broader range of problems"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#applied-research",
    "href": "materials/slides/week1-perspectives.html#applied-research",
    "title": "On data science",
    "section": "Applied research",
    "text": "Applied research\nApplied research might involve:\n\nanalyzing a specific dataset or producing a novel analysis of existing data\ncreating ad-hoc methods for a domain-specific problem\nimporting methodology from another area to bear on a domain-specific problem"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#data-science-capstones",
    "href": "materials/slides/week1-perspectives.html#data-science-capstones",
    "title": "On data science",
    "section": "Data science capstones",
    "text": "Data science capstones\nMost of the time, our data science capstones fall pretty squarely in the applied domain:\n\nsponsor provides data and high-level goals\nstudent team works on producing an analysis or analyses\nmentor advises on methodology"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#relevant-skills",
    "href": "materials/slides/week1-perspectives.html#relevant-skills",
    "title": "On data science",
    "section": "Relevant skills",
    "text": "Relevant skills\nThere are a few avenues to prepare for this sort of work.\n\nWe’ll focus on:\n\nrecognizing problem patterns\ndeveloping a functional view of methodology\ncollaborating efficiently\nindependent learning strategies\nengaging with literature constructively\n\n\n\nIt won’t provide you with exhaustive methodological preparation, but should support you in learning ‘on the job’."
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#reading-responses",
    "href": "materials/slides/week1-perspectives.html#reading-responses",
    "title": "On data science",
    "section": "Reading responses",
    "text": "Reading responses\nQuestions on the perspectives paper (Peng and Parker 2022) to review:\n\nWhat is meant by a ‘systems approach’ to data science?\nWhat is meant by ‘design thinking’ in data science?\n(Why) Are these useful concepts?"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#systems-approach",
    "href": "materials/slides/week1-perspectives.html#systems-approach",
    "title": "On data science",
    "section": "Systems approach",
    "text": "Systems approach\n\n\n\n\nSeveral systems affect the relationship between expected and actual results. Where would you locate them on the figure?\n\nData analytic\nSoftware\nScientific"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#example-systems-for-data-cleaning",
    "href": "materials/slides/week1-perspectives.html#example-systems-for-data-cleaning",
    "title": "On data science",
    "section": "Example systems for data cleaning",
    "text": "Example systems for data cleaning\n\n\nHow might this diagram help an analyst?"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#design-thinking",
    "href": "materials/slides/week1-perspectives.html#design-thinking",
    "title": "On data science",
    "section": "Design thinking",
    "text": "Design thinking\nThe design thinking framework might be summed up:\n\ndata scientists trade in data analyses\na data analysis is a designed product\nthinking about design principles can help make a better product\n\n\nMany of you focused on how design principles are a response to project constraints. Are there other ways a design perspective might be useful?"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#scenario-1",
    "href": "materials/slides/week1-perspectives.html#scenario-1",
    "title": "On data science",
    "section": "Scenario 1",
    "text": "Scenario 1\nYou’re working at a news organization and developing a recommender system for targeted article previews to deploy on the organization’s website. It will show users article previews based on their behavior. Assume you don’t have any significant resource constraints, and can access users’ profiles in full and log interactions in near-real-time.\n\nGoal: show previews most likely to attract interest.\n\n\nConsiderations:\n\nwhat material should be shown in the preview? headlines? images? text?\nwhat behavior can/should be leveraged for the recommender system?\nwhat are a few relevant design aspects of how the system should behave?\nare there ethical concerns?"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#scenario-2",
    "href": "materials/slides/week1-perspectives.html#scenario-2",
    "title": "On data science",
    "section": "Scenario 2",
    "text": "Scenario 2\nYou’re working on a research team studying ecological impacts of land use. The team has access to longitudinal species surveys at locations of interest across the U.S., quarterly county-level land allocation statistics, satellite images, and state budget information for sustainability, restoration, and conservation initiatives.\n\nGoal: identify intervention opportunities that are most likely to positively impact ecological diversity.\n\n\nConsiderations:\n\nwhat data would you use and how would you combine data sources?\nare there external data that might be useful?\nwhat analysis outputs would be most important for identifying intervention opportunities?\ncan you think of other design features that might be useful for the data analysis?"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#a-few-design-principles",
    "href": "materials/slides/week1-perspectives.html#a-few-design-principles",
    "title": "On data science",
    "section": "A few design principles",
    "text": "A few design principles\nLet’s look at some design principles from (McGowan, Peng, and Hicks 2021)."
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#design-principles-matchedness",
    "href": "materials/slides/week1-perspectives.html#design-principles-matchedness",
    "title": "On data science",
    "section": "Design principles: matchedness",
    "text": "Design principles: matchedness"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#design-principles-exhuastiveness",
    "href": "materials/slides/week1-perspectives.html#design-principles-exhuastiveness",
    "title": "On data science",
    "section": "Design principles: exhuastiveness",
    "text": "Design principles: exhuastiveness"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#design-principles-transparency",
    "href": "materials/slides/week1-perspectives.html#design-principles-transparency",
    "title": "On data science",
    "section": "Design principles: transparency",
    "text": "Design principles: transparency"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#design-principles-reproducibility",
    "href": "materials/slides/week1-perspectives.html#design-principles-reproducibility",
    "title": "On data science",
    "section": "Design principles: reproducibility",
    "text": "Design principles: reproducibility"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#next-time",
    "href": "materials/slides/week1-perspectives.html#next-time",
    "title": "On data science",
    "section": "Next time",
    "text": "Next time\nWe’ll do a github icebreaker activity.\n\nComplete lab activity from Wednesday section meeting\nBring laptops"
  },
  {
    "objectID": "materials/slides/week1-perspectives.html#references",
    "href": "materials/slides/week1-perspectives.html#references",
    "title": "On data science",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\nDonoho, David. 2017. “50 Years of Data Science.” Journal of Computational and Graphical Statistics 26 (4): 745–66.\n\n\nEmmert-Streib, Frank, Zhen Yang, Han Feng, Shailesh Tripathi, and Matthias Dehmer. 2020. “An Introductory Review of Deep Learning for Prediction Models with Big Data.” Frontiers in Artificial Intelligence 3: 4.\n\n\nMcGowan, Lucy D’Agostino, Roger D Peng, and Stephanie C Hicks. 2021. “Design Principles for Data Analysis.” arXiv Preprint arXiv:2103.05689.\n\n\nPeng, Roger D, and Hilary S Parker. 2022. “Perspective on Data Science.” Annual Review of Statistics and Its Application 9: 1–20.\n\n\nTukey, John W. 1962. “The Future of Data Analysis.” The Annals of Mathematical Statistics 33 (1): 1–67.\n\n\nTukey, John W et al. 1977. Exploratory Data Analysis. Vol. 2. Reading, MA."
  },
  {
    "objectID": "materials/activities/github-basics.html",
    "href": "materials/activities/github-basics.html",
    "title": "GitHub basics",
    "section": "",
    "text": "This activity introduces GitHub repositories and basic Git actions; students will be expected to use these skills to access materials and complete assignments.\nObjectives:\nPrerequisites: completion of lab 1, particularly cloning the group sandbox repository."
  },
  {
    "objectID": "materials/activities/github-basics.html#why-are-we-using-git-and-github",
    "href": "materials/activities/github-basics.html#why-are-we-using-git-and-github",
    "title": "GitHub basics",
    "section": "Why are we using Git and GitHub?",
    "text": "Why are we using Git and GitHub?\nVersion control has many benefits, including the ability to track changes and contributions precisely, work in parallel with other contributors, revert to prior versions of files, keep track of issues, quickly share and disseminate work, and solicit user contributions from the coding public. Arguably, for all of these reasons and because of its widespread use, Git/GitHub is a must for data scientists.\nIn this class you’ll learn and practice some basics that will allow you to easily access course files, collaborate with each other, and efficiently submit your coursework. This should equip you to utilize a repository for efficient collaboration with your peers on your capstone project."
  },
  {
    "objectID": "materials/activities/github-basics.html#basic-workflow",
    "href": "materials/activities/github-basics.html#basic-workflow",
    "title": "GitHub basics",
    "section": "Basic workflow",
    "text": "Basic workflow\nIf I am working out of a repository and want to alter a file and make those changes available to anyone else accessing my repository, most of the time I need to:\n\ncreate/update local copies of repository files on my laptop;\nmake the desired change(s) locally;\nsend the changes back to the remote repository.\n\nTypically these steps are performed iteratively as work progresses – they are a basic workflow.\nWorkflow can be understood as a sequence of Git actions: actions that modify the repository files and/or metadata. The most basic sequence that accomplishes the above steps is:\n\ngit pull update the local repository (technically, fetch changes + merge changes from the remote repository);\ngit add stage file changes to be committed to the local repository;\ngit commit commit staged changes to the local repository;\ngit push send committed changes back to the remote repository.\n\nSometimes contributors take different or additional actions; the complexity of the Git actions required to make a change depends largely on repository settings, permissions, and agreements among collaborators about how workflow should be structured."
  },
  {
    "objectID": "materials/activities/github-basics.html#basic-git-actions",
    "href": "materials/activities/github-basics.html#basic-git-actions",
    "title": "GitHub basics",
    "section": "Basic Git actions",
    "text": "Basic Git actions\nHere you’ll make a local change and then push that change to the remote repository.\n\nPull\nThe first step to making a change is ensuring you have the most up-to-date version of the repository files.\n\n\n\n\n\n\nAction (individual)\n\n\n\nPull changes from the remote repository.\n\nIn your GitHub client, open the group sandbox repository and then look for a ‘Pull’ menu item.\nIf you are using GitHub desktop, you can alternatively ‘fetch origin’ first via a toolbar button. This will retrieve changes but without modifying local files, and if changes are detected, a button will appear in the main screen of the client to pull changes.\nIn the terminal: navigate to the root directory of the repository and git pull\n\n\n\nNow check the repository history to see what changes you just pulled. In GitHub Desktop, there is a history tab on the left-hand side that lists commits chronologically. Select a commit to view line-by-line differences for every file that was altered.\nYou should see two changes: that there is now a class-activity folder containing a copy of this activity; and the README file has been updated. Look at the differences on the readme file.\n\n\n\n\n\n\nRemark\n\n\n\nFetching vs. pulling\nFetching allows you to retrieve changes from the remote repository without merging them into your local repository. If there are commits that you haven’t merged, you can examine them before doing so in one of two ways:\n\nin the terminal, git diff main origin/main\nopen the remote repository on github.com and check the commit history (look for a clock icon with the number of commits in the upper right corner of the file navigator in the code menu); open any commit to see a line-by-line comparison of differences.\n\n\n\n\n\nMake changes\nNow that you have the most up-to-date version of all files, create a new markdown file in the class activity folder with a fun fact about you (or anything else if you’d rather) that you’ll upload to the repository.\n\n\n\n\n\n\nAction (individual)\n\n\n\nCreate a markdown file:\n\nIn RStudio, select File &gt; New File &gt; Markdown File\nAdd an ‘About Me’ or similar header (use one or more hashes # before the header text)\nWrite a fun fact about yourself\nSave the file as YOURGITHUBUSERNAME-about.md in the class activity folder\n\n\n\n\n\nStage and commit changes\nNow that your new file is ready to go you can stage the changes to be committed to the repository and create a commit.\nA commit is a bundle of changes that will be submitted to the repository along with a message briefly explaining the changes made. Your GitHub client will often fill in a default message such as ‘update FILENAME.EXT’.\n\n\n\n\n\n\nAction (individual)\n\n\n\nStage and commit:\n\nIn your client, look for a menu item to add or stage changes. By default any changes made to any file will be included. In GitHub Desktop, look for the ‘Changes’ menu next to ‘History’; you can stage changes by simply selecting or unselecting the checkbox next to each file that was altered.\n\nOr in the terminal: git add FILENAME\n\nOnce you have staged changes, look for a menu item to commit changes. Add a message and commit the changes. In GitHub Desktop, this appears at the bottom of the ‘Changes’ menu.\n\nOr in the terminal: git commit -m \"your message here\"\n\n\n\n\nOften these actions are performed together. However, in some workflows it may make sense to stage changes incrementally and create commits that bundle several changes at once. For example, if you need to make an update that requires modifying files A, B, and C, it may make sense to edit and stage changes to A first, followed by B, followed by C, and create the commit only once the full update has been implemented.\n\n\nPush\nThe last step is to push your commit to the remote repository. However, as you will see in a moment, too many people trying to push changes at once can create some problems.\n\n\n\n\n\n\nAction (group)\n\n\n\n\nChoose one person in your team to push their changes. The very first person to do this will have no problems, since their local repository is up to date with the remote.\nThen choose someone else to try. Since the first person modified the remote repository, the next person to push changes will no longer be up to date. Git will detect this and the push won’t go through.\nHave the second person update their local repository by pulling changes, and then try the push again. It should go through once their local is up to date with the remote.\nHave everyone in your team pull changes but do not push any additional commits.\n\n\n\nSo far everyone is working on independent files and there’s no overlap between changes, so although it would be a bit of a hassle to have everyone check for changes every time they push, in principle it could be done. However, there is a more efficient way to work in parallel: by creating branches."
  },
  {
    "objectID": "materials/activities/github-basics.html#branching",
    "href": "materials/activities/github-basics.html#branching",
    "title": "GitHub basics",
    "section": "Branching",
    "text": "Branching\nInspect your GitHub client closely, and note that you are currently on the ‘main’ branch of the repository. Think of this as the primary version of the repository. Branches allow contributors to create parallel versions of the repository that they can modify for development purposes while leaving the primary version unaffected.\n\nCreate a branch\nHere you’ll use branches to avoid stepping on each others’ toes while pushing your team’s remaining commits. The strategy will be to create a personal branch, push your commit to that branch, and then merge the branch back into the main branch of the repository.\n\n\n\n\n\n\nAction (individual)\n\n\n\nCreate a branch and push your previous commit:\n\nIn your GitHub client, look for a menu item to create and switch to a new branch.\nName your branch your GitHub username.\nCheck to see that you are currently on your personal branch.\nPush your previous commit. You shouldn’t have to repeat any of the previous steps, but you can if need be.\n\nIf you were one of the two who pushed their commit to main, make some small change to your file to push to your personal branch.\n\n\n\n\nAccess your neighbor’s branch\nWhile often the main purpose of branching is to create a version of the repository that only you will modify, contributors can inspect any branch of the repository. This can be useful for sharing ideas or getting input or help.\n\n\n\n\n\n\nAction (in pairs)\n\n\n\nMake a commit to your neighbor’s branch\n\nFind out your neighbor’s (to your right) username and switch to their branch in your GitHub client.\nIn RStudio, verify that you are on their branch by executing git status in the terminal.\nOpen their markdown file, ask them a simple question about themselves (nothing too personal, please), and add the information to their markdown file.\nStage, commit, and push the change.\nWhen your neighbor (to your left) has done the same with you, switch back to your own branch in your GitHub client and pull changes."
  },
  {
    "objectID": "materials/activities/github-basics.html#pull-requests",
    "href": "materials/activities/github-basics.html#pull-requests",
    "title": "GitHub basics",
    "section": "Pull requests",
    "text": "Pull requests\nOnce you are ready to integrate changes you’ve developed on a branch you can open a pull request to merge the development branch with the main branch. (Technically, pull requests can be opened between any two branches, so could also be used, for example, to update your branch if the main branch has new commits.)\n‘Pull request’ is a bit of an odd term; think of it as you making a request that your collaborators pull your changes for review.\n\n\n\n\n\n\nAction (individual)\n\n\n\nOpen a pull request:\n\nIn your GitHub client, find a menu item for opening a pull request. GitHub Desktop will simply redirect you to github.com to open the request.\nSpecify the pull request from your branch to the main branch and submit.\n\n\n\nOnce a pull request is opened, usually a collaborator with maintain privileges must be the one to merge changes and close the request. However, the rules for this depend on repository settings. For this repository, all contributors can merge and close pull requests.\n\n\n\n\n\n\nAction (pairs)\n\n\n\n\nOpen the repository in the browser. Navigate to pull requests.\nFind your neighbor’s (to your right) pull request; merge their changes and close the request. Then delete the branch.\n\n\n\nOnce everyone in your team is finished, examine the repository and verify that everyone’s markdown file is present on the main branch. Then have each contributor pull changes and check that they see the same."
  },
  {
    "objectID": "materials/activities/github-basics.html#merge-conflicts",
    "href": "materials/activities/github-basics.html#merge-conflicts",
    "title": "GitHub basics",
    "section": "Merge conflicts",
    "text": "Merge conflicts\nGit is pretty clever at merging changes when you pull, push, or merge branches via pull request. However, occasionally commits will conflict in such a way that can’t be resolved automatically. These are known as merge conflicts.\nMerge conflicts happen when:\n\ntwo commits differ on the same line of the same file;\nfiles are moved or deleted in conflicting ways.\n\nHere you’ll create an artificial merge conflict to see what this looks like and how to fix it.\n\n\n\n\n\n\nAction (group)\n\n\n\nCreate a merge conflict\n\nHave someone in your team open the README file and add the group members’ names in a list on one line, e.g.,\ngroup: Laura Baracaldo, Erika MchPillips\nCommit and push changes\nThen have someone, without pulling new changes, create a commit with the names shown differently somehow, such as last, first, or initials, or spanning multiple lines with one name per line.\nAttempt to push the commit. Your client will detect ‘upstream’ changes on the remote repository and prompt you to pull changes.\nAttempt to pull the changes. The client will then report a merge conflict and prompt you to resolve the conflict and commit changes. GitHub Desktop in particular will prompt you to open RStudio to resolve the conflict. Go ahead and follow the prompt.\n\nResolve a merge conflict\nYou will see a version of the file with the conflict that shows &lt;&lt;&lt;&lt;HEAD … &gt;&gt;&gt;&gt; followed by a long alphanumeric string. Within the angle brackets the two conflicting versions of the file will be shown, separated by ===== .\n\nAgree with your team on one version of the README file (or another representation of your names).\nCommit and push the change.\n\n\n\nWhen detected, merge conflicts must be resolved with a commit that takes precedence over the conflicting commits. You can read more about resolving merge conflicts here."
  },
  {
    "objectID": "materials/activities/github-basics.html#checklist",
    "href": "materials/activities/github-basics.html#checklist",
    "title": "GitHub basics",
    "section": "Checklist",
    "text": "Checklist\n\nOn github.com, your group-sandbox repository has a directory called class-activity containing a copy of this activity and one markdown file for each group member with two fun facts about them.\nThe repository has only one open branch.\nThe README file lists each group member’s name.\nEach group member has an up-to-date local copy of the repository."
  },
  {
    "objectID": "materials/labs/lab1-setup/lab1-setup.html",
    "href": "materials/labs/lab1-setup/lab1-setup.html",
    "title": "Course technology overview",
    "section": "",
    "text": "Read this and complete all instructions in the ‘action’ boxes during your lab section. Your TA will walk you through the activity and help to troubleshoot issues and answer any questions along the way.\nObjectives:"
  },
  {
    "objectID": "materials/labs/lab1-setup/lab1-setup.html#prerequisites",
    "href": "materials/labs/lab1-setup/lab1-setup.html#prerequisites",
    "title": "Course technology overview",
    "section": "Prerequisites",
    "text": "Prerequisites\n\nTo complete the activity you’ll need to:\n\nhave all of the software listed on the course technology page installed;\nfind (or create) your GitHub account credentials (if you are creating an account for the first time, see advice on choosing a username).\n\n\n\n\n\n\n\nAction\n\n\n\nPreparations:\n\nLog in to your GitHub account.\nOpen your GitHub client.\nOpen a new session in RStudio.\nCreate a class folder for PSTAT197 somewhere on your machine, e.g., ~/documents/pstat197."
  },
  {
    "objectID": "materials/labs/lab1-setup/lab1-setup.html#rstudio-projects",
    "href": "materials/labs/lab1-setup/lab1-setup.html#rstudio-projects",
    "title": "Course technology overview",
    "section": "RStudio projects",
    "text": "RStudio projects\nFirst we’ll get acquainted with the basic functionality of the RStudio IDE and the use of projects as a means of organizing files. If you’ve already used RStudio, great – this will still serve to introduce you to how we’ll use RStudio projects in this class.\n\nRStudio Setup\n\nYour TA will briefly review the (default) layout of the RStudio IDE. You should be able to identify/find the following:\n\nconsole\nterminal\nfile navigator\nenvironment\nhistory\n\nWe’ll use several R packages throughout the quarter. Some of these we will install on the go, but we can install several that we’ll rely on now.\n\n\n\n\n\n\nAction\n\n\n\nInstall packages\nNavigate to the console and copy-paste the following commands. You only need to do this once. This will take a minute or two to complete.\n\n\n\n# package install list \nurl &lt;- 'https://raw.githubusercontent.com/PSTAT197-F23/pstat197a/main/materials/scripts/package-installs.R'\nsource(url)\n\n\n# clear environment\nrm(list = ls())\n\n\n\nCreate a local project\nProjects are a means of keeping your work organized. When you create a project in a directory on your local machine, RStudio keeps track of project metadata, history, and the working environment so that every time you open the project you see whatever you had open when you last closed it.\n\n\n\n\n\n\nAction\n\n\n\nCreate a new project:\n\nSelect File &gt; New project\nCreate the project in a new directory as a subdirectory of your class folder\nName it example-project\n\nComment: when naming files it’s good practice to avoid spaces, special characters, and the like. A naming convention we try to follow: choose a descriptive name comprising 1-3 words or common abbreviations separated by hyphens.\n\n\nTake a moment to observe the file navigator. It should consist of a single example-project.Rproj file.\n\n\nAdd content\nWe may as well populate the project with a few files – so let’s add a dataset and write a short script, as if we’re just starting a data analysis.\n\n\n\n\n\n\nAction\n\n\n\nRetrieve data and store a local copy\n\nOpen a new script: File &gt; New File &gt; R Script\nIn the navigator, create a folder called data and a folder called scripts\nCopy and paste the code chunk below into your script.\nExecute once, then save in the scripts folder as data-retrieval.R and close\n\n\n\n\nlibrary(tidyverse)\n\n# retrieve pollution data\nurl &lt;- 'https://raw.githubusercontent.com/PSTAT197-F23/pstat197a/main/materials/labs/lab1-setup/data/pollution.csv'\npollution &lt;- read_csv(url)\n\n# write as csv to file\nwrite_csv(pollution, file = 'data/pollution.csv')\n\n# clear environment\nrm(list = ls())\n\n\nNext, we’ll do a simple regression analysis.\n\n\n\n\n\n\nAction\n\n\n\nCreate a script\n\nCreate a new script as before\nCopy-paste the code chunk below into your script\nExecute once and examine the results\nSave in the scripts folder as slr-analysis.R\n\n\n\n\nlibrary(tidyverse)\n\n# load data\npollution &lt;- read_csv('data/pollution.csv')\n\n# examine scatterplot with SLR fit\nggplot(pollution,\n       aes(x = log(SO2), y = Mort)) +\n  geom_point() +\n  geom_smooth(method = 'lm')\n\n\n\n\n\n\n\n# compute SLR fit\nfit &lt;- lm(Mort ~ log(SO2), data = pollution)\nbroom::tidy(fit)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    887.      17.6      50.4  1.37e-49\n2 log(SO2)        16.7      4.99      3.35 1.40e- 3\n\n# interpret\nfit_ci &lt;- confint(fit, parm = 'log(SO2)')*log(1.2)\n\npaste('With 95% confidence, every 20% increase in sulfur dioxide pollution is associated with an increase in two-year mortality rate between', \n      round(fit_ci[1], 2), \n      'and', \n      round(fit_ci[2], 2), \n      'per 100k', sep = ' ') %&gt;% \n  print()\n\n[1] \"With 95% confidence, every 20% increase in sulfur dioxide pollution is associated with an increase in two-year mortality rate between 1.23 and 4.87 per 100k\"\n\n\n\nCongrats on your first project! You can close the RStudio session now.\nWe’ll be using projects structured much like what you just set up, but with one catch: we’ll link up our RStudio projects with shared repositories so that we can all collaborate on the same set of project files."
  },
  {
    "objectID": "materials/labs/lab1-setup/lab1-setup.html#github-repositories",
    "href": "materials/labs/lab1-setup/lab1-setup.html#github-repositories",
    "title": "Course technology overview",
    "section": "GitHub repositories",
    "text": "GitHub repositories\n\nWe will be distributing course assignments as repositories via GitHub Classroom. A repository is simply a storage space.\nHere we’ll walk you through how to access and copy the files in a repository just as you will for course assignments. The first step is to accept an assignment through a link we’ve given to you – this will create a repository for you with the files we intend for you to have.\nFor now, we’ll make a ‘group sandbox’ that you can play in during our next class meeting.\n\n\n\n\n\n\nAction\n\n\n\nAccept an assignment in GitHub Classroom\n\nFollow the link to accept the group-sandbox assignment. Since it’s a group assignment, you will be prompted to join/create a team.\nJoin/create a team. Groups should have 3-5 members.\n\nYou should be directed to a team repository on github.com. You may need to refresh your browser. Keep this window open; you will need the URL.\n\n\n\nGit and GitHub\nAt some point in time – possibly quite recently – you had to install Git on your local machine, as well as create a GitHub account. So, Git and GitHub are two different things.\nGit is version control software that enables you to systematically track and control file changes within a repository – a collection of files possibly with some directory structure. (The definition of ‘repository’ is simply ‘storage place’.)\nGitHub is an online platform for hosting repositories remotely. Anyone with access to a repository can make changes to files in the repository, and this enables multiple people to collaborate on code.\n\n\nlocal &lt;&gt; remote\nUsually remote repositories are not updated directly because contributors need to execute codes to test their changes and the remote server that hosts the repository is not equipped to do this.\nInstead, contributors will prepare changes on their own machine where they can test them, and then update the remote repository once their changes are complete.\nThis process of implementing file changes in a repository involves communicating information between local and remote locations. For this purpose a local copy of the remote repository is needed.\n\n\nCloning a repository\nIn Git lingo, a clone is a local copy of a remote repository. Creating a clone copies files and establishes the link between local and remote repositories so that changes can be sent to and received from the remote repository. You only need to create a clone once.\nTo clone a repository, all one needs is:\n\nthe remote location URL;\nthe local destination where the clone will be created;\npermission from the repository owner, if private.\n\nHere you’ll clone the group sandbox repository you just created/joined. You will need the URL; if you happened to close the page when you accepted the assignment earlier, you should be able to find the repository from your home page on github.com.\n\n\n\n\n\n\n\nAction\n\n\n\nClone the sandbox repository:\n\nOpen your GitHub client (GitKracken or GitHub Desktop or similar) and ensure you are logged in to your GitHub account.\nLook for a ‘Clone Repo’ menu item or similar and simply input the URL and the place you’d like to clone it; proceed through any prompts.\nCheck your file navigator to confirm that the repository files were copied.\n\n\n\n\nAn alternative possibility is to create the clone using a terminal command. In the terminal, navigate to the desired destination, and input:\ngit clone https://github.com/USERNAME/REPONAME\n\n\n\n\n\n\nRemarks\n\n\n\nOn terminal commands:\n\nIt’s recommended to manage Git actions through a visual client, as it’s much easier to see and understand what’s happening.\nHowever, if you know exactly what you’re doing, executing simple actions via Git bash in the terminal can be more efficient at times.\nFor example, you can keep a terminal open in RStudio and manage your repository workflow from there, without having to toggle between environments.\nTry experimenting with terminal commands from RStudio after you have a little experience with basic Git actions."
  },
  {
    "objectID": "materials/labs/lab1-setup/lab1-setup.html#checklist",
    "href": "materials/labs/lab1-setup/lab1-setup.html#checklist",
    "title": "Course technology overview",
    "section": "Checklist",
    "text": "Checklist\nHave you completed all of the activity action items?\n\nInstall software: R, RStudio, Git, and a GitHub client\nCreate a GitHub account\nInstall R packages that will be used frequently\nCreate a local project in RStudio\nAccept the group sandbox assignment on GitHub Classroom\nClone the group sandbox repo"
  },
  {
    "objectID": "materials/slides/week2-workshop.html#today",
    "href": "materials/slides/week2-workshop.html#today",
    "title": "Group assignment workshops",
    "section": "Today",
    "text": "Today\n\nsetup for first group assignment due next Friday (10/18)\n\nassignment objectives and instructions\nreview repository\n\nworkshop ideas and plan tasks in groups"
  },
  {
    "objectID": "materials/slides/week2-workshop.html#assignment-objective",
    "href": "materials/slides/week2-workshop.html#assignment-objective",
    "title": "Group assignment workshops",
    "section": "Assignment objective",
    "text": "Assignment objective\nYour task: prepare and present a descriptive analysis of the survey responses addressing 2-3 questions or goals of your choosing.\n\nquestions or goals should be of moderate complexity; easy to state and understand but should require a little work to answer\n\ntoo simple: what proportion of students have research experience?\nbetter: are students with research experience more confident/comfortable with technical skills than students without, considering coursework history?"
  },
  {
    "objectID": "materials/slides/week2-workshop.html#outcomes",
    "href": "materials/slides/week2-workshop.html#outcomes",
    "title": "Group assignment workshops",
    "section": "Outcomes",
    "text": "Outcomes\nThe learning outcomes for this assignment are less focused on methodology:\n\nformulate questions and plan a simple analysis\npractice using a GitHub repo in a team project setting\nprepare a report\nlearn one or more new-to-you data manipulation techniques"
  },
  {
    "objectID": "materials/slides/week2-workshop.html#assignment-set-up",
    "href": "materials/slides/week2-workshop.html#assignment-set-up",
    "title": "Group assignment workshops",
    "section": "Assignment set-up",
    "text": "Assignment set-up\n\naccept GH classroom assignment here; this will create/add your team repo\nclone repo to local\nreview repo contents:\n\ndata with survey responses and metadata\nscripts with preprocessing and in-class analysis\nresults with report template for preparing write-up\nREADME.md with assignment instructions"
  },
  {
    "objectID": "materials/slides/week2-workshop.html#deliverable",
    "href": "materials/slides/week2-workshop.html#deliverable",
    "title": "Group assignment workshops",
    "section": "Deliverable",
    "text": "Deliverable\nYour ‘submission’ will be in the form of commits to the group repository, in particular:\n\nan updated results/report.qmd file containing your write-up source\na rendered results/report.html file\n\n\nPlease make final commits by Friday, October 18, 11:59pm PST.\n\n\nRecall there is a one-hour grace period; any commits submitted after Saturday 1:00am will not receive review."
  },
  {
    "objectID": "materials/slides/week2-workshop.html#resources",
    "href": "materials/slides/week2-workshop.html#resources",
    "title": "Group assignment workshops",
    "section": "Resources",
    "text": "Resources\n\nlab 2 and in-class analysis scripts/week2-surveys.R\nslack and course staff OH\nMDSR ch. 4 on data wrangling\nRStudio cheatsheets and tidyverse documentation, esp. dplyr, tidyr, ggplot2"
  },
  {
    "objectID": "materials/slides/week2-workshop.html#workshopping-today",
    "href": "materials/slides/week2-workshop.html#workshopping-today",
    "title": "Group assignment workshops",
    "section": "Workshopping today",
    "text": "Workshopping today\nTry to accomplish three goals:\n\npool ideas for questions or themes to explore in the data\npair up, divide work, and assign tasks\nagree on a communication plan for finishing work\n\nslack groupchat or similar\nmeeting outside of class sometime"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#today",
    "href": "materials/slides/week2-classdata.html#today",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Today",
    "text": "Today\n\nreview sampling concepts\nintroduce class survey data\npresent descriptive analysis"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#samples-and-populations",
    "href": "materials/slides/week2-classdata.html#samples-and-populations",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Samples and populations",
    "text": "Samples and populations\npopulation: collection of all subjects/units of interest\nsample: subjects/units observed in a study\n\nstatistical methodology strives to account for the possibility that the sample could have been different in order to make reliable inferences about the population based on knowledge of the sampling mechanism"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#what-if-inferences-arent-possible",
    "href": "materials/slides/week2-classdata.html#what-if-inferences-arent-possible",
    "title": "Sampling concepts and descriptive analysis",
    "section": "What if inferences aren’t possible?",
    "text": "What if inferences aren’t possible?\nEven if inference isn’t possible, data still have value and could be used for:\n\ndescriptive analysis of the sample;\nhypothesis generation;\ndeveloping analysis pipelines."
  },
  {
    "objectID": "materials/slides/week2-classdata.html#what-about-prediction",
    "href": "materials/slides/week2-classdata.html#what-about-prediction",
    "title": "Sampling concepts and descriptive analysis",
    "section": "What about prediction?",
    "text": "What about prediction?\nPrediction is a separate goal but still a form of generalization.\n\nsamples must reflect a broader population for predictions to be accurate at the population level\n\n\n\nif an analyst can’t expect sample statistics to provide reliable estimates of population quantities, they shouldn’t expect predictions based on the sample to be reliable either"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#common-problems",
    "href": "materials/slides/week2-classdata.html#common-problems",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Common problems",
    "text": "Common problems\nSeveral issues arise very often in practice that compromise or complicate an analyst’s ability to make inferences (or predictions). Among them:\n\nscope of inference from the sample doesn’t match the study population\nsubjects/units are selected haphazardly or by convenience\nresearcher conflates sample size with number of observations, i.e., takes lots of measurements on few subjects/units"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#helpful-questions",
    "href": "materials/slides/week2-classdata.html#helpful-questions",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Helpful questions",
    "text": "Helpful questions\nThe following questions can help make an assessment of the scope of inference:\n\n(protocol) how were subjects/units chosen for measurement and how were measurements collected?\n(mechanism) was there any random selection mechanism?\n(exclusion) are there any subjects/units that couldn’t possibly have been chosen?\n(nonresponse) were any subjects/units selected but not measured?"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#class-survey-data",
    "href": "materials/slides/week2-classdata.html#class-survey-data",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Class survey data",
    "text": "Class survey data\n\nsurvey distributed to all students offered enrollment in PSTAT197A fall 2024\n\\(n = 43\\) responses\n\nmay include a few students who did not enroll\ndoes not include several students who did enroll\n\nno random selection"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#can-the-data-support-inference",
    "href": "materials/slides/week2-classdata.html#can-the-data-support-inference",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Can the data support inference?",
    "text": "Can the data support inference?\nFrom the reading responses:\n\nIt depends on the question. If you want to draw conclusions about the pstat197a class specifically, this sample is the population and thus will have reliable data. If you want to draw conclusions about the pstat department as a whole, then this is a bad sample because it is likely biased and thus unreliable"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#alternative-perspectives",
    "href": "materials/slides/week2-classdata.html#alternative-perspectives",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Alternative perspectives",
    "text": "Alternative perspectives\nThe comment points to two ways to view the data:\n\na census of PSTAT197A enrollees\na convenience sample of…\n\ncapstone applicants OR\nstudents qualified for capstones OR\nstudents interested in data science OR\nall UCSB students???"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#is-there-a-right-answer",
    "href": "materials/slides/week2-classdata.html#is-there-a-right-answer",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Is there a right answer?",
    "text": "Is there a right answer?\nEither way – census or convenience sample – excludes inference.\n\ncensus \\(\\longrightarrow\\) no inference needed\nconvenience \\(\\longrightarrow\\) no inference possible\n\n\nSo on a practical level, it won’t make much difference for designing an analysis of the survey data."
  },
  {
    "objectID": "materials/slides/week2-classdata.html#descriptive-analysis",
    "href": "materials/slides/week2-classdata.html#descriptive-analysis",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Descriptive analysis",
    "text": "Descriptive analysis\nAny analysis of survey data should be regarded as descriptive in nature:\n\nsummary statistics and/or models are not reliable measures of any broader population\nresults should be interpreted narrowly in terms of the sample at hand"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#a-general-approach",
    "href": "materials/slides/week2-classdata.html#a-general-approach",
    "title": "Sampling concepts and descriptive analysis",
    "section": "A general approach",
    "text": "A general approach\nStart simple and add complexity gradually.\nFrom simpler to more complex consider questions involving:\n\nSample characteristics\nSingle-variable summaries\nMultivariate summaries\nModel-based outputs (estimates, predictions, etc.)"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#questions-of-interest",
    "href": "materials/slides/week2-classdata.html#questions-of-interest",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Questions of interest",
    "text": "Questions of interest\nSample characteristics\n\nIs the proportion of men/women in the class equal (taking into account randomness)?\n\nSingle-variable summaries\n\nAmong the students offered a seat in PSTAT197, what data science classes are the students most interested in?\nWhat level of comfort do students interested in data analysis at UCSB have with mathematics?"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#questions-of-interest-1",
    "href": "materials/slides/week2-classdata.html#questions-of-interest-1",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Questions of interest",
    "text": "Questions of interest\nMultivariate summaries\n\nAre students who ranked themselves as strong in statistics, mathematics, and computing more likely or less likely to select an ‘industry’ project as the project type that they want to work on?\n\nModel-based outputs\n\nAre there distinct groups of students in the class defined by self-assessed proficiencies and/or comfort levels with mathematics, statistics, and programming?"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#sample-characteristics",
    "href": "materials/slides/week2-classdata.html#sample-characteristics",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Sample characteristics",
    "text": "Sample characteristics\nIs the proportion of men/women in the class equal (taking into account randomness)?\n\nClass standingGenderRaceData sharing\n\n\n\n\n\n\n\nstanding\nn\n\n\n\n\nJunior\n2\n\n\nSenior\n41\n\n\n\n\n\n\n\n\n\n\n\n\ngender\nn\n\n\n\n\nFemale\n18\n\n\nMale\n25\n\n\n\n\n\n\n\n\n\n\n\n\nrace\nn\n\n\n\n\nAsian\n26\n\n\nBlack\n1\n\n\nCaucasian\n11\n\n\nMultiracial\n3\n\n\nPrefer not to say\n2\n\n\n\n\n\n\n\nconsentprof: consent to share project preferences\nconsentback: consent to share background and preparation\n\n\n\n\n\nconsentprof\nconsentback\nn\n\n\n\n\nNo\nNo\n2\n\n\nYes\nNo\n2\n\n\nYes\nYes\n39"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#courses",
    "href": "materials/slides/week2-classdata.html#courses",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Courses",
    "text": "Courses"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#privacy",
    "href": "materials/slides/week2-classdata.html#privacy",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Privacy",
    "text": "Privacy\nThe following information have been removed from the dataset that will be distributed to the class:\n\npersonal information from section 1 of the survey\nlong text and free response answers, contain some personal details\nresponses from students who did not consent to share\ntype distinction between research experiences"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#single-variable-summaries",
    "href": "materials/slides/week2-classdata.html#single-variable-summaries",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Single-variable summaries",
    "text": "Single-variable summaries\nWhat level of comfort do students interested in data analysis at UCSB have with mathematics?\n\nComfortProficiency (numeric)Proficiency (factor)\n\n\n\n\n\n\n\nvariable\nmax\nmean\nmedian\nmin\n\n\n\n\nmath.comf\n5\n3.847458\n4\n2\n\n\nprog.comf\n5\n3.966102\n4\n3\n\n\nstat.comf\n5\n4.084746\n4\n2\n\n\n\n\n\n\n\n\n\n\n\n\nvariable\nmean\nmedian\n\n\n\n\nmath\n2.355932\n2\n\n\nprog\n2.237288\n2\n\n\nstat\n2.576271\n3\n\n\n\n\n\n\n\n\n\n\n\n\nprog\nn1\nmath\nn2\nstat\nn3\n\n\n\n\nBeg\n3\nBeg\n3\nBeg\n2\n\n\nInt\n39\nInt\n32\nInt\n21\n\n\nAdv\n17\nAdv\n24\nAdv\n36"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#multivariable-summaries",
    "href": "materials/slides/week2-classdata.html#multivariable-summaries",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Multivariable summaries",
    "text": "Multivariable summaries\nAre students who ranked themselves as strong in statistics, mathematics, and computing more likely or less likely to select an ‘industry’ project as the project type that they want to work on?\n\nCountsProportions\n\n\n\n\n\n\n\nmean.proficiency.fac\nboth\nind\nlab\n\n\n\n\n[1,2.33]\n6\n25\n1\n\n\n(2.33,2.67]\n5\n9\n1\n\n\n(2.67,3]\n3\n6\n1\n\n\n\n\n\n\n\n\n\n\n\n\nmean.proficiency.fac\nboth\nind\nlab\nn\n\n\n\n\n[1,2.33]\n0.188\n0.781\n0.031\n32\n\n\n(2.33,2.67]\n0.333\n0.600\n0.067\n15\n\n\n(2.67,3]\n0.300\n0.600\n0.100\n10"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#combinations",
    "href": "materials/slides/week2-classdata.html#combinations",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Combinations",
    "text": "Combinations\nConsider the distinct combinations of comfort and proficiency ratings (separately):\n\nProficiencyComfort\n\n\n\n\n\n\n\nprog\nmath\nstat\nn\n\n\n\n\n1\n1\n1\n1\n\n\n1\n2\n2\n1\n\n\n1\n2\n3\n1\n\n\n2\n1\n1\n1\n\n\n2\n1\n2\n1\n\n\n2\n2\n2\n13\n\n\n2\n2\n3\n11\n\n\n2\n3\n2\n3\n\n\n2\n3\n3\n10\n\n\n3\n2\n2\n2\n\n\n3\n2\n3\n4\n\n\n3\n3\n2\n1\n\n\n3\n3\n3\n10\n\n\n\n\n\n\n\n\n\n\n\n\nprog\nmath\nstat\nn\n\n\n\n\n3\n2\n2\n1\n\n\n3\n3\n3\n2\n\n\n3\n3\n4\n2\n\n\n3\n3\n5\n2\n\n\n3\n4\n3\n4\n\n\n3\n4\n4\n7\n\n\n4\n3\n3\n2\n\n\n4\n3\n4\n5\n\n\n4\n3\n5\n2\n\n\n4\n4\n3\n1\n\n\n4\n4\n4\n5\n\n\n4\n4\n5\n3\n\n\n4\n5\n3\n1\n\n\n4\n5\n4\n4\n\n\n4\n5\n5\n2\n\n\n5\n3\n4\n5\n\n\n5\n3\n5\n1\n\n\n5\n4\n4\n2\n\n\n5\n4\n5\n1\n\n\n5\n5\n4\n1\n\n\n5\n5\n5\n6"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#clustering",
    "href": "materials/slides/week2-classdata.html#clustering",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Clustering",
    "text": "Clustering\nCan students be grouped based on combinations of preferences and comfort levels?\n\nCentersVisualizationMethodInterpretation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nprog.prof\nmath.prof\nstat.prof\nprog.comf\nmath.comf\nstat.comf\nsize\ncluster\n\n\n\n\n2.478\n2.739\n2.957\n4.435\n4.522\n4.522\n23\n1\n\n\n2.048\n1.857\n2.238\n4.048\n3.048\n4.048\n21\n2\n\n\n2.133\n2.467\n2.467\n3.133\n3.933\n3.467\n15\n3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClustering method, “k means”, groups data by nearest Euclidean distance to each of \\(k\\) centers. \\(k\\) is user-specified; the method finds the centers that minimize within-cluster variance.\n\n\nBased on the centers:\n\nCluster 1: advanced proficiency, very comfortable\nCluster 2: intermediate with less mathematical preparation\nCluster 3: intermediate with less programming preparation"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#assignment",
    "href": "materials/slides/week2-classdata.html#assignment",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Assignment",
    "text": "Assignment\nYour task is to extend this analysis with your group by next Tuesday.\n\nHere are some ideas:\n\nexplore variable associations further (e.g., coursework and self-evaluations)\nexperiment with clustering on different variable subsets or using different methods\nsummarize domain or area of interest variables (requires some text manipulation)"
  },
  {
    "objectID": "materials/slides/week2-classdata.html#next-time",
    "href": "materials/slides/week2-classdata.html#next-time",
    "title": "Sampling concepts and descriptive analysis",
    "section": "Next time",
    "text": "Next time\nMost of next meeting we’ll devote to planning your group’s task.\n\nDo a little brainstorming on your own\nCome with a few questions/ideas"
  },
  {
    "objectID": "materials/labs/lab2-tidyverse/lab2-tidyverse.html",
    "href": "materials/labs/lab2-tidyverse/lab2-tidyverse.html",
    "title": "Tidyverse basics",
    "section": "",
    "text": "Read through the R Basics section and then complete all actions in the Tidyverse basics section. This lab is for your own benefit and no submission is expected.\nObjectives:"
  },
  {
    "objectID": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#data-types",
    "href": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#data-types",
    "title": "Tidyverse basics",
    "section": "Data types",
    "text": "Data types\nThere are five main data types in R.\nNumeric (double- or single-precision floating point) data represent real numbers. Numeric data are abbreviated num and by default are stored as double-precision floating point.\n\n# a number\n4.5\n\n[1] 4.5\n\n# check structure\nstr(4.5)\n\n num 4.5\n\n# stored as double\nis.double(4.5)\n\n[1] TRUE\n\n\nInteger data are integers. For the most part they behave like numeric data, except they occupy less memory, which can in some cases be convenient. To distinguish integers from doubles, R uses a trailing L after values; the data type is abbreviated int.\n\n# an integer\n4L\n\n[1] 4\n\n# check structure\nstr(4L)\n\n int 4\n\n\nLogical data are binary and represented in R as having values TRUE and FALSE. They are abbreviated logi in R. Often they are automatically coerced to integer data with values 0 (false) and 1 (true) to perform arithmetic and other operations.\n\n# logical value\nTRUE\n\n[1] TRUE\n\n# check structure\nstr(TRUE)\n\n logi TRUE\n\n# arithmetic\nTRUE + FALSE\n\n[1] 1\n\n# check structure\nstr(FALSE + FALSE)\n\n int 0\n\n\nCharacter data represent strings of text and are sometimes called ‘strings’. They are abbreviated chr in R and values are surrounded by quotation marks; this distinguishes, for example, the character 4 from the number 4. Single quotations can be used to input strings as well as double quotations. Arithmetic is not possible with strings for obvious reasons.\n\n# a character string\n'yay'\n\n[1] \"yay\"\n\n# check structure\nstr('yay')\n\n chr \"yay\"\n\n# string arithmetic won't work\n'4' + '1'\n\nError in \"4\" + \"1\": non-numeric argument to binary operator\n\n# but can be performed after coercing character to string\nas.numeric('4') + as.numeric('1')\n\n[1] 5\n\n\nFactor data represent categorical variables. In R these are encoded numerically according to the number of ‘levels’ of the factor, which represent the unique values of the categorical variable, and each level is labeled. R will print the labels, not the levels, of factors; the data type is abbreviated fct.\n\n# a factor\nfactor(1, levels = c(1, 2), labels = c('blue', 'red'))\n\n[1] blue\nLevels: blue red\n\n# less verbose definition\nfactor('blue', levels = c('blue', 'red'))\n\n[1] blue\nLevels: blue red\n\n# check structure\nstr(factor('blue', levels = c('blue', 'red')))\n\n Factor w/ 2 levels \"blue\",\"red\": 1\n\n\nUsually factors won’t be defined explicitly, but instead interpreted from character data. The levels and labels of factors can be manipulated using a variety of helper functions."
  },
  {
    "objectID": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#object-classes",
    "href": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#object-classes",
    "title": "Tidyverse basics",
    "section": "Object classes",
    "text": "Object classes\nThe most basic type of object in R is a vector. Vectors are concatenations of data values of the same type. They are defined using the concatenation operator c() and are indexed by consecutive integers; subvectors can be retrieved by specifying the indices between square brackets.\n\n# numeric vector\nc(1, 4, 7)\n\n[1] 1 4 7\n\n# character vector\nc('blue', 'red')\n\n[1] \"blue\" \"red\" \n\n# indexing\nc(1, 4, 7)[1]\n\n[1] 1\n\nc(1, 4, 7)[2]\n\n[1] 4\n\nc(1, 4, 7)[3]\n\n[1] 7\n\nc(1, 4, 7)[2:3]\n\n[1] 4 7\n\nc(1, 4, 7)[c(1, 3)]\n\n[1] 1 7\n\n\nUsually objects are assigned names for easy retrieval. Vectors will not show any special object class if the structure is examined; str() will simply return the data type, index range, and the values.\n\n# assign a name\nmy_vec &lt;- c(1, 4, 7)\n\n# check structure\nstr(my_vec)\n\n num [1:3] 1 4 7\n\n\nNext up in complexity are arrays. These are blocks of data values of the same type indexed along two or more dimensions. For arrays, str() will return the data type, index structure, and data values; when printed directly, data values are arranged according to the indexing.\n\n# an array\nmy_ary &lt;- array(data = c(1, 2, 3, 4, 5, 6, 7, 8), \n           dim = c(2, 4))\n\nmy_ary\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    3    5    7\n[2,]    2    4    6    8\n\nstr(my_ary)\n\n num [1:2, 1:4] 1 2 3 4 5 6 7 8\n\n# another array\nmy_oth_ary &lt;- array(data = c(1, 2, 3, 4, 5, 6, 7, 8), \n           dim = c(2, 2, 2))\n\nmy_oth_ary\n\n, , 1\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n, , 2\n\n     [,1] [,2]\n[1,]    5    7\n[2,]    6    8\n\nstr(my_oth_ary)\n\n num [1:2, 1:2, 1:2] 1 2 3 4 5 6 7 8\n\n\nFor arrays, elements can be retrieved by index coordinates, and slices can be retrieved by leaving index positions blank, which will return all elements along the corresponding indices.\n\n# one element\nmy_ary[1, 2]\n\n[1] 3\n\n# one element\nmy_oth_ary[1, 2, 1]\n\n[1] 3\n\n# a slice (second row)\nmy_ary[2, ]\n\n[1] 2 4 6 8\n\n# a slice (first layer)\nmy_oth_ary[ , , 1]\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n\nNext there are lists, which are perhaps the most flexible data structure. A list is an indexed collection of any objects.\n\n# a list\nlist('cat', c(1, 4, 7), TRUE)\n\n[[1]]\n[1] \"cat\"\n\n[[2]]\n[1] 1 4 7\n\n[[3]]\n[1] TRUE\n\n# a named list\nlist(animal = 'cat',\n     numbers = c(1, 4, 7),\n     short = TRUE)\n\n$animal\n[1] \"cat\"\n\n$numbers\n[1] 1 4 7\n\n$short\n[1] TRUE\n\n\nList elements can be retrieved by index in double square brackets, or by name.\n\n# assign a name\nmy_lst &lt;- list(animal = 'cat',\n               numbers = c(1, 4, 7),\n               short = TRUE)\n\n# check structure\nstr(my_lst)\n\nList of 3\n $ animal : chr \"cat\"\n $ numbers: num [1:3] 1 4 7\n $ short  : logi TRUE\n\n# retrieve an element\nmy_lst[[1]]\n\n[1] \"cat\"\n\n# equivalent\nmy_lst$animal\n\n[1] \"cat\"\n\n\nFinally, data frames are type-heterogeneous lists of vectors of equal length. More informally, they are 2D arrays with columns of differing data types. str() will essentially show the list structure; but when printed, data frames will appear arranged in a table.\n\n# a data frame\nmy_df &lt;- data.frame(animal = c('cat', 'hare', 'tortoise'),\n                    has.fur = c(TRUE, TRUE, FALSE),\n                    weight.lbs = c(9.1, 8.2, 22.7))\n\nstr(my_df)\n\n'data.frame':   3 obs. of  3 variables:\n $ animal    : chr  \"cat\" \"hare\" \"tortoise\"\n $ has.fur   : logi  TRUE TRUE FALSE\n $ weight.lbs: num  9.1 8.2 22.7\n\nmy_df\n\n    animal has.fur weight.lbs\n1      cat    TRUE        9.1\n2     hare    TRUE        8.2\n3 tortoise   FALSE       22.7\n\n\nThe data frame is the standard object type for representing datasets in R. For the most part, modern computing in R is designed around the data frame."
  },
  {
    "objectID": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#packages",
    "href": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#packages",
    "title": "Tidyverse basics",
    "section": "Packages",
    "text": "Packages\nR packages are add-ons that can include special functions, datasets, object classes, and the like. They are published software and can be installed using install.packages('PACKAGE NAME') and, once installed, loaded via library('PACKAGE NAME') or require('PACKAGE NAME')."
  },
  {
    "objectID": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#concepts",
    "href": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#concepts",
    "title": "Tidyverse basics",
    "section": "Concepts",
    "text": "Concepts\nThe tidyverse is a collection of packages for data manipulation, visualization, and statistical modeling. Some are specialized, such as forcats or lubridate, which contain functions for manipulating factors and dates and times, respectively. The packages share some common underyling principles.\n\nPackages are built around the data frame\nFunctions are designed to work with the pipe operator %&gt;%\nPackages facilitate readable code\n\nThe tidyverse facilitates programming in readable sequences of steps that are performed on dataframe. For example:\n\nmy_df %&gt;% STEP1() %&gt;% STEP2() %&gt;% STEP3()\n\nIf it helps, imagine that step 1 is defining a new variable, step 2 is selecting a subset of columns, and step 3 is fitting a model of some kind."
  },
  {
    "objectID": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#tibbles",
    "href": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#tibbles",
    "title": "Tidyverse basics",
    "section": "Tibbles",
    "text": "Tibbles\ntidyverse packages leverage a slight generalization of the data frame called a tibble. For the most part, tibbles behave as data frames do, but they are slightly more flexible in ways you’ll encounter later.\nFor now, think of a tibble as just another name for a data frame."
  },
  {
    "objectID": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#the-pipe-operator",
    "href": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#the-pipe-operator",
    "title": "Tidyverse basics",
    "section": "The pipe operator %>%",
    "text": "The pipe operator %&gt;%\nIn short, x %&gt;% f(y) is equivalent to f(x, y) .\nIn other words, the pipe operator ‘pipes’ the result of the left-hand operation into the first argument of the right-hand function.\n\n# a familiar example\nmy_vec &lt;- c(1, 2, 5) \nstr(my_vec)\n\n num [1:3] 1 2 5\n\n# use the pipe operator instead\nmy_vec %&gt;% str()\n\n num [1:3] 1 2 5"
  },
  {
    "objectID": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#dplyr-verbs",
    "href": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#dplyr-verbs",
    "title": "Tidyverse basics",
    "section": "dplyr verbs",
    "text": "dplyr verbs\nThe dplyr package contains functions for manipulating data frames (tibbles). The functions are named with verbs that describe common operations.\n\nCore verbs\n\n\n\n\n\n\nAction\n\n\n\nFor each verb listed below, copy the code chunk into your script and execute.\nGo through the list with your neighbor and check your understanding by describing what the code example accomplishes.\n\n\nfilter – filter the rows of a data frame according to a condition and return a subset of rows meeting that condition\n\n# filter rows\nbackground %&gt;%\n  filter(math.comf &gt; 3)\n\n# A tibble: 25 × 30\n   response.id prog.prof prog.comf math.prof math.comf stat.prof stat.comf\n         &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n 1           2 Adv               5 Adv               4 Adv               5\n 2           3 Int               4 Adv               5 Adv               5\n 3           4 Adv               4 Adv               4 Adv               5\n 4           5 Adv               5 Adv               5 Adv               4\n 5           6 Int               4 Adv               5 Adv               5\n 6           8 Adv               5 Adv               4 Adv               4\n 7           9 Int               4 Int               4 Adv               5\n 8          13 Int               4 Adv               5 Int               4\n 9          14 Int               4 Adv               5 Int               4\n10          15 Int               3 Adv               4 Adv               4\n# ℹ 15 more rows\n# ℹ 23 more variables: updv.num &lt;chr&gt;, dom &lt;chr&gt;, consent &lt;chr&gt;,\n#   `PSTAT120A-B` &lt;chr&gt;, PSTAT122 &lt;chr&gt;, PSTAT126 &lt;chr&gt;, PSTAT131 &lt;chr&gt;,\n#   PSTAT134 &lt;chr&gt;, `PSTAT160A-B` &lt;chr&gt;, CS9 &lt;chr&gt;, PSTAT100 &lt;chr&gt;,\n#   PSTAT174 &lt;chr&gt;, PSTAT115 &lt;chr&gt;, PSTAT127 &lt;chr&gt;, CS16 &lt;chr&gt;, ECE180 &lt;chr&gt;,\n#   `CS5A-B` &lt;chr&gt;, PSTAT175 &lt;chr&gt;, `CS130A-B` &lt;chr&gt;, `CS165A-B` &lt;chr&gt;,\n#   LING110 &lt;chr&gt;, ECON145 &lt;chr&gt;, rsrch &lt;lgl&gt;\n\n\nselect – select a subset of columns from a data frame\n\n# select a column\nbackground %&gt;%\n  select(math.comf)\n\n# A tibble: 41 × 1\n   math.comf\n       &lt;dbl&gt;\n 1         4\n 2         5\n 3         4\n 4         5\n 5         5\n 6         3\n 7         4\n 8         4\n 9         3\n10         3\n# ℹ 31 more rows\n\n\npull – extract a single column from a data frame\n\n# pull a column\nbackground %&gt;%\n  pull(rsrch)\n\n [1]  TRUE FALSE  TRUE FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE\n[13] FALSE FALSE  TRUE FALSE  TRUE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE\n[25] FALSE  TRUE  TRUE FALSE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE FALSE\n[37] FALSE FALSE  TRUE FALSE  TRUE\n\n\nmutate – define a new column as a function of existing columns\n\n# define a new variable\nbackground %&gt;%\n  mutate(avg.comf = (math.comf + prog.comf + stat.comf)/3)\n\n# A tibble: 41 × 31\n   response.id prog.prof prog.comf math.prof math.comf stat.prof stat.comf\n         &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n 1           2 Adv               5 Adv               4 Adv               5\n 2           3 Int               4 Adv               5 Adv               5\n 3           4 Adv               4 Adv               4 Adv               5\n 4           5 Adv               5 Adv               5 Adv               4\n 5           6 Int               4 Adv               5 Adv               5\n 6           7 Int               4 Int               3 Int               3\n 7           8 Adv               5 Adv               4 Adv               4\n 8           9 Int               4 Int               4 Adv               5\n 9          10 Int               4 Int               3 Int               4\n10          11 Int               3 Adv               3 Adv               4\n# ℹ 31 more rows\n# ℹ 24 more variables: updv.num &lt;chr&gt;, dom &lt;chr&gt;, consent &lt;chr&gt;,\n#   `PSTAT120A-B` &lt;chr&gt;, PSTAT122 &lt;chr&gt;, PSTAT126 &lt;chr&gt;, PSTAT131 &lt;chr&gt;,\n#   PSTAT134 &lt;chr&gt;, `PSTAT160A-B` &lt;chr&gt;, CS9 &lt;chr&gt;, PSTAT100 &lt;chr&gt;,\n#   PSTAT174 &lt;chr&gt;, PSTAT115 &lt;chr&gt;, PSTAT127 &lt;chr&gt;, CS16 &lt;chr&gt;, ECE180 &lt;chr&gt;,\n#   `CS5A-B` &lt;chr&gt;, PSTAT175 &lt;chr&gt;, `CS130A-B` &lt;chr&gt;, `CS165A-B` &lt;chr&gt;,\n#   LING110 &lt;chr&gt;, ECON145 &lt;chr&gt;, rsrch &lt;lgl&gt;, avg.comf &lt;dbl&gt;\n\n\nThese operations can be chained together, for example:\n\n# sequence of verbs\nbackground %&gt;%\n  filter(stat.prof == 'Adv') %&gt;%\n  mutate(avg.comf = (math.comf + prog.comf + stat.comf)/3) %&gt;%\n  select(avg.comf, rsrch) \n\n# A tibble: 23 × 2\n   avg.comf rsrch\n      &lt;dbl&gt; &lt;lgl&gt;\n 1     4.67 TRUE \n 2     4.67 FALSE\n 3     4.33 TRUE \n 4     4.67 FALSE\n 5     4.67 FALSE\n 6     4.33 TRUE \n 7     4.33 FALSE\n 8     3.33 FALSE\n 9     3.67 FALSE\n10     4.33 TRUE \n# ℹ 13 more rows\n\n\n\n\n\n\n\n\n\nAction\n\n\n\n\nWrite a chain of verbs in order to find the proficiency ratings of all respondents with research experience and 6-8 upper division courses.\nWrite a chain of verbs in order to find the proficiency ratings of all respondents without research experience and the same number of upper division courses\nCompare results and discuss with your neighbor: do these suggest any patterns?\n\n\n\n\n\nSummaries\nSummaries are easily computed across rows using summarize() . So if for example we want to use the filtering and selection from before to find the proportion of advanced students in statistics with research experience, use:\n\n# a summary\nbackground %&gt;%\n  filter(stat.prof == 'Adv') %&gt;%\n  mutate(avg.comf = (math.comf + prog.comf + stat.comf)/3) %&gt;%\n  select(avg.comf, rsrch) %&gt;%\n  summarize(prop.rsrch = mean(rsrch))\n\n# A tibble: 1 × 1\n  prop.rsrch\n       &lt;dbl&gt;\n1      0.391\n\n# equivalent\nbackground %&gt;%\n  filter(stat.prof == 'Adv') %&gt;%\n  mutate(avg.comf = (math.comf + prog.comf + stat.comf)/3) %&gt;%\n  select(avg.comf, rsrch) %&gt;%\n  pull(rsrch) %&gt;%\n  mean()\n\n[1] 0.3913043\n\n\nThe advantage of summarize , however, is that multiple summaries can be computed at once:\n\nbackground %&gt;%\n  filter(stat.prof == 'Adv') %&gt;%\n  mutate(avg.comf = (math.comf + prog.comf + stat.comf)/3) %&gt;%\n  select(avg.comf, rsrch) %&gt;%\n  summarize(prop.rsrch = mean(rsrch),\n            med.comf = median(avg.comf))\n\n# A tibble: 1 × 2\n  prop.rsrch med.comf\n       &lt;dbl&gt;    &lt;dbl&gt;\n1      0.391     4.33\n\n\nThe variant summarize_all computes the same summary across all columns. (Notice the use of the helper verb contains() to select all columns containing a particular string.)\n\n# average comfort levels across all students\nbackground %&gt;%\n  select(contains('comf')) %&gt;%\n  summarise_all(.funs = mean)\n\n# A tibble: 1 × 3\n  prog.comf math.comf stat.comf\n      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1      3.85      3.80      3.98\n\n\nGrouped summaries are summaries computed separately among subsets of observations. To define a grouping structure using an existing column, use group_by() . Notice the ‘groups’ attribute printed with the output.\n\n# create a grouping\nbackground %&gt;%\n  group_by(stat.prof)\n\n# A tibble: 41 × 30\n# Groups:   stat.prof [2]\n   response.id prog.prof prog.comf math.prof math.comf stat.prof stat.comf\n         &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n 1           2 Adv               5 Adv               4 Adv               5\n 2           3 Int               4 Adv               5 Adv               5\n 3           4 Adv               4 Adv               4 Adv               5\n 4           5 Adv               5 Adv               5 Adv               4\n 5           6 Int               4 Adv               5 Adv               5\n 6           7 Int               4 Int               3 Int               3\n 7           8 Adv               5 Adv               4 Adv               4\n 8           9 Int               4 Int               4 Adv               5\n 9          10 Int               4 Int               3 Int               4\n10          11 Int               3 Adv               3 Adv               4\n# ℹ 31 more rows\n# ℹ 23 more variables: updv.num &lt;chr&gt;, dom &lt;chr&gt;, consent &lt;chr&gt;,\n#   `PSTAT120A-B` &lt;chr&gt;, PSTAT122 &lt;chr&gt;, PSTAT126 &lt;chr&gt;, PSTAT131 &lt;chr&gt;,\n#   PSTAT134 &lt;chr&gt;, `PSTAT160A-B` &lt;chr&gt;, CS9 &lt;chr&gt;, PSTAT100 &lt;chr&gt;,\n#   PSTAT174 &lt;chr&gt;, PSTAT115 &lt;chr&gt;, PSTAT127 &lt;chr&gt;, CS16 &lt;chr&gt;, ECE180 &lt;chr&gt;,\n#   `CS5A-B` &lt;chr&gt;, PSTAT175 &lt;chr&gt;, `CS130A-B` &lt;chr&gt;, `CS165A-B` &lt;chr&gt;,\n#   LING110 &lt;chr&gt;, ECON145 &lt;chr&gt;, rsrch &lt;lgl&gt;\n\n\nSometimes it can be helpful to simply count the observations in each group:\n\n# count observations\nbackground %&gt;%\n  group_by(stat.prof) %&gt;%\n  count()\n\n# A tibble: 2 × 2\n# Groups:   stat.prof [2]\n  stat.prof     n\n  &lt;chr&gt;     &lt;int&gt;\n1 Adv          23\n2 Int          18\n\n\nTo compute a grouped summary, first group the data frame and then specify the summary of interest:\n\n# a grouped summary\nbackground %&gt;%\n  group_by(stat.prof) %&gt;%\n  select(contains('.comf')) %&gt;%\n  summarize_all(.funs = mean)\n\n# A tibble: 2 × 4\n  stat.prof prog.comf math.comf stat.comf\n  &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 Adv            4         4.17      4.43\n2 Int            3.67      3.33      3.39\n\n\n\n\n\n\n\n\nAction\n\n\n\nGrouped summaries\n\nCompute the median comfort level of all students in each subject area.\nCompute the median comfort level of all students in each subject area after grouping by number of upper division classes taken.\nCompare and discuss with your neighbor: do you notice any interesting patterns?"
  },
  {
    "objectID": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#tidyr-verbs",
    "href": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#tidyr-verbs",
    "title": "Tidyverse basics",
    "section": "tidyr verbs",
    "text": "tidyr verbs\nIn general, tidyr verbs reshape data frames in various ways. For now, we’ll just cover two tidyr verbs.\nSuppose we want to calculate multiple summaries of multiple variables using the techniques above. By default, the output is one row with one column for each summary/variable combination:\n\n# many variables, many summaries\ncomf_sum &lt;- background %&gt;%\n  select(contains('comf')) %&gt;%\n  summarise_all(.funs = list(mean = mean, \n                             median = median,\n                             min = min, \n                             max = max))\n\ncomf_sum\n\n# A tibble: 1 × 12\n  prog.comf_mean math.comf_mean stat.comf_mean prog.comf_median math.comf_median\n           &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n1           3.85           3.80           3.98                4                4\n# ℹ 7 more variables: stat.comf_median &lt;dbl&gt;, prog.comf_min &lt;dbl&gt;,\n#   math.comf_min &lt;dbl&gt;, stat.comf_min &lt;dbl&gt;, prog.comf_max &lt;dbl&gt;,\n#   math.comf_max &lt;dbl&gt;, stat.comf_max &lt;dbl&gt;\n\n\nIt would be much better to reshape this into a table. gather will reshape the data frame from wide format to long format by ‘gathering’ the columns together.\n\n# gather columns into long format\ncomf_sum %&gt;% gather(stat, val) \n\n# A tibble: 12 × 2\n   stat               val\n   &lt;chr&gt;            &lt;dbl&gt;\n 1 prog.comf_mean    3.85\n 2 math.comf_mean    3.80\n 3 stat.comf_mean    3.98\n 4 prog.comf_median  4   \n 5 math.comf_median  4   \n 6 stat.comf_median  4   \n 7 prog.comf_min     2   \n 8 math.comf_min     2   \n 9 stat.comf_min     2   \n10 prog.comf_max     5   \n11 math.comf_max     5   \n12 stat.comf_max     5   \n\n\nThis is a little better, but it would be more legible in a 2x2 table. We can separate the ‘stat’ variable that has the column names into two columns:\n\n# separate into rows and columns\ncomf_sum %&gt;%\n  gather(stat, val) %&gt;%\n  separate(stat, into = c('variable', 'stat'), sep = '_') \n\n# A tibble: 12 × 3\n   variable  stat     val\n   &lt;chr&gt;     &lt;chr&gt;  &lt;dbl&gt;\n 1 prog.comf mean    3.85\n 2 math.comf mean    3.80\n 3 stat.comf mean    3.98\n 4 prog.comf median  4   \n 5 math.comf median  4   \n 6 stat.comf median  4   \n 7 prog.comf min     2   \n 8 math.comf min     2   \n 9 stat.comf min     2   \n10 prog.comf max     5   \n11 math.comf max     5   \n12 stat.comf max     5   \n\n\nAnd then spread the stat column over a few rows, resulting in a table where the rows are the variables and the columns are the summaries:\n\n# spread into table\ncomf_sum %&gt;%\n  gather(stat, val) %&gt;%\n  separate(stat, into = c('variable', 'stat'), sep = '_') %&gt;%\n  spread(stat, val)\n\n# A tibble: 3 × 5\n  variable    max  mean median   min\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 math.comf     5  3.80      4     2\n2 prog.comf     5  3.85      4     2\n3 stat.comf     5  3.98      4     2"
  },
  {
    "objectID": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#ggplot",
    "href": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#ggplot",
    "title": "Tidyverse basics",
    "section": "ggplot",
    "text": "ggplot\nThe ggplot package is for data visualization. The syntax takes some getting used to if you haven’t seen it before. We’ll just look at one example.\nSuppose we want to summarize the prior coursework in the class.\n\n# summary of classes taken\nclasses &lt;- background %&gt;%\n  select(11:29) %&gt;%\n  mutate_all(~factor(.x, levels = c('no', 'yes'))) %&gt;%\n  mutate_all(~as.numeric(.x) - 1) %&gt;%\n  summarize_all(mean) %&gt;%\n  gather(class, proportion)\n\nclasses\n\n# A tibble: 19 × 2\n   class       proportion\n   &lt;chr&gt;            &lt;dbl&gt;\n 1 PSTAT120A-B     1     \n 2 PSTAT122        0.902 \n 3 PSTAT126        0.976 \n 4 PSTAT131        0.780 \n 5 PSTAT134        0.317 \n 6 PSTAT160A-B     0.439 \n 7 CS9             0.854 \n 8 PSTAT100        0.366 \n 9 PSTAT174        0.268 \n10 PSTAT115        0.317 \n11 PSTAT127        0.0732\n12 CS16            0.146 \n13 ECE180          0.0488\n14 CS5A-B          0.0488\n15 PSTAT175        0.0244\n16 CS130A-B        0.0244\n17 CS165A-B        0.0244\n18 LING110         0.0244\n19 ECON145         0.0244\n\n\nWe could report the results in a table, in which case perhaps arranging in descending order may be helpful:\n\nclasses %&gt;% arrange(desc(proportion))\n\n# A tibble: 19 × 2\n   class       proportion\n   &lt;chr&gt;            &lt;dbl&gt;\n 1 PSTAT120A-B     1     \n 2 PSTAT126        0.976 \n 3 PSTAT122        0.902 \n 4 CS9             0.854 \n 5 PSTAT131        0.780 \n 6 PSTAT160A-B     0.439 \n 7 PSTAT100        0.366 \n 8 PSTAT134        0.317 \n 9 PSTAT115        0.317 \n10 PSTAT174        0.268 \n11 CS16            0.146 \n12 PSTAT127        0.0732\n13 ECE180          0.0488\n14 CS5A-B          0.0488\n15 PSTAT175        0.0244\n16 CS130A-B        0.0244\n17 CS165A-B        0.0244\n18 LING110         0.0244\n19 ECON145         0.0244\n\n\nLet’s say we’d rather plot this data. We’ll put the course number on one axis and the proportion of students who took it on the other.\n\n# plot it\nclasses %&gt;%\n  ggplot(aes(x = proportion, y = class)) +\n  geom_point()\n\n\n\n\n\n\n\n\nThese commands work by defining plot layers. In the chunk above, the first argument to ggplot() is the data. Then, aes() defines an ‘aesthetic mapping’ of the columns of the input data frame to graphical elements. This defines a set of axes. Then, a layer of points is added to the plot with geom_point() ; no arguments are needed because the geometric object (‘geom’) inherits attributes (x and y coordinates) from the aesthetic mapping.\nAgain we might prefer to arrange the classes by descending order in proportion.\n\nfig &lt;- classes %&gt;%\n  ggplot(aes(x = proportion, y = reorder(class, proportion))) +\n  geom_point()\n\nfig\n\n\n\n\n\n\n\n\nAnd perhaps fix the plot labels:\n\n# adjust labels\nfig + labs(x = 'proportion of class', y = '')\n\n\n\n\n\n\n\n\nNotice that ggplot allows for a plot to be stored by name and then further modified with additional layers."
  },
  {
    "objectID": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#checklist",
    "href": "materials/labs/lab2-tidyverse/lab2-tidyverse.html#checklist",
    "title": "Tidyverse basics",
    "section": "Checklist",
    "text": "Checklist\n\nAll actions were completed.\nAll code chunks were copied into your script.\nYour script is saved in a lab subfolder of your class directory with an associated project."
  },
  {
    "objectID": "materials/course-materials.html#module-1-biomarker-identification",
    "href": "materials/course-materials.html#module-1-biomarker-identification",
    "title": "Course materials",
    "section": "Module 1: biomarker identification",
    "text": "Module 1: biomarker identification\nObjectives: introduce variable selection, classification, and multiple testing problems; discuss classification accuracy metrics and data partitioning; fit logistic regression and random forest classifiers in R; learn to implement multiple testing corrections for FDR control (Benjamini-Hochberg and Benjamini-Yekutieli); discuss selection via penalized estimation. Data from Hewitson et al. (2021) .\n\nWeek 3\n\nTuesday meeting: introducing biomarker data; multiple testing [slides]\nSection meeting: iteration strategies [activity]\nThursday meeting: correlation analysis; random forests [slides] [activity]\nAssignments due by next class meeting (Tuesday 10/22):\n\nread MDSR 10.1 - 10.2\nread Hewitson et al. (2021)\nprepare a reading response",
    "crumbs": [
      "PSTAT197A",
      "Course materials"
    ]
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#announcementsreminders",
    "href": "materials/slides/week3-biomarkers.html#announcementsreminders",
    "title": "Multiple testing corrections",
    "section": "Announcements/reminders",
    "text": "Announcements/reminders\n\ndon’t forget to fill out attendance form for each class meeting\n\nbut don’t fill it out if you don’t come to class\n\nfirst group assignment due Friday 10/18 11:59pm PST\nsection attendance is expected"
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#background",
    "href": "materials/slides/week3-biomarkers.html#background",
    "title": "Multiple testing corrections",
    "section": "Background",
    "text": "Background\nLevels of proteins in plasma/serum are altered in autism spectrum disorder (ASD).\n\nGoal: identify a panel of proteins useful as a blood biomarker for early detection of ASD.\n\na ‘panel’ is a handful of tests that help distinguish between conditions\nso in other words, find proteins whose serum levels are predictive of ASD"
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#dataset",
    "href": "materials/slides/week3-biomarkers.html#dataset",
    "title": "Multiple testing corrections",
    "section": "Dataset",
    "text": "Dataset\nData from Hewitson et al. (2021)\n\nSerum samples from 76 boys with ASD and 78 typically developing (TD) boys, 18 months-8 years of age\nA total of 1,125 proteins were analyzed from each sample\n\n1,317 measured, 192 failed quality control\n(we don’t know which ones failed QC so will use all)"
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#sample-characteristics",
    "href": "materials/slides/week3-biomarkers.html#sample-characteristics",
    "title": "Multiple testing corrections",
    "section": "Sample characteristics",
    "text": "Sample characteristics\n\nAgeDemographicsComorbiditiesMedications\n\n\n\n\n\n\n\nAttribute\nASD (n = 76)\nTD (n = 78)\n\n\n\n\nAge: mean (SD) years\n5.6 (1.7)\n5.7 (2.0)\n\n\n\n\n\n\n\n\n\n\n\n\nAttribute\nASD (n = 76)\nTD (n = 78)\n\n\n\n\nWhite/Caucasian\n33 (45.2%)\n40 (51.9%)\n\n\nHispanic/Latino\n26 (35.6%)\n6 (7.8%)\n\n\nAfrican American/Black\n3 (4.1%)\n14 (18.2%)\n\n\nAsian or Pacific Islander\n2 (2.6%)\n3 (3.9%)\n\n\nMultiple ethnicities or Other\n9 (12.3%)\n14 (18.2%)\n\n\nNot reported\n3 (4.1%)\n1 (1.2%)\n\n\n\n\n\n\n\n\n\n\n\n\nAttribute\nASD (n = 76)\nTD (n = 78)\n\n\n\n\nNone\n38 (52.8%)\n58 (75.3%)\n\n\nADHD\n2 (2.8%)\n1 (1.3%)\n\n\nSeasonal Allergies\n30 (41.7%)\n17 (22.4%)\n\n\nAsthma\n2 (2.8%)\n0 (0%)\n\n\nCeliac Disease\n1 (1.4%)\n0 (0%)\n\n\nGERD\n1 (1.4%)\n0 (0%)\n\n\nPTSD\n0 (0%)\n1 (1.3%)\n\n\nSleep Apnea\n2 (2.8%)\n0 (0%)\n\n\nNot reported\n4 (5.6%)\n1 (1.3%)\n\n\n\n\n\n\n\n\n\n\n\n\nAttribute\nASD (n = 76)\nTD (n = 78)\n\n\n\n\nNone\n69 (92%)\n75 (97.4%)\n\n\nAnti-depressant\n2 (2.7%)\n0 (0%)\n\n\nAnti-psychotic\n0 (0%)\n1 (1.3%)\n\n\nSedative\n1 (1.3%)\n0 (0%)\n\n\nSSRI\n2 (2.27%)\n0 (0%)\n\n\nStimulant\n1 (1.3%)\n1 (1.3%)\n\n\nNot reported\n1 (1.3%)\n1 (1.3%)"
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#data-glimpse",
    "href": "materials/slides/week3-biomarkers.html#data-glimpse",
    "title": "Multiple testing corrections",
    "section": "Data glimpse",
    "text": "Data glimpse\n\nExample rowsGroup sizes\n\n\n\nasd_clean %&gt;% head(5)\n\n# A tibble: 5 × 1,318\n  group    CHIP  CEBPB     NSE   PIAS4 `IL-10 Ra`  STAT3   IRF1 `c-Jun` `Mcl-1`\n  &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 ASD    0.335   0.520 -0.554   0.650      -0.358  0.305 -0.484   0.309  1.57  \n2 ASD   -0.0715  1.01   3       1.28       -0.133  1.13   0.253   0.408  0.0643\n3 ASD   -0.406  -0.531 -0.0592  1.13        0.554 -0.334  0.287  -0.845  1.42  \n4 ASD   -0.102  -0.251  1.47    0.0773     -0.705  0.893  2.61   -0.372 -0.467 \n5 ASD   -0.395  -0.536  0.0410 -0.299      -0.830  0.899  1.01   -0.843 -1.15  \n# ℹ 1,308 more variables: OAS1 &lt;dbl&gt;, `c-Myc` &lt;dbl&gt;, SMAD3 &lt;dbl&gt;, SMAD2 &lt;dbl&gt;,\n#   `IL-23` &lt;dbl&gt;, PDGFRA &lt;dbl&gt;, `IL-12` &lt;dbl&gt;, STAT1 &lt;dbl&gt;, STAT6 &lt;dbl&gt;,\n#   LRRK2 &lt;dbl&gt;, Osteocalcin &lt;dbl&gt;, `IL-5` &lt;dbl&gt;, GPDA &lt;dbl&gt;, IgA &lt;dbl&gt;,\n#   LPPL &lt;dbl&gt;, HEMK2 &lt;dbl&gt;, PDXK &lt;dbl&gt;, TLR4 &lt;dbl&gt;, REG4 &lt;dbl&gt;,\n#   `HSP 27` &lt;dbl&gt;, `YKL-40` &lt;dbl&gt;, `Alpha enolase` &lt;dbl&gt;, `Apo L1` &lt;dbl&gt;,\n#   CD38 &lt;dbl&gt;, CD59 &lt;dbl&gt;, FABPL &lt;dbl&gt;, `GDF-11` &lt;dbl&gt;, BTC &lt;dbl&gt;,\n#   `HIF-1a` &lt;dbl&gt;, S100A6 &lt;dbl&gt;, SECTM1 &lt;dbl&gt;, RSPO3 &lt;dbl&gt;, PSP &lt;dbl&gt;, …\n\n\n\n\n\n\n# A tibble: 2 × 2\n  group     n\n  &lt;chr&gt; &lt;int&gt;\n1 ASD      76\n2 TD       78"
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#module-objectives",
    "href": "materials/slides/week3-biomarkers.html#module-objectives",
    "title": "Multiple testing corrections",
    "section": "Module objectives",
    "text": "Module objectives\nMethodology\n\nmultiple testing\nclassification: logistic regression; random forests\nvariable selection: LASSO regularization\nclassification accuracy measures\n\n\nConcepts\n\ndata partitioning for predictive modeling\nmodel interpretability\nhigh dimensional data \\(n &lt; p\\)"
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#marginal-differences",
    "href": "materials/slides/week3-biomarkers.html#marginal-differences",
    "title": "Multiple testing corrections",
    "section": "Marginal differences",
    "text": "Marginal differences\nIdea: test for a significant difference in serum levels between groups for a given protein, say protein \\(i\\).\n\nNotation:\n\n\\(\\mu^i_{ASD}\\): mean serum level of protein \\(i\\) in the ASD group\n\\(\\mu^i_{TD}\\): mean serum level of protein \\(i\\) in the TD group\n\\(\\delta_i\\): difference in means \\(\\mu^i_{ASD} - \\mu^i_{TD}\\)\nhats indicate sample estimates (e.g. \\(\\hat{\\delta}_i\\))"
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#review-t-test",
    "href": "materials/slides/week3-biomarkers.html#review-t-test",
    "title": "Multiple testing corrections",
    "section": "Review: \\(t\\)-test",
    "text": "Review: \\(t\\)-test\nThe \\(t\\)-test tests \\(H_{0i}: \\delta_i = 0\\) against its negation \\(\\neg H_{0i}: \\delta_i \\neq 0\\) using the rule\n\\[\n\\text{reject $H_{0i}$ if}\\qquad \\left|\\frac{\\hat{\\delta}_i}{SE(\\hat{\\delta}_i)}\\right| &gt; t_\\alpha\n\\]\n\n\\(SE(\\hat{\\delta}_i)\\) is a standard error for the difference estimate; quantifies variability of the estimate\nprocedure controls type I error at \\(\\alpha\\), ensuring \\(P\\left(\\text{reject}_i|H_i\\right) \\leq 0.05\\)"
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#review-p-values",
    "href": "materials/slides/week3-biomarkers.html#review-p-values",
    "title": "Multiple testing corrections",
    "section": "Review: \\(p\\)-values",
    "text": "Review: \\(p\\)-values\nThe \\(p\\)-value for a test is the probability of obtaining a sample at least as contrary to \\(H_{0i}\\) as the sample in hand, assuming \\(H_{0i}\\) is true.\n\nBy construction, \\(p &lt; \\alpha\\) just in case the test rejects with type I error controlled at \\(\\alpha\\).\n\n\nSo a common heuristic is:\n\\[\n\\text{reject $H_{0i}$ if} \\qquad p_i \\leq \\alpha\n\\]"
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#one-test",
    "href": "materials/slides/week3-biomarkers.html#one-test",
    "title": "Multiple testing corrections",
    "section": "One test",
    "text": "One test\nHere is R output for one test.\n\nasd %&gt;%\n  t_test(formula = CHIP ~ group,\n         order = c('ASD', 'TD'),\n         alternative = 'two-sided',\n         var.equal = F)\n\n# A tibble: 1 × 7\n  statistic  t_df p_value alternative estimate lower_ci upper_ci\n      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1     0.927  75.7   0.357 two.sided       384.    -441.    1210.\n\n\n\nQuestions:\n\nWhat are the hypotheses in words?\nWhat are the test assumptions?\nWhat is the conclusion of the test?"
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#many-tests",
    "href": "materials/slides/week3-biomarkers.html#many-tests",
    "title": "Multiple testing corrections",
    "section": "Many tests",
    "text": "Many tests\nA plausible approach for identifying a protein panel, then, is to select all those proteins for which the \\(t\\)-test indicates a significant difference.\n\n1,317 tests\neasy to compute\nconceptually straightforward\n\n\nHow likely are mistakes?"
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#test-outcomes",
    "href": "materials/slides/week3-biomarkers.html#test-outcomes",
    "title": "Multiple testing corrections",
    "section": "Test outcomes",
    "text": "Test outcomes\nLet \\(H_i\\) denote the \\(i\\)th null hypothesis and \\(R_i\\) denote the event that \\(H_i\\) is rejected.\n\n\n\n\n\n\\(H_i\\)\n\\(\\neg H_i\\)\n\n\n\n\n\\(R_i\\)\n\\(V\\) false rejections\n\\(S\\) correct\n\n\n\\(\\neg R_i\\)\n\\(T\\) correct\n\\(W\\) false non-rejections\n\n\n\n\n\nThe multiple testing problem is that individual error rates compound over multiple tests."
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#familywise-error",
    "href": "materials/slides/week3-biomarkers.html#familywise-error",
    "title": "Multiple testing corrections",
    "section": "Familywise error",
    "text": "Familywise error\nFamilywise error rate (FWER) is the probability of one or more type I errors: \\(P(V \\geq 1)\\).\n\nSuppose there are \\(m\\) true hypotheses \\(\\mathcal{H}: \\{H_i: i \\in C\\}\\).\n\n\nIf the tests are independent and exact then:\n\\[\n\\begin{aligned}\nP(V \\geq 1)\n&= P\\left[ \\bigcup_{i \\in C} R_i | \\mathcal{H} \\right] \\\\\n&= 1 - \\prod_{i \\in C} \\left( 1- P(R_i|H_i) \\right) \\\\\n&= 1 - (1 - \\alpha)^m\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#fwer-example",
    "href": "materials/slides/week3-biomarkers.html#fwer-example",
    "title": "Multiple testing corrections",
    "section": "FWER Example",
    "text": "FWER Example\nIf individual tests are exactly controlled at \\(\\alpha = 0.05\\) and independent, at least one error is nearly certain by 100 tests.\n\nFamilywise error rate as a function of the number of tests, assuming tests are independent with exact type I error 0.05."
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#bonferroni-correction",
    "href": "materials/slides/week3-biomarkers.html#bonferroni-correction",
    "title": "Multiple testing corrections",
    "section": "Bonferroni correction",
    "text": "Bonferroni correction\nThe simplest multiple testing correction is based on the Bonferroni inequality:\n\\[\nP\\left[ \\bigcup_{i \\in C} R_i | \\mathcal{H} \\right] \\leq \\sum_{i \\in C} P(R_i|\\mathcal{H})\n\\]\n\nIf the individual tests are controlled at level \\(\\alpha\\), then \\(FWER \\leq m\\alpha\\).\n\n\nSo a simple solution is to test at level \\(\\alpha^* = \\frac{\\alpha}{m}\\).\n\n\nIn other words, reject if \\(p_i &lt; \\frac{\\alpha}{m}\\)."
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#false-discovery-rate",
    "href": "materials/slides/week3-biomarkers.html#false-discovery-rate",
    "title": "Multiple testing corrections",
    "section": "False discovery rate",
    "text": "False discovery rate\nFWER control will limit false rejections, but at the cost of power; controlling the probability of one type I error is a conservative approach.\n\nMore common in modern applications are procedures to control false discovery rate: the expected proportion of rejections that are false.\n\\[\n\\text{FDR} = \\mathbb{E}\\left[\\frac{\\text{false rejections}}{\\text{total rejections}}\\right]\n\\]\n\n\nConceptually, if say FDR is controlled at \\(0.05\\), then one would expect 5% of rejections to be false."
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#benjamini-hochberg-correction",
    "href": "materials/slides/week3-biomarkers.html#benjamini-hochberg-correction",
    "title": "Multiple testing corrections",
    "section": "Benjamini-Hochberg correction",
    "text": "Benjamini-Hochberg correction\nBenjamini and Hochberg (1995) conceived a procedure based on sorting \\(p\\)-values.\n\nSupposing \\(m\\) independent tests are performed:\n\nSort the \\(p\\)-values in increasing order \\(p_{(1)}, p_{(2)}, \\dots, p_{(m)}\\)\nReject whenever \\(p_{(i)} &lt; \\frac{i\\alpha}{m}\\)\n\n\n\nThey proved that this controls FDR at \\(\\alpha\\)."
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#benjamini-yekutieli-correction",
    "href": "materials/slides/week3-biomarkers.html#benjamini-yekutieli-correction",
    "title": "Multiple testing corrections",
    "section": "Benjamini-Yekutieli correction",
    "text": "Benjamini-Yekutieli correction\nThe Benjamini-Hochberg assumes tests are independent, which is obviously not true in most situations. (Why?)\n\nBenjamini and Yekutieli (2001) modified the correction to hold without the independence assumption:\n\nSort the \\(p\\)-values in increasing order \\(p_{(1)}, p_{(2)}, \\dots, p_{(m)}\\)\nReject whenever \\(p_{(i)} &lt; \\frac{i\\alpha}{m H_m}\\)\n\n\n\nAbove, \\(H_m = \\sum_{i = 1}^m \\frac{1}{i}\\) ."
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#implementing-corrections",
    "href": "materials/slides/week3-biomarkers.html#implementing-corrections",
    "title": "Multiple testing corrections",
    "section": "Implementing corrections",
    "text": "Implementing corrections\nThe easiest way to implement these corrections is to adjust the \\(p\\)-values with a multiplier:\n\n(Bonferroni) \\(p^b_i = m\\times p_i\\)\n(Benjamini-Hochberg) \\(p^{bh}_{(i)} = \\frac{m}{i} p_{(i)}\\)\n(Benjamini-Yekuteili) \\(p^{bh}_{(i)} = \\frac{m H_m}{i} p_{(i)}\\)"
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#computations",
    "href": "materials/slides/week3-biomarkers.html#computations",
    "title": "Multiple testing corrections",
    "section": "Computations",
    "text": "Computations\n\nPreprocessingAssumptionsTestsCorrections\n\n\n\ntrim_fn &lt;- function(x){\n  x[x &gt; 3] &lt;- 3\n  x[x &lt; -3] &lt;- -3\n  \n  return(x)\n}\n\nasd_clean &lt;- asd %&gt;% \n  select(-ados) %&gt;%\n  # log transform\n  mutate(across(.cols = -group, log10)) %&gt;%\n  # center and scale\n  mutate(across(.cols = -group, ~ scale(.x)[, 1])) %&gt;%\n  # trim outliers (affects results??)\n  mutate(across(.cols = -group, trim_fn))\n\nasd_nested &lt;- asd_clean %&gt;%\n  pivot_longer(-group, \n               names_to = 'protein', \n               values_to = 'level') %&gt;%\n  nest(data = c(level, group))\n\nasd_nested %&gt;% head(4)\n\n# A tibble: 4 × 2\n  protein data              \n  &lt;chr&gt;   &lt;list&gt;            \n1 CHIP    &lt;tibble [154 × 2]&gt;\n2 CEBPB   &lt;tibble [154 × 2]&gt;\n3 NSE     &lt;tibble [154 × 2]&gt;\n4 PIAS4   &lt;tibble [154 × 2]&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# compute for several groups\ntest_fn &lt;- function(.df){\n  t_test(.df, \n         formula = level ~ group,\n         order = c('ASD', 'TD'),\n         alternative = 'two-sided',\n         var.equal = F)\n}\n\ntt_out &lt;- asd_nested %&gt;%\n  mutate(ttest = map(data, test_fn)) %&gt;%\n  unnest(ttest) %&gt;%\n  arrange(p_value)\n\ntt_out %&gt;% head(5)\n\n# A tibble: 5 × 9\n  protein     data     statistic  t_df     p_value alternative estimate lower_ci\n  &lt;chr&gt;       &lt;list&gt;       &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;\n1 DERM        &lt;tibble&gt;     -6.10  151.     8.27e-9 two.sided     -0.885    -1.17\n2 RELT        &lt;tibble&gt;     -5.65  152.     7.82e-8 two.sided     -0.775    -1.05\n3 FSTL1       &lt;tibble&gt;     -5.27  152.     4.66e-7 two.sided     -0.783    -1.08\n4 C1QR1       &lt;tibble&gt;     -5.26  152.     4.79e-7 two.sided     -0.782    -1.08\n5 Calcineurin &lt;tibble&gt;     -5.24  151.     5.37e-7 two.sided     -0.734    -1.01\n# ℹ 1 more variable: upper_ci &lt;dbl&gt;\n\n\n\n\n\n# multiple testing corrections\nm &lt;- nrow(tt_out)\nhm &lt;- log(m) + 1/(2*m) - digamma(1)\n  \ntt_corrected &lt;- tt_out %&gt;%\n  select(data, protein, p_value) %&gt;%\n  mutate(rank = row_number()) %&gt;%\n  mutate(p_bh = p_value*m/rank,\n         p_by = p_value*m*hm/rank,\n         p_bonf = p_value*m)\n\ntt_corrected %&gt;% head(5)\n\n# A tibble: 5 × 7\n  data               protein           p_value  rank      p_bh      p_by  p_bonf\n  &lt;list&gt;             &lt;chr&gt;               &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 &lt;tibble [154 × 2]&gt; DERM        0.00000000827     1 0.0000109 0.0000845 1.09e-5\n2 &lt;tibble [154 × 2]&gt; RELT        0.0000000782      2 0.0000515 0.000400  1.03e-4\n3 &lt;tibble [154 × 2]&gt; FSTL1       0.000000466       3 0.000205  0.00159   6.14e-4\n4 &lt;tibble [154 × 2]&gt; C1QR1       0.000000479       4 0.000158  0.00122   6.31e-4\n5 &lt;tibble [154 × 2]&gt; Calcineurin 0.000000537       5 0.000141  0.00110   7.07e-4"
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#results",
    "href": "materials/slides/week3-biomarkers.html#results",
    "title": "Multiple testing corrections",
    "section": "Results",
    "text": "Results\n\nComparing methodsTop 10 proteins\n\n\n\n\n\n\n\nAdjusted vs. raw p-values for each multiple correction method.\n\n\n\n\n\n\n\n# top 10\ntt_corrected %&gt;%\n  select(protein, p_by) %&gt;%\n  slice_min(order_by = p_by, n = 10)\n\n# A tibble: 10 × 2\n   protein              p_by\n   &lt;chr&gt;               &lt;dbl&gt;\n 1 DERM            0.0000845\n 2 RELT            0.000400 \n 3 Calcineurin     0.00110  \n 4 C1QR1           0.00122  \n 5 MRC2            0.00132  \n 6 IgD             0.00136  \n 7 CXCL16, soluble 0.00149  \n 8 PTN             0.00154  \n 9 FSTL1           0.00159  \n10 Cadherin-5      0.00179"
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#neat-graphic-volcano-plot",
    "href": "materials/slides/week3-biomarkers.html#neat-graphic-volcano-plot",
    "title": "Multiple testing corrections",
    "section": "Neat graphic: volcano plot",
    "text": "Neat graphic: volcano plot\n\nUpregulation and downregulation of serum levels of proteins analyzed – p-values against number of doublings (positive) or halvings (negative) of serum level in ASD group relative to TD group."
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#next-time",
    "href": "materials/slides/week3-biomarkers.html#next-time",
    "title": "Multiple testing corrections",
    "section": "Next time",
    "text": "Next time\nOther approaches to the same problem:\n\ncorrelation with ADOS (severity diagnostic score)\nvariable importance in random forest classifier"
  },
  {
    "objectID": "materials/slides/week3-biomarkers.html#references",
    "href": "materials/slides/week3-biomarkers.html#references",
    "title": "Multiple testing corrections",
    "section": "References",
    "text": "References\n\n\n\n\n\n\n\n\nBenjamini, Yoav, and Yosef Hochberg. 1995. “Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing.” Journal of the Royal Statistical Society: Series B (Methodological) 57 (1): 289–300.\n\n\nBenjamini, Yoav, and Daniel Yekutieli. 2001. “The Control of the False Discovery Rate in Multiple Testing Under Dependency.” Annals of Statistics, 1165–88.\n\n\nHewitson, Laura, Jeremy A Mathews, Morgan Devlin, Claire Schutte, Jeon Lee, and Dwight C German. 2021. “Blood Biomarker Discovery for Autism Spectrum Disorder: A Proteomic Analysis.” PLoS One 16 (2): e0246581."
  },
  {
    "objectID": "materials/labs/lab3-iteration/lab3-iteration.html",
    "href": "materials/labs/lab3-iteration/lab3-iteration.html",
    "title": "Iteration strategies",
    "section": "",
    "text": "In class we discussed multiple testing in the context of an application that involved performing 1,317 \\(t\\)-tests. Implementing these tests involves iteration: repeatedly performing the same computations.\nObjective. Here we’ll look at a few strategies for iteration in R:\n\nloops\nfunctions in the apply family\nfunctional programming using tidyverse\n\nWe’ll illustrate these strategies using the biomarker data and reproduce some of the results shown in class.\n\n\n\n\n\n\nAction\n\n\n\nCreate a new script for lab 3 in your labs project/folder and copy-paste the code chunk below at the top of the script.\n\n\n\nlibrary(tidyverse)\n# install.packages('infer') # execute once then comment out\n\n# data location\nurl &lt;- 'https://raw.githubusercontent.com/pstat197/pstat197a/main/materials/labs/lab3-iteration/data/biomarker-clean.csv'\n\n# function for outlier trimming\ntrim_fn &lt;- function(x){\n  x[x &gt; 3] &lt;- 3\n  x[x &lt; -3] &lt;- -3\n  \n  return(x)\n}\n\n# read in and preprocess data\nasd &lt;- read_csv(url) %&gt;%\n  select(-ados) %&gt;%\n  # log transform\n  mutate(across(.cols = -group, log10)) %&gt;%\n  # center and scale\n  mutate(across(.cols = -group, ~ scale(.x)[, 1])) %&gt;%\n  # trim outliers\n  mutate(across(.cols = -group, trim_fn))"
  },
  {
    "objectID": "materials/labs/lab3-iteration/lab3-iteration.html#background",
    "href": "materials/labs/lab3-iteration/lab3-iteration.html#background",
    "title": "Iteration strategies",
    "section": "",
    "text": "In class we discussed multiple testing in the context of an application that involved performing 1,317 \\(t\\)-tests. Implementing these tests involves iteration: repeatedly performing the same computations.\nObjective. Here we’ll look at a few strategies for iteration in R:\n\nloops\nfunctions in the apply family\nfunctional programming using tidyverse\n\nWe’ll illustrate these strategies using the biomarker data and reproduce some of the results shown in class.\n\n\n\n\n\n\nAction\n\n\n\nCreate a new script for lab 3 in your labs project/folder and copy-paste the code chunk below at the top of the script.\n\n\n\nlibrary(tidyverse)\n# install.packages('infer') # execute once then comment out\n\n# data location\nurl &lt;- 'https://raw.githubusercontent.com/pstat197/pstat197a/main/materials/labs/lab3-iteration/data/biomarker-clean.csv'\n\n# function for outlier trimming\ntrim_fn &lt;- function(x){\n  x[x &gt; 3] &lt;- 3\n  x[x &lt; -3] &lt;- -3\n  \n  return(x)\n}\n\n# read in and preprocess data\nasd &lt;- read_csv(url) %&gt;%\n  select(-ados) %&gt;%\n  # log transform\n  mutate(across(.cols = -group, log10)) %&gt;%\n  # center and scale\n  mutate(across(.cols = -group, ~ scale(.x)[, 1])) %&gt;%\n  # trim outliers\n  mutate(across(.cols = -group, trim_fn))"
  },
  {
    "objectID": "materials/labs/lab3-iteration/lab3-iteration.html#loops",
    "href": "materials/labs/lab3-iteration/lab3-iteration.html#loops",
    "title": "Iteration strategies",
    "section": "Loops",
    "text": "Loops\n\nSimple examples\nA loop is a set of instructions to be repeated a specified number of times while incrementing a flag or index value. For example:\n\nfor(i in 1:4){\n  print(2*i)\n}\n\n[1] 2\n[1] 4\n[1] 6\n[1] 8\n\n\nHere the instructions are:\n\ninitialize index/flag i at i = 1\nexecute code within the braces {...}\nincrement i &lt;- i + 1\nrepeat steps 2-3 until i = 5\n\nWe could make the loop a bit more verbose:\n\nflag_vals &lt;- c(1, 2, 3, 4)\nfor(i in flag_vals){\n  out &lt;- 2*i\n  print(out)\n}\n\n[1] 2\n[1] 4\n[1] 6\n[1] 8\n\n\nNow to retain the results in memory, a storage data structure must be defined and the output of each iteration assigned to some element(s) of the storage object.\n\nrslt &lt;- rep(NA, 4)\nfor(i in 1:4){\n  rslt[i] &lt;- 2*i\n}\nrslt\n\n[1] 2 4 6 8\n\n\nIf we want to perform the same calculation for all values in a vector, we might do something like this:\n\nrslt &lt;- rep(NA, 4)\ninput_vals &lt;- c(15, 27, 3, 12.6)\nfor(i in 1:4){\n  rslt[i] &lt;- 2*input_vals[i]\n}\nrslt\n\n[1] 30.0 54.0  6.0 25.2\n\n\n\n\n\n\n\n\nCheck your understanding\n\n\n\nWhy does the following loop produce an NA ?\n\nrslt &lt;- rep(NA, 4)\ninput_vals &lt;- rnorm(n = 3)\nfor(i in 1:4){\n  rslt[i] &lt;- 2*input_vals[i]\n}\n\nrslt\n\n[1] -3.607136  3.254478  2.446879        NA\n\n\n\n\nLoops are substantially similar in any programming language but usually not optimized for performance. Additionally, they are somewhat verbose and hard to read due to explicit use of indexing in the syntax.\n\n\nMultiple testing with loops\nIn base R, the \\(t\\)-test is performed using t.test(...) , which takes as arguments two vectors of observations (one for each group). For instance:\n\nx &lt;- asd %&gt;% filter(group == 'ASD') %&gt;% pull(CHIP)\ny &lt;- asd %&gt;% filter(group == 'TD') %&gt;% pull(CHIP)\nt.test(x, y, var.equal = F)\n\n\n    Welch Two Sample t-test\n\ndata:  x and y\nt = -0.18812, df = 151.74, p-value = 0.851\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.2588194  0.2138173\nsample estimates:\n  mean of x   mean of y \n-0.04922954 -0.02672849 \n\n\nThe output is a list:\n\nt.test(x, y) %&gt;% str()\n\nList of 10\n $ statistic  : Named num -0.188\n  ..- attr(*, \"names\")= chr \"t\"\n $ parameter  : Named num 152\n  ..- attr(*, \"names\")= chr \"df\"\n $ p.value    : num 0.851\n $ conf.int   : num [1:2] -0.259 0.214\n  ..- attr(*, \"conf.level\")= num 0.95\n $ estimate   : Named num [1:2] -0.0492 -0.0267\n  ..- attr(*, \"names\")= chr [1:2] \"mean of x\" \"mean of y\"\n $ null.value : Named num 0\n  ..- attr(*, \"names\")= chr \"difference in means\"\n $ stderr     : num 0.12\n $ alternative: chr \"two.sided\"\n $ method     : chr \"Welch Two Sample t-test\"\n $ data.name  : chr \"x and y\"\n - attr(*, \"class\")= chr \"htest\"\n\n\nSo if we want the p-value:\n\nt.test(x, y, var.equal = F)$p.value\n\n[1] 0.8510352\n\n\nTo calculate \\(p\\)-values for all tests using a loop, we wrap the code we used to perform one \\(t\\)-test in a for loop and add appropriate indexing. For speed, we’ll just compute the first 100 tests:\n\nn_tests &lt;- 100\np_vals &lt;- rep(NA, n_tests)\nfor(i in 1:n_tests){\n  x &lt;- asd %&gt;% filter(group == 'ASD') %&gt;% pull(i + 1)\n  y &lt;- asd %&gt;% filter(group == 'TD') %&gt;% pull(i + 1)\n  p_vals[i] &lt;- t.test(x, y, var.equal = F)$p.value\n}\n\nTo line these up with the proteins they correspond to, it’s necessary to keep track of the indexing carefully. In this case, the indexing corresponds to the order of columns. So we could create a data frame like so:\n\ntibble(protein = colnames(asd)[2:(n_tests + 1)],\n       p = p_vals)\n\n# A tibble: 100 × 2\n   protein        p\n   &lt;chr&gt;      &lt;dbl&gt;\n 1 CHIP     0.851  \n 2 CEBPB    0.0322 \n 3 NSE      0.350  \n 4 PIAS4    0.104  \n 5 IL-10 Ra 0.0232 \n 6 STAT3    0.00183\n 7 IRF1     0.592  \n 8 c-Jun    0.0351 \n 9 Mcl-1    0.999  \n10 OAS1     0.942  \n# ℹ 90 more rows\n\n\nAlternatively, we could have set up the loop to output this result:\n\nn_tests &lt;- 100\nrslt &lt;- tibble(protein = colnames(asd)[2:(n_tests + 1)],\n               p = NA)\nfor(i in 1:n_tests){\n  x &lt;- asd %&gt;% filter(group == 'ASD') %&gt;% pull(i + 1)\n  y &lt;- asd %&gt;% filter(group == 'TD') %&gt;% pull(i + 1)\n  rslt$p[i] &lt;- t.test(x, y, var.equal = F)$p.value\n}\n\n\n\n\n\n\n\nAction\n\n\n\nFollow the example above to write a loop that stores both the \\(p\\)-values and the estimated differences for the first 50 proteins."
  },
  {
    "objectID": "materials/labs/lab3-iteration/lab3-iteration.html#apply-family",
    "href": "materials/labs/lab3-iteration/lab3-iteration.html#apply-family",
    "title": "Iteration strategies",
    "section": "Apply family",
    "text": "Apply family\n\nSimple examples\nIn R, the apply family of functions allows one to efficiently iterate a function over an index set. So, to execute our simple for loop using apply , we could do something like this:\n\nvals &lt;- rnorm(n = 4)\nsimple_fn &lt;- function(x){2*x}\nlapply(vals, simple_fn)\n\n[[1]]\n[1] 3.470945\n\n[[2]]\n[1] -0.8976415\n\n[[3]]\n[1] 1.2582\n\n[[4]]\n[1] -3.367953\n\n\nThis applies simple_fn to each element of vals , and returns the result as a list. If we want a neater output format, we could use sapply , which is short for sort-apply:\n\nsapply(vals, simple_fn)\n\n[1]  3.4709454 -0.8976415  1.2581997 -3.3679532\n\n\nIn more complex settings it often makes sense to apply a function across an index set. This is very similar conceptually to a for loop, but faster and easier to read.\n\n# apply a function to an index set\nsimple_fn_ix &lt;- function(i){2*vals[i]}\nrslt_apply &lt;- sapply(1:length(vals), simple_fn_ix)\n\n# equivalent for loop\nrslt_loop &lt;- rep(NA, length(vals))\nfor(i in 1:length(vals)){\n  rslt_loop[i] &lt;- 2*vals[i]\n}\n\n# compare\nrbind(rslt_loop, rslt_apply)\n\n               [,1]       [,2]   [,3]      [,4]\nrslt_loop  3.470945 -0.8976415 1.2582 -3.367953\nrslt_apply 3.470945 -0.8976415 1.2582 -3.367953\n\n\n\n\n\\(t\\)-tests using apply\nWe can use apply functions to compute \\(t\\)-tests for the proteins in the ASD data by coercing the data to a list of data frames that contain the grouping and level for each protein.\n\n# number of tests to perform\nn_tests &lt;- 100\n\n# convert to a list\nasd_list &lt;- asd %&gt;% \n  select(1:(n_tests + 1)) %&gt;%\n  pivot_longer(cols = -group,\n               names_to = 'protein',\n               values_to = 'level') %&gt;%\n  group_by(protein) %&gt;%\n  group_split()\n\n# first entry in list\nasd_list[[1]]\n\n# A tibble: 154 × 3\n   group protein                     level\n   &lt;chr&gt; &lt;chr&gt;                       &lt;dbl&gt;\n 1 ASD   14-3-3 protein beta/alpha -0.124 \n 2 ASD   14-3-3 protein beta/alpha  0.487 \n 3 ASD   14-3-3 protein beta/alpha -0.801 \n 4 ASD   14-3-3 protein beta/alpha  2.73  \n 5 ASD   14-3-3 protein beta/alpha  1.24  \n 6 ASD   14-3-3 protein beta/alpha  0.250 \n 7 ASD   14-3-3 protein beta/alpha  0.932 \n 8 ASD   14-3-3 protein beta/alpha  0.0873\n 9 ASD   14-3-3 protein beta/alpha  0.213 \n10 ASD   14-3-3 protein beta/alpha  0.157 \n# ℹ 144 more rows\n\n\nThe function t.test(...) can also perform the test using a formula of the form y ~ x and a data frame containing x and y, as below.\n\nt.test(level ~ group, data = asd_list[[1]])\n\n\n    Welch Two Sample t-test\n\ndata:  level by group\nt = -1.5671, df = 150.2, p-value = 0.1192\nalternative hypothesis: true difference in means between group ASD and group TD is not equal to 0\n95 percent confidence interval:\n -0.54341111  0.06269287\nsample estimates:\nmean in group ASD  mean in group TD \n       -0.1341683         0.1061909 \n\n\nIf we just want the \\(p\\)-value again, we can wrap this code in a function whose argument is the index \\(i\\). This function will return the \\(p\\)-value for the \\(i\\)th protein.\n\n# p value for ith protein\ntt_fn &lt;- function(i){\n  t.test(level ~ group, data = asd_list[[i]])$p.value\n}\n\n# check\ntt_fn(1)\n\n[1] 0.1191888\n\n\nNow to perform many tests, we can simply iterate this function over consecutive index values 1:n_tests:\n\nsapply(1:n_tests, tt_fn)\n\n  [1] 1.191888e-01 2.972829e-01 8.136635e-01 8.297144e-01 3.034583e-02\n  [6] 9.517828e-01 3.553359e-01 2.671529e-03 6.458878e-01 3.915314e-04\n [11] 1.759142e-01 4.788545e-07 2.862286e-01 5.714296e-01 7.052780e-03\n [16] 3.220583e-02 8.510352e-01 1.267133e-03 9.482110e-03 1.293157e-04\n [21] 7.804081e-03 2.208460e-04 1.407044e-01 1.023033e-01 8.995855e-02\n [26] 1.578665e-02 3.113212e-04 2.920587e-02 4.663516e-07 2.395764e-01\n [31] 5.709433e-03 8.962287e-02 2.700053e-02 5.357313e-01 7.392658e-01\n [36] 8.665332e-01 3.538260e-02 1.956257e-04 8.658766e-01 2.111378e-04\n [41] 3.286108e-01 5.374305e-01 1.108687e-01 1.711403e-01 4.808293e-01\n [46] 3.556564e-03 2.322968e-02 3.746226e-01 7.804488e-01 3.175372e-01\n [51] 4.085249e-01 4.746117e-03 5.917788e-01 3.748021e-02 6.775472e-01\n [56] 6.433125e-01 1.121721e-03 3.234610e-03 4.154758e-03 1.726578e-03\n [61] 1.669733e-03 9.990017e-01 7.100178e-04 4.811425e-01 8.978465e-01\n [66] 3.503310e-01 9.423978e-01 3.925728e-01 3.025965e-01 4.511875e-02\n [71] 4.219360e-01 2.117196e-01 1.036412e-01 8.148983e-01 1.399029e-02\n [76] 5.096269e-04 9.145121e-02 3.331394e-02 4.350959e-01 8.721647e-01\n [81] 3.266951e-03 2.704495e-01 4.929196e-01 1.010954e-03 3.144033e-01\n [86] 7.933758e-04 1.929813e-03 6.104791e-02 1.832399e-03 1.333513e-02\n [91] 6.412884e-01 2.605232e-02 4.130732e-01 2.579393e-01 4.096623e-01\n [96] 2.925137e-01 5.866341e-01 3.505666e-02 5.095419e-01 5.590746e-01\n\n\nYou might have noticed this was much faster than the loop. We can time it:\n\nstart &lt;- Sys.time()\nrslt &lt;- sapply(1:n_tests, tt_fn)\nend &lt;- Sys.time()\n\nend - start\n\nTime difference of 0.07815504 secs\n\n\nAnd compare with the for loop:\n\nstart &lt;- Sys.time()\nn_tests &lt;- 100\nrslt &lt;- tibble(protein = colnames(asd)[2:(n_tests + 1)],\n               p = NA)\nfor(i in 1:n_tests){\n  x &lt;- asd %&gt;% filter(group == 'ASD') %&gt;% pull(i + 1)\n  y &lt;- asd %&gt;% filter(group == 'TD') %&gt;% pull(i + 1)\n  rslt$p[i] &lt;- t.test(x, y, var.equal = F)$p.value\n}\nend &lt;- Sys.time()\n\nend - start\n\nTime difference of 3.090725 secs\n\n\nAnother nice feature of sapply is its ability to sort and arrange multiple outputs. For example, if the function is adjusted to return both the \\(p\\)-value and the test statistic:\n\ntt_fn &lt;- function(i){\n  test_rslt &lt;- t.test(level ~ group, data = asd_list[[i]])\n  out &lt;- c(pval = test_rslt$p.value, \n           tstat = test_rslt$statistic)\n  out\n}\n\ntt_fn(1)\n\n      pval    tstat.t \n 0.1191888 -1.5671297 \n\n\nThen sapply will return a matrix:\n\nsapply(1:5, tt_fn) %&gt;% t() %&gt;% as_tibble()\n\n# A tibble: 5 × 2\n    pval tstat.t\n   &lt;dbl&gt;   &lt;dbl&gt;\n1 0.119   -1.57 \n2 0.297   -1.05 \n3 0.814    0.236\n4 0.830   -0.215\n5 0.0303  -2.19 \n\n\n\n\n\n\n\n\nAction\n\n\n\n\nUse sapply to obtain the estimated differences and standard errors for the groupwise comparisons for the first 50 proteins.\nArrange the result in a data frame with a column indicating the protein, a column indicating the estimated group difference, and a column indicating the standard error."
  },
  {
    "objectID": "materials/labs/lab3-iteration/lab3-iteration.html#tidyverse",
    "href": "materials/labs/lab3-iteration/lab3-iteration.html#tidyverse",
    "title": "Iteration strategies",
    "section": "Tidyverse",
    "text": "Tidyverse\nA final strategy for iteration comes from functional programming tools in tidyverse . The basic idea is:\n\ndefine a grouping structure using relevant variables (in this case, proteins)\ncollapse the data into separate data frames by group\napply a function to each data frame that produces test output given input data\n\n\nNesting\nOne thing that tibbles can do that data frames cannot is store list-columns: columns that are lists of arbitrary objects. This allows for the arrangement of a much more general collection of objects in tabular form.\nAn intuitive example is nested data: a list-column of data frames having the same columns. If we nest the ASD data by protein, we obtain a data frame that looks like this:\n\nasd_nested &lt;- asd %&gt;%\n  pivot_longer(-group, \n               names_to = 'protein', \n               values_to = 'level') %&gt;%\n  nest(data = c(level, group))\n\nasd_nested %&gt;% head(5)\n\n# A tibble: 5 × 2\n  protein  data              \n  &lt;chr&gt;    &lt;list&gt;            \n1 CHIP     &lt;tibble [154 × 2]&gt;\n2 CEBPB    &lt;tibble [154 × 2]&gt;\n3 NSE      &lt;tibble [154 × 2]&gt;\n4 PIAS4    &lt;tibble [154 × 2]&gt;\n5 IL-10 Ra &lt;tibble [154 × 2]&gt;\n\n\nThe data column consists of data frames, one per protein, containing the variables group and level :\n\nasd_nested %&gt;%\n  slice(1L) %&gt;%\n  pull(data)\n\n[[1]]\n# A tibble: 154 × 2\n     level group\n     &lt;dbl&gt; &lt;chr&gt;\n 1  0.335  ASD  \n 2 -0.0715 ASD  \n 3 -0.406  ASD  \n 4 -0.102  ASD  \n 5 -0.395  ASD  \n 6 -0.126  ASD  \n 7  0.486  ASD  \n 8 -0.990  ASD  \n 9 -0.108  ASD  \n10  0.485  ASD  \n# ℹ 144 more rows\n\n\n\n\nThe map() function\nIn an ordinary data frame one can define a new variable as a function of other variables. The same can be done with list-columns in a tibble: one can define a new variable as a function of the elements of a list stored in another column. To do this, one uses the map() function, which is essentially the tidyverse version of lapply() :\n\nmap(.x, .fn) means roughly “apply the function .fn to each element in .x”\n\nHere we can write a function that takes a data frame with protein level and group as input, and returns a t test as output; then computing each test is as simple as calling mutate :\n\ntt_fn &lt;- function(.df){\n  t.test(level ~ group, data = .df)\n}\n\nrslt &lt;- asd_nested %&gt;%\n  slice(1:10) %&gt;%\n  mutate(ttest.out = map(data, tt_fn))\n\nrslt\n\n# A tibble: 10 × 3\n   protein  data               ttest.out\n   &lt;chr&gt;    &lt;list&gt;             &lt;list&gt;   \n 1 CHIP     &lt;tibble [154 × 2]&gt; &lt;htest&gt;  \n 2 CEBPB    &lt;tibble [154 × 2]&gt; &lt;htest&gt;  \n 3 NSE      &lt;tibble [154 × 2]&gt; &lt;htest&gt;  \n 4 PIAS4    &lt;tibble [154 × 2]&gt; &lt;htest&gt;  \n 5 IL-10 Ra &lt;tibble [154 × 2]&gt; &lt;htest&gt;  \n 6 STAT3    &lt;tibble [154 × 2]&gt; &lt;htest&gt;  \n 7 IRF1     &lt;tibble [154 × 2]&gt; &lt;htest&gt;  \n 8 c-Jun    &lt;tibble [154 × 2]&gt; &lt;htest&gt;  \n 9 Mcl-1    &lt;tibble [154 × 2]&gt; &lt;htest&gt;  \n10 OAS1     &lt;tibble [154 × 2]&gt; &lt;htest&gt;  \n\nrslt %&gt;% slice(1L) %&gt;% pull(ttest.out)\n\n[[1]]\n\n    Welch Two Sample t-test\n\ndata:  level by group\nt = -0.18812, df = 151.74, p-value = 0.851\nalternative hypothesis: true difference in means between group ASD and group TD is not equal to 0\n95 percent confidence interval:\n -0.2588194  0.2138173\nsample estimates:\nmean in group ASD  mean in group TD \n      -0.04922954       -0.02672849 \n\n\nWhile all the data we might want are there, the output is a little unwieldy. Luckily, the infer package contains a pipe-operator-friendly function infer::t_test that returns results in a tidy fashion.\n\nasd_nested %&gt;% \n  slice(1L) %&gt;% \n  unnest(cols = data) %&gt;% \n  infer::t_test(formula = level ~ group,\n         order = c('ASD', 'TD'),\n         alternative = 'two-sided',\n         var.equal = F)\n\n# A tibble: 1 × 7\n  statistic  t_df p_value alternative estimate lower_ci upper_ci\n      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1    -0.188  152.   0.851 two.sided    -0.0225   -0.259    0.214\n\n\nWe can create a wrapper around this function suitable for our purposes and then apply it to the list-column in asd_nested :\n\n# wrapper around infer::t_test\ntt_fn &lt;- function(.df){\n  infer::t_test(.df, \n         formula = level ~ group,\n         order = c('ASD', 'TD'),\n         alternative = 'two-sided',\n         var.equal = F)\n}\n\n# compute test results\ntt_out &lt;- asd_nested %&gt;%\n  slice(1:n_tests) %&gt;%\n  mutate(ttest = map(data, tt_fn))\n\n# preview\ntt_out %&gt;% head(4)\n\n# A tibble: 4 × 3\n  protein data               ttest           \n  &lt;chr&gt;   &lt;list&gt;             &lt;list&gt;          \n1 CHIP    &lt;tibble [154 × 2]&gt; &lt;tibble [1 × 7]&gt;\n2 CEBPB   &lt;tibble [154 × 2]&gt; &lt;tibble [1 × 7]&gt;\n3 NSE     &lt;tibble [154 × 2]&gt; &lt;tibble [1 × 7]&gt;\n4 PIAS4   &lt;tibble [154 × 2]&gt; &lt;tibble [1 × 7]&gt;\n\n\nNotice that ttest is also a list-column comprised of separate data frames. This column can be un-nested to show the output of infer::t_test explicitly:\n\ntt_out %&gt;% \n  unnest(ttest) %&gt;%\n  head(4)\n\n# A tibble: 4 × 9\n  protein data     statistic  t_df p_value alternative estimate lower_ci\n  &lt;chr&gt;   &lt;list&gt;       &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;\n1 CHIP    &lt;tibble&gt;    -0.188  152.  0.851  two.sided    -0.0225  -0.259 \n2 CEBPB   &lt;tibble&gt;     2.16   150.  0.0322 two.sided     0.317    0.0273\n3 NSE     &lt;tibble&gt;     0.937  151.  0.350  two.sided     0.148   -0.164 \n4 PIAS4   &lt;tibble&gt;     1.64   152.  0.104  two.sided     0.222   -0.0459\n# ℹ 1 more variable: upper_ci &lt;dbl&gt;\n\n\nThis approach has a few advantages, namely, it is syntactically more readable than either of the other approaches and it works with the pipe operator, so could in theory be incorporated into a chain that performs additional calculations on, say, the results of the \\(t\\)-test. However, the drawback is that it is slow:\n\n# time it\nstart &lt;- Sys.time()\ntt_out &lt;- asd_nested %&gt;%\n  slice(1:n_tests) %&gt;%\n  mutate(ttest = map(data, tt_fn))\nend &lt;- Sys.time()\n\nend - start\n\nTime difference of 1.702877 secs\n\n\nIt’s not as slow as a for loop, but it’s much slower than apply functions. If speed is a concern or the number of iterations is especially large, apply would be a better choice.\n\n\nAdjusting p-values\nTo adjust the \\(p\\)-values, we simply manipulate the p_value column:\n\n# bonferroni correction\ntt_out %&gt;% \n  unnest(ttest) %&gt;%\n  mutate(p_adj = p_value*n_tests) %&gt;%\n  select(protein, p_value, p_adj) %&gt;%\n  arrange(p_adj) %&gt;%\n  head(4)\n\n# A tibble: 4 × 3\n  protein     p_value     p_adj\n  &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;\n1 FSTL1   0.000000466 0.0000466\n2 C1QR1   0.000000479 0.0000479\n3 DSC2    0.000129    0.0129   \n4 HIF-1a  0.000196    0.0196   \n\n\n\n\n\n\n\n\nAction\n\n\n\nImplement the Benjamini-Hochberg correction\n\nSort the raw \\(p\\)-values using arrange()\nAdd a rank column of consecutive integers (neat trick: try using row_number())\nAdd a column p_adj containing \\(\\frac{m}{i} p_{(i)}\\) where \\(m\\) is the number of tests, \\(i\\) is the rank of the \\(p\\)-value, and \\(p_{(i)}\\) is the \\(i\\)th smallest \\(p\\)-value\nFind the collection of proteins with significantly different serum levels between the ASD and TD groups while controlling the false discovery rate at 1%.\n\nDevelop working code to execute 50 tests. Then use it to compute all 1317 tests."
  },
  {
    "objectID": "materials/labs/lab3-iteration/lab3-iteration.html#checklist",
    "href": "materials/labs/lab3-iteration/lab3-iteration.html#checklist",
    "title": "Iteration strategies",
    "section": "Checklist",
    "text": "Checklist\n\nYou’ve computed \\(p\\)-values iteratively using a loop, sapply , and nest+map .\nYou have commented codes in your script for each action item.\nYou’ve obtained a list of the significant proteins with 1% FDR.\nYou’ve saved your work somewhere where you can access it later."
  },
  {
    "objectID": "materials/slides/week3-randomforest.html#announcementsreminders",
    "href": "materials/slides/week3-randomforest.html#announcementsreminders",
    "title": "Random forests",
    "section": "Announcements/reminders",
    "text": "Announcements/reminders\nMake your final commits for the first group assignment by Friday 10/18 11:59pm PST\n\nshould include an updated report.qmd and report.html with your write-up\n\n\nNext group assignment to be distributed on next Tuesday.\n\nGroups will be randomly assigned\nTask: replicate and redesign proteomic analysis"
  },
  {
    "objectID": "materials/slides/week3-randomforest.html#last-time",
    "href": "materials/slides/week3-randomforest.html#last-time",
    "title": "Random forests",
    "section": "Last time",
    "text": "Last time\n\nintroduced ASD proteomic dataset\nused multiple testing corrections to identify proteins whose serum levels differ significantly between ASD/TD groups\ndiscussed the difference between controlling familywise error vs. false discovery"
  },
  {
    "objectID": "materials/slides/week3-randomforest.html#today",
    "href": "materials/slides/week3-randomforest.html#today",
    "title": "Random forests",
    "section": "Today",
    "text": "Today\nWe’ll talk about two more approaches to identifying proteins of interest:\n\ncorrelation-based identification of proteins\nrandom forest classifier"
  },
  {
    "objectID": "materials/slides/week3-randomforest.html#ados-score",
    "href": "materials/slides/week3-randomforest.html#ados-score",
    "title": "Random forests",
    "section": "ADOS score",
    "text": "ADOS score\nAutism Diagnostic Observation Schedule (ADOS) scores are determined by psychological assessment and measure ASD severity.\n\nasd %&gt;% \n  select(group, ados) %&gt;% \n  group_by(group) %&gt;% \n  sample_n(size = 2)\n\n# A tibble: 4 × 2\n# Groups:   group [2]\n  group  ados\n  &lt;chr&gt; &lt;dbl&gt;\n1 ASD       9\n2 ASD      16\n3 TD       NA\n4 TD       NA\n\n\n\nados is only measured for the ASD group\nnumerical score between 6 and 23 (at least in this sample)"
  },
  {
    "objectID": "materials/slides/week3-randomforest.html#correlation-approach",
    "href": "materials/slides/week3-randomforest.html#correlation-approach",
    "title": "Random forests",
    "section": "Correlation approach",
    "text": "Correlation approach\nSo here’s an idea:\n\ncompute correlations of each protein with ADOS;\npick proteins with the strongest correlation"
  },
  {
    "objectID": "materials/slides/week3-randomforest.html#computation",
    "href": "materials/slides/week3-randomforest.html#computation",
    "title": "Random forests",
    "section": "Computation",
    "text": "Computation\nThis is a simple aggregation operation and can be executed in R with summarize() :\n\n# compute correlations\nasd_clean %&gt;%\n  pivot_longer(cols = -ados,\n               names_to = 'protein',\n               values_to = 'level') %&gt;%\n  group_by(protein) %&gt;%\n  summarize(correlation = cor(ados, level))\n\n# A tibble: 1,317 × 2\n   protein                   correlation\n   &lt;chr&gt;                           &lt;dbl&gt;\n 1 14-3-3                         0.0970\n 2 14-3-3 protein beta/alpha      0.0481\n 3 14-3-3 protein theta           0.122 \n 4 14-3-3 protein zeta/delta      0.0735\n 5 14-3-3E                       -0.127 \n 6 17-beta-HSD 1                  0.0969\n 7 3HAO                           0.0773\n 8 3HIDH                          0.0452\n 9 4-1BB                          0.0290\n10 4-1BB ligand                   0.0799\n# ℹ 1,307 more rows"
  },
  {
    "objectID": "materials/slides/week3-randomforest.html#visual-assessment",
    "href": "materials/slides/week3-randomforest.html#visual-assessment",
    "title": "Random forests",
    "section": "Visual assessment",
    "text": "Visual assessment"
  },
  {
    "objectID": "materials/slides/week3-randomforest.html#sort-and-slice",
    "href": "materials/slides/week3-randomforest.html#sort-and-slice",
    "title": "Random forests",
    "section": "Sort and slice",
    "text": "Sort and slice\n\nListVisual\n\n\n\n\n# A tibble: 10 × 4\n   protein          correlation abs.corr  rank\n   &lt;chr&gt;                  &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;\n 1 CO8A1                  0.362    0.362  1317\n 2 C5b, 6 Complex        -0.337    0.337     1\n 3 Thrombospondin-1       0.310    0.310  1316\n 4 ILT-2                 -0.309    0.309     2\n 5 TRAIL R4               0.296    0.296  1315\n 6 HCE000414             -0.296    0.296     3\n 7 C1r                   -0.290    0.290     4\n 8 GM-CSF                 0.290    0.290  1314\n 9 Angiogenin            -0.284    0.284     5\n10 HCG                    0.278    0.278  1313"
  },
  {
    "objectID": "materials/slides/week3-randomforest.html#slr-coefficients-instead",
    "href": "materials/slides/week3-randomforest.html#slr-coefficients-instead",
    "title": "Random forests",
    "section": "SLR coefficients instead?",
    "text": "SLR coefficients instead?\nFact: the simple linear regression coefficient estimate is proportional to the correlation coefficient.\n\nSo it should give similar results to sort the SLR coefficients by significance.\n\n\n\n\n# A tibble: 1,317 × 3\n   protein          estimate p.value\n   &lt;chr&gt;               &lt;dbl&gt;   &lt;dbl&gt;\n 1 CO8A1               0.391 0.00131\n 2 C5b, 6 Complex     -0.401 0.00290\n 3 Thrombospondin-1    0.317 0.00635\n 4 ILT-2              -0.546 0.00664\n 5 TRAIL R4            0.353 0.00933\n 6 HCE000414          -0.278 0.00950\n 7 C1r                -0.341 0.0110 \n 8 GM-CSF              0.345 0.0110 \n 9 Angiogenin         -0.314 0.0130 \n10 HCG                 0.306 0.0149 \n# ℹ 1,307 more rows"
  },
  {
    "objectID": "materials/slides/week3-randomforest.html#fdr-control",
    "href": "materials/slides/week3-randomforest.html#fdr-control",
    "title": "Random forests",
    "section": "FDR control",
    "text": "FDR control\nIf we do the correlation analysis this way, do the identify proteins pass multiple testing significance thresholds?\n\n\n# A tibble: 10 × 3\n   protein          p.value  p.adj\n   &lt;chr&gt;              &lt;dbl&gt;  &lt;dbl&gt;\n 1 CO8A1            0.00131 0.0145\n 2 C5b, 6 Complex   0.00290 0.0342\n 3 Thrombospondin-1 0.00635 0.112 \n 4 ILT-2            0.00664 0.0641\n 5 TRAIL R4         0.00933 1.29  \n 6 HCE000414        0.00950 0.916 \n 7 C1r              0.0110  0.242 \n 8 GM-CSF           0.0110  0.124 \n 9 Angiogenin       0.0130  0.142 \n10 HCG              0.0149  0.159 \n\n\n\nprobably just introducing selection noise\nbut also, this result diverges considerably from the paper (?)"
  },
  {
    "objectID": "materials/slides/week3-randomforest.html#aside-correlation-test",
    "href": "materials/slides/week3-randomforest.html#aside-correlation-test",
    "title": "Random forests",
    "section": "Aside: correlation test",
    "text": "Aside: correlation test\nThe SLR approach is equivalent to sorting the correlations.\n\nWe could use inference on the population correlation to obtain a \\(p\\)-value associated with each sample correlation coefficient. These match the ones from SLR.\n\ncor_test &lt;- function(x, y){\n  cor_out &lt;- cor.test(x, y)\n  tibble(estimate = cor_out$estimate,\n         p.value = cor_out$p.value)\n}\n\nasd_clean %&gt;%\n  pivot_longer(cols = -ados,\n               names_to = 'protein',\n               values_to = 'level') %&gt;%\n  group_by(protein) %&gt;%\n  summarize(correlation = cor_test(ados, level)) %&gt;%\n  unnest(correlation) %&gt;%\n  arrange(p.value)\n\n# A tibble: 1,317 × 3\n   protein          estimate p.value\n   &lt;chr&gt;               &lt;dbl&gt;   &lt;dbl&gt;\n 1 CO8A1               0.362 0.00131\n 2 C5b, 6 Complex     -0.337 0.00290\n 3 Thrombospondin-1    0.310 0.00635\n 4 ILT-2              -0.309 0.00664\n 5 TRAIL R4            0.296 0.00933\n 6 HCE000414          -0.296 0.00950\n 7 C1r                -0.290 0.0110 \n 8 GM-CSF              0.290 0.0110 \n 9 Angiogenin         -0.284 0.0130 \n10 HCG                 0.278 0.0149 \n# ℹ 1,307 more rows"
  },
  {
    "objectID": "materials/slides/week3-randomforest.html#background",
    "href": "materials/slides/week3-randomforest.html#background",
    "title": "Random forests",
    "section": "Background",
    "text": "Background\nA binary tree is a directed graph in which:\n\nthere is at most one path between any two nodes\neach node has at most two outward-directed edges"
  },
  {
    "objectID": "materials/slides/week3-randomforest.html#classification-tree",
    "href": "materials/slides/week3-randomforest.html#classification-tree",
    "title": "Random forests",
    "section": "Classification tree",
    "text": "Classification tree\nA classification tree is a binary tree in which the paths represent classification rules.\n\nA goofy classification tree."
  },
  {
    "objectID": "materials/slides/week3-randomforest.html#example-classifying-high-earners",
    "href": "materials/slides/week3-randomforest.html#example-classifying-high-earners",
    "title": "Random forests",
    "section": "Example: classifying high earners",
    "text": "Example: classifying high earners\nSay we want to predict income based on capital gains and education level using census data.\n\n\n# A tibble: 6 × 3\n# Groups:   income [2]\n  income educ     capital_gain\n  &lt;fct&gt;  &lt;fct&gt;           &lt;dbl&gt;\n1 &lt;=50K  hs                  0\n2 &lt;=50K  advanced            0\n3 &lt;=50K  college             0\n4 &gt;50K   hs               7688\n5 &gt;50K   hs               7688\n6 &gt;50K   college          5178\n\n\n\nWe could construct a classification tree by ‘splitting’ based on the values of predictor variables."
  },
  {
    "objectID": "materials/slides/week3-randomforest.html#activity-building-trees",
    "href": "materials/slides/week3-randomforest.html#activity-building-trees",
    "title": "Random forests",
    "section": "Activity: building trees",
    "text": "Activity: building trees\nTo get a sense of the process of tree construction, we’ll do an activity in groups: each group will build a tree ‘by hand’.\n\nfirst let’s look at the instructions together\nthen work on today’s activity in teams"
  },
  {
    "objectID": "materials/slides/week3-randomforest.html#random-forests",
    "href": "materials/slides/week3-randomforest.html#random-forests",
    "title": "Random forests",
    "section": "Random forests",
    "text": "Random forests\nA random forest is a classifier based on many trees. It is constructed by:\n\nbuilding some large number of \\(T\\) trees using bootstrap samples and random subsets of predictors (what you just did, repeated many times)\ntaking a majority vote across all trees to determine the classification\n\n\nSo let’s take a vote using your trees!"
  },
  {
    "objectID": "materials/slides/week3-randomforest.html#variable-importance-scores",
    "href": "materials/slides/week3-randomforest.html#variable-importance-scores",
    "title": "Random forests",
    "section": "Variable importance scores",
    "text": "Variable importance scores\nIf the number of trees \\(T\\) is large (as it should be):\n\ntrees are built using lots of random subsets of predictors\ncan keep track of which ones are used most often to define splits\n\n\nVariable importance scores provide a measure of how influential each predictor is in a random forest."
  },
  {
    "objectID": "materials/slides/week3-randomforest.html#results",
    "href": "materials/slides/week3-randomforest.html#results",
    "title": "Random forests",
    "section": "Results",
    "text": "Results\nBack to the proteomics data, the variable importance scores from a random forest provide another means of identifying proteins.\n\n# grow RF\nset.seed(101222)\nrf_out &lt;- randomForest(x = asd_preds, y = asd_resp,\n                       mtry = 100, ntree = 1000, \n                       importance = T)\n\n# variable importance\nrf_out$importance %&gt;% \n  as_tibble() %&gt;%\n  mutate(protein = rownames(rf_out$importance)) %&gt;%\n  slice_max(MeanDecreaseGini, n = 10) %&gt;%\n  select(protein)\n\n# A tibble: 10 × 1\n   protein    \n   &lt;chr&gt;      \n 1 DERM       \n 2 IgD        \n 3 MAPK14     \n 4 Notch 1    \n 5 RELT       \n 6 ALCAM      \n 7 ERBB1      \n 8 eIF-4H     \n 9 CK-MB      \n10 TGF-b R III"
  },
  {
    "objectID": "materials/slides/week3-randomforest.html#errors",
    "href": "materials/slides/week3-randomforest.html#errors",
    "title": "Random forests",
    "section": "Errors",
    "text": "Errors\nBut how accurate is the predictor?\n\n\n\n\n\n\nASD\nTD\nclass.error\n\n\n\n\nASD\n53\n23\n0.3026316\n\n\nTD\n19\n59\n0.2435897"
  },
  {
    "objectID": "materials/slides/week3-randomforest.html#next-week",
    "href": "materials/slides/week3-randomforest.html#next-week",
    "title": "Random forests",
    "section": "Next week",
    "text": "Next week\n\nlogistic regression\nvariable selection\na design view of the proteomic analysis"
  },
  {
    "objectID": "materials/activities/making-trees.html",
    "href": "materials/activities/making-trees.html",
    "title": "Activity: making trees",
    "section": "",
    "text": "For the purposes of this activity we’ll use data from the 1994 census (there are fewer variables than the proteomics data and sometimes a little variety is nice).\nlibrary(tidyverse)\n\n# data location\nurl &lt;- \"http://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n\n# import\ncensus &lt;- read_csv(url,\n                   col_names = c(\"age\", \n                                 \"workclass\", \n                                 \"fnlwgt\", \n                                 \"education\",\n                                 \"education_1\",\n                                 \"marital_status\",\n                                 \"occupation\",\n                                 \"relationship\",\n                                 \"race\",\n                                 \"sex\",\n                                 \"capital_gain\",\n                                 \"capital_loss\",\n                                 \"hours_per_week\",\n                                 \"native_country\",\n                                 \"income\")) %&gt;%\n  mutate(income = factor(income)) %&gt;%\n  select(-fnlwgt, -education_1)\n\ncensus %&gt;% head(4)\n\n# A tibble: 4 × 13\n    age workclass   education marital_status occupation relationship race  sex  \n  &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;          &lt;chr&gt;      &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt;\n1    39 State-gov   Bachelors Never-married  Adm-cleri… Not-in-fami… White Male \n2    50 Self-emp-n… Bachelors Married-civ-s… Exec-mana… Husband      White Male \n3    38 Private     HS-grad   Divorced       Handlers-… Not-in-fami… White Male \n4    53 Private     11th      Married-civ-s… Handlers-… Husband      Black Male \n# ℹ 5 more variables: capital_gain &lt;dbl&gt;, capital_loss &lt;dbl&gt;,\n#   hours_per_week &lt;dbl&gt;, native_country &lt;chr&gt;, income &lt;fct&gt;\nYour group’s job is to build a tree to classify high earners and low earners based on a bootstrap sample and a random subset of predictors.\nThe income variable is the response or variable of interest; the rest are potential predictors.\n# inspect repsonse\ncensus %&gt;% pull(income) %&gt;% str()\n\n Factor w/ 2 levels \"&lt;=50K\",\"&gt;50K\": 1 1 1 1 1 1 1 2 2 2 ..."
  },
  {
    "objectID": "materials/activities/making-trees.html#building-a-tree",
    "href": "materials/activities/making-trees.html#building-a-tree",
    "title": "Activity: making trees",
    "section": "Building a tree",
    "text": "Building a tree\n\nStep 1: resample the data\nFirst, draw a sample with replacement from the observations. This is known as a bootstrap sample. We’ll keep this relatively small to simplify matters.\n\n\n\n\n\n\nAction\n\n\n\nDraw a bootstrap sample\nCopy the code chunk below and execute once.\n\n# resample data\ncensus_boot &lt;- census %&gt;%\n  sample_n(size = 200, replace = T)\n\n\n\nYou will build your tree using this data.\n\n\nStep 2: select predictors at random\nNext draw a set of predictors at random. We’ll also keep this set small so that you can develop the tree ‘by hand’.\n\n\n\n\n\n\nAction\n\n\n\nSelect two random predictors\nCopy and paste the code chunk below into your script and execute once without modification.\n\n# retrieve column names\npossible_predictors &lt;- census %&gt;% \n  select(-income) %&gt;%\n  colnames()\n\n# grab 2 columns at random\npredictors &lt;- sample(possible_predictors,\n                     size = 2, \n                     replace = F)\n\n# select these columns from the bootstrap sample\ntrain &lt;- census_boot %&gt;% \n  select(c(income, any_of(predictors)))\n\n\n\nYou will build your tree using only these predictors from the bootstrap sample as training data.\n\n\nStep 3a: find your first split\nNow your job is to choose exactly one of the predictors to make a binary split of the data.\n\nfor categorical variables, determine which categories you will classify as high-income vs. low-income\nfor continuous variables, choose a cutoff value so that any observation greater/less than the cutoff is classified as a high/low (or vice-versa) earner\n\nYou do not need to make quantitatively rigorous choices. Try to make a choice that you think is reasonable, but don’t agonize over it. The code below might help you decide which variable to use and how to make the split: use it to inspect each of the two variables and decide which one better distinguishes the income groups.\n\n# comment out -- don't overwrite your bootstrap sample!\ncensus_boot &lt;- census %&gt;% \n  sample_n(size = 200, replace = T)\n\n# for continuous variables\ncensus_boot %&gt;%\n  ggplot(aes(x = age, # replace with predictor name \n             y = income)) +\n  geom_jitter(height = 0.1) +\n  geom_vline(xintercept = 35) # adjust cutoff\n\n\n\n\n\n\n\ncensus_boot %&gt;%\n  ggplot(aes(x = age, # replace with predictor name\n           y = ..density..)) +\n  geom_density(aes(color = income, fill = income),\n               alpha = 0.5) +\n  geom_vline(xintercept = 35) # adjust cutoff\n\n\n\n\n\n\n\n# for categorical variables\ncensus_boot %&gt;%\n  group_by(workclass, income) %&gt;%\n  count() %&gt;%\n  spread(income, n) %&gt;%\n  mutate_all(~ replace_na(.x, 0)) %&gt;%\n  mutate(high.inc = `&lt;=50K` &gt; `&gt;50K`)\n\n# A tibble: 7 × 4\n# Groups:   workclass [7]\n  workclass        `&lt;=50K` `&gt;50K` high.inc\n  &lt;chr&gt;              &lt;int&gt;  &lt;int&gt; &lt;lgl&gt;   \n1 ?                     10      5 TRUE    \n2 Federal-gov            4      2 TRUE    \n3 Local-gov              9      4 TRUE    \n4 Private              108     30 TRUE    \n5 Self-emp-inc           5      4 TRUE    \n6 Self-emp-not-inc       8      5 TRUE    \n7 State-gov              4      2 TRUE    \n\n# pick out categories that are majority high income\nhighinc_categories &lt;- census_boot %&gt;%\n  group_by(workclass, # replace with predictor name\n           income) %&gt;%\n  count() %&gt;%\n  spread(income, n) %&gt;%\n  mutate_all(~ replace_na(.x, 0)) %&gt;%\n  mutate(high.inc = `&lt;=50K` &lt; `&gt;50K`) %&gt;%\n  filter(high.inc == T) %&gt;%\n  pull(workclass) # replace with predictor name \n\n\n\n\n\n\n\nAction\n\n\n\nMake your first split\n\nChoose whichever of the two predictors you think best distinguishes the high income and low income groups.\nFind a cutoff value if the variable is continuous, or the categories that you will classify as high income if the variable is categorical.\nIf categorical, store a vector of the category names that are classified as high income (see code above). If continuous, store the cutoff value.\nWrite down the rule.\n\n\n\n\n\nStep 3b: find your second split\nNow filter the data to just those rows classified as (but not necessarily actually) high income based on your first split.\n\n# continuous case -- example\ncensus_boot_sub &lt;- census_boot %&gt;% filter(age &gt; cutoff) \n\n# categorical case -- example\ncensus_boot_sub &lt;- census_boot %&gt;%\n  filter(workclass %in% highinc_categories)\n\n\n\n\n\n\n\nAction\n\n\n\nFind a second split\n\nRepeat step 3a but with the filtered data census_boot_sub instead of the full bootstrap sample.\nWrite down the rule.\n\n\n\n\n\nStep 4: draw the tree\nWe could in theory keep creating binary splits until all observations are correctly classified. However, since we’re doing this by hand and just for illustration purposes, we’ll stop after two splits.\n\n\n\n\n\n\nAction\n\n\n\nMake a diagram\nDraw your tree by hand. It should have just one ‘root’ node and just three ‘leaf’ nodes."
  },
  {
    "objectID": "materials/activities/making-trees.html#action-4",
    "href": "materials/activities/making-trees.html#action-4",
    "title": "Activity: making trees",
    "section": "Action",
    "text": "Action\nFind a second split\n\nRepeat step 3a but with the filtered data census_boot_sub instead of the full bootstrap sample.\nWrite down the rule. :::\n\n\nStep 4: draw the tree\nWe could in theory keep creating binary splits until all observations are correctly classified. However, since we’re doing this by hand and just for illustration purposes, we’ll stop after two splits.\n\n\n\n\n\n\nAction\n\n\n\nMake a diagram\nDraw your tree by hand. It should have just one ‘root’ node and just three ‘leaf’ nodes."
  },
  {
    "objectID": "materials/activities/making-trees.html#classifying-a-new-observation",
    "href": "materials/activities/making-trees.html#classifying-a-new-observation",
    "title": "Activity: making trees",
    "section": "Classifying a new observation",
    "text": "Classifying a new observation\nUse your tree to determine how to classify the following observation:\n\ncensus %&gt;%\n  sample_n(size = 1) %&gt;%\n  t() %&gt;%\n  knitr::kable()\n\n\n\n\nage\n24\n\n\nworkclass\nPrivate\n\n\neducation\nBachelors\n\n\nmarital_status\nMarried-civ-spouse\n\n\noccupation\nProf-specialty\n\n\nrelationship\nHusband\n\n\nrace\nWhite\n\n\nsex\nMale\n\n\ncapital_gain\n0\n\n\ncapital_loss\n0\n\n\nhours_per_week\n40\n\n\nnative_country\nUnited-States\n\n\nincome\n&lt;=50K"
  },
  {
    "objectID": "materials/activities/making-trees.html#algorithmic-considerations",
    "href": "materials/activities/making-trees.html#algorithmic-considerations",
    "title": "Activity: making trees",
    "section": "Algorithmic considerations",
    "text": "Algorithmic considerations\nYou just did a loose version of what’s known as recursive partitioning – repeatedly splitting the data. That’s a specific method of constructing a tree.\nHow did you decide which of your two variables to use? Could you write code to make the same choice automatically? This, it turns out, is the main challenge in fully automating the process. The recursive partitioning algorithm requires two things:\n\na criterion by which one split is considered ‘better’ than another\na stopping rule\n\nIt is fairly simple to compute the best cutoff (or categorical mapping) for a given predictor – one can do a brute-force search for the split that minimizes misclassifications. However, when there are many possible variables to split on, a criterion is needed to determine the best choice. You can read about how this is done in MDSR 11.1.1."
  }
]