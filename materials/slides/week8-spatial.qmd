---
title: "Spatial prediction"
subtitle: "PSTAT197A/CMPSC190DD Fall 2023"
institute: 'UCSB'
bibliography: refs.bib
format: 
  revealjs:
    incremental: true
    # footer: 'PSTAT197A/CMPSC190DD Fall 2023'
    # logo: 'img/ucsbds_hex.png'
    fig-width: 6
    fig-height: 4
    fig-align: 'left'
    slide-number: true
execute:
  message: false
  warning: false
  echo: false
  cache: true
---

## Final group assignment

- Sign up in a Group (3-5 members) \[[here](https://docs.google.com/spreadsheets/d/1RyqNG0k3t-sG54mbs3YxUR7eMRpqITOho9Z-3XoiAMk/edit?usp=sharing)\]

-   task: create a *method vignette* on a data science topic or theme

    -   goal: create a reference that you or someone else might use as a starting point next term

    -   deliverable: public repository in the `pstat197-F23` workspace

## Possible vignette topics {.smaller}

-   clustering methods

-   neural net architecture(s) for ... \[images, text, time series, spatial data\]

-   configuring a database and writing queries in R

-   analysis of network data

-   numerical optimization

-   bootstrapping

-   geospatial data structures

-   anomaly detection

-   functional regression

## Outputs

Your repository should contain:

1.  A brief .README summarizing repo content and listing the best references on your topic for a user to consult after reviewing your vignette if they wish to learn more
2.  A primary vignette document that explains methods and walks through implementation line-by-line (similar to an in-class or lab activity)
3.  At least one example dataset
4.  A script containing commented codes appearing in the vignette

## Timeline

-   let me know your topic by end of day Friday 12/1

-   present a draft in class Wednesday 12/6

-   finalize repository by Wednesday 12/13

## Expectations {.smaller}

You'll need to yourself learn about the topic and implementation by finding reference materials and code examples.

. . .

It ***is okay*** to borrow closely from other vignettes in creating your own, but you should:

-   cite them

-   use different data

-   do something new

. . .

It ***is not okay*** to make a collage of reference materials by copying verbatim, or simply rewrite an existing vignette.

-   the best safeguard against this is to find your own data so you're forced to translate codes/steps to apply in your particular case


# Wrapping up soil temp forecasting

## From last time

```{r}
library(tidyverse)
library(lubridate)
library(forecast)
library(sf)
library(sp)
library(gstat)
library(fda)
library(ggspatial)
library(ggmap)
theme_set(theme(text = element_text(size = 20)))
#setwd("~/pstat197/pstat197a/materials/slides")
sites <- read_csv(unz("data/soil-temp-data.zip", 
                      "data/USArray_Sites.csv"))

site_df <- sites %>%
  dplyr::select(site, longitude, latitude, elevation)

soil <- read_csv('data/soiltemp-200cm.csv') %>%
  left_join(site_df, by = 'site')
```

We had fit the *site-specific* model:

$$
\begin{aligned}
Y_{i, t} &= f_i (t) + \epsilon_{i, t} \quad\text{(nonlinear regression)} \\
\epsilon_{i, t} &= \sum_{d = 1}^D \alpha_{i,d}\epsilon_{i, t - d} + \xi_{i, t} \quad\text{(AR(D) errors)}
\end{aligned}
$$

. . .

And computed forecasts $\hat{Y}_{i, t+ 1} = \mathbb{E}(Y_{i, t + 1}|Y_{i, t})$

## Fitting and forecasts for one site {.scrollable}

::: panel-tabset
### Partitions

```{r}
#| echo: true
# data partitioning
site15 <- soil %>% 
  dplyr::select(-year, -elev) %>%
  filter(site == soil$site[15]) %>%
  arrange(date)

train <- site15 %>%
  filter(date < ymd('2018-06-01'))

test <- site15 %>%
  filter(date >= ymd('2018-06-01'))

train %>% head()
```

### Fitting

```{r}
#| echo: true
x_train <- pull(train, day) %>% 
  fourier(nbasis = 4, period = 365)
y_train <- pull(train, temp)

fit <- Arima(y_train, 
      order = c(2, 0, 0), 
      xreg = x_train, 
      include.mean = F,
      method = 'ML')

fit
```

### Forecasting

```{r}
#| echo: true
x_test <- pull(test, day) %>% 
  fourier(nbasis = 4, period = 365)

preds <- forecast(fit, h = nrow(x_test), xreg = x_test)

head(preds$mean)
```

### Visualization

```{r}
train %>%
  bind_cols(fitted = fit$fitted) %>%
  ggplot(aes(x = date, y = temp)) +
  geom_path() +
  geom_path(aes(y = fitted), 
            color = 'blue',
            alpha = 0.5) +
  geom_path(data = test, linetype = 'dotted') +
  geom_path(data = bind_cols(test, pred = preds$mean),
            aes(y = pred),
            color = 'blue',
            alpha = 0.5)
```
:::

## Now for many sites {.scrollable}

Remember the functional programming iteration strategy?

::: panel-tabset
### Fitting

```{r}
fit_fn <- function(.x, .y){
  out <- forecast::Arima(y = .y, 
               order = c(2, 0, 0), 
               xreg = .x, 
               include.mean = F, 
               method = 'ML')
  return(out)
}

pred_fn <- function(.fit, .reg){
  out <- forecast::forecast(.fit, h = nrow(.reg), xreg = .reg)
  return(out)
}

fit_df <- soil %>% 
  dplyr::select(-year, -elev) %>%
  filter(!str_starts(site, 'SHA')) %>%
  arrange(date) %>%
  nest(data = c(day, date, temp)) %>%
  mutate(train = map(data, ~filter(.x, date < ymd('2018-05-01'))),
         test = map(data, ~filter(.x, date >= ymd('2018-05-01'))),
         x = map(train, ~fourier(.x$day, nbasis = 4, period = 365)),
         y = map(train, ~pull(.x, temp)),
         fit = map2(x, y, fit_fn),
         xtest = map(test, ~fourier(.x$day, nbasis = 4, period = 365)),
         pred = map2(fit, xtest, pred_fn))

fit_df %>% 
  dplyr::select(site, train, test, fit, pred)
```

### Fit

```{r}
#| fig-width: 12
#| fig-height: 10

fit_df %>%
  mutate(fitted = map(fit, ~.x$fitted)) %>%
  dplyr::select(site, train, fitted) %>%
  unnest(everything()) %>%
  ggplot(aes(x = date, y = temp)) +
  geom_path() +
  geom_path(aes(y = fitted), color = 'blue', alpha = 0.5) +
  facet_wrap(~site) +
  labs(x =  '', y = '') +
  theme(axis.text.x = element_text(angle = 90))

```

### Predictions

```{r}
#| fig-width: 12
#| fig-height: 10
pred_df <- fit_df %>%
  mutate(y.pred = map(pred, ~.x$mean)) %>%
  dplyr::select(site, y.pred, test) %>%
  unnest(everything()) %>%
  left_join(site_df, by = 'site')

pred_df %>%
  ggplot(aes(x = date, y = temp, group = site)) +
  geom_path() +
  geom_path(aes(y = y.pred), color = 'blue') +
  facet_wrap(~site) +
  labs(x =  '', y = '') +
  theme(axis.text.x = element_text(angle = 90))
```
:::

## Spatial prediction

We could consider our data to be more explicitly spatial:

$$
Y_{i, t} = Y_t(s_i)
\qquad\text{where}\qquad
s_i = \text{location of site }i
$$

. . .

In other words, our data at a given time are a realization of a spatial process $Y(s)$ observed at locations $s_1, \dots, s_n$.

. . .

Can we predict $Y(s_{n + 1})$ based on $Y(s_1), \dots, Y(s_n)$?

## Intuition

Tobler's first law of geography:

> *"everything is related to everything else, but near things are more related than distant things"*

. . .

So a weighted average of some kind makes sense for spatial prediction

$$
\hat{Y}(s) = \sum_i w_i Y(s_i)
$$

where the *weights* $w_i$ are larger for $s_i$ closer to $s$.

## Inverse distance weighting

A simple and fully nonparametric method of spatial prediction is to set $w_i \propto 1/d(s, s_i)$ where $d$ is a distance measure.

. . .

***Inverse distance weighting*** does just that, for *powers* of distance:

$$
\hat{Y}(s) = \sum_i c \times d(s, s_i)^{-p} \times Y(s_i)
$$

Where $c$ is the normalizing constant $1/\sum_i d(s, s_i)^{-p}$.

## Power parameter

::: columns
::: {.column width="35%"}
The power parameter $p$ controls the rate of weight decay with distance:

$$
w_i \propto \frac{1}{d(s, s_i)^p}
$$
:::

::: {.column width="65%"}
```{r}
#| fig-width: 6
#| fig-height: 5
#| fig-align: center
tibble(d = seq(from = 1, to = 10, length = 100)) %>%
  mutate(`0.1` = 1/d^(0.1), 
         `0.5` = 1/sqrt(d), 
         `1` = 1/d, 
         `2` = 1/d^2) %>%
  pivot_longer(-d, values_to = 'w', names_to = 'power') %>%
  ggplot(aes(x = d, y = w, color = power, linetype = power)) +
  geom_path()
```
:::
:::

## Interpolation

Spatial ***interpolation*** refers to 'filling in' values between observed locations.

1.  Generate a spatial mesh of with centers $g_1, g_2, \dots, g_m$
2.  Predict $\hat{Y}(g_j)$ for every center $g_j$
3.  Make a raster plot

. . .

::: callout-tip
## Mesh

For spatial problems, a ***mesh*** is a mutually exclusive partitioning of an area into subregions. Subregions could be regular (*e.g.*, squares, polygons) or irregular (try googling 'Voronoi tesselation').
:::

## Map of locations

Earlier, I fit models and generated forecasts for 26 sites chosen largely based on having overlapping observation windows.

![](img/map_I.png){fig-align="center"}

## Forecasts

I also truncated the training data to stop on the same date (April 30, 2018). So we can plot point forecasts for May 1.

![](img/map_II.png){fig-align="center"}

## Interpolations using IDW

So interpolating between forecasts yields spatial forecasts.

![](img/map_III.png){fig-align="center"}

## Effect of IDW parameter $p$

The power parameter $p$ controls the rate of decay of interpolation weight $w_i$ with distance.

![](img/map_IV.png){fig-align="center"}

## Considerations

-   Choosing $p$ can be done based on optimizing predictions or by hand.

-   Uncertainty quantification?

    -   usually, could use variance of weighted average

    -   but also tricky in this case because we are interpolating *forecasts*, which themselves have some associated uncertainty
